<!DOCTYPE html>
<html>
<head>
    

    

    



    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    
    
    
    <title>Deep Learning and Computer Vision Recommended Paper | world4jason | 菜鳥搬磚日常</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="Paper List,Self-Study,Deep Learning,Computer Vision">
    <meta name="description" content="Image ClassificationMust Read : LeNet, AlexNet, VGG-16, GoogleNet, ResNet   Figure Title Authors Pub. Links">
<meta name="keywords" content="Paper List,Self-Study,Deep Learning,Computer Vision">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Learning and Computer Vision Recommended Paper">
<meta property="og:url" content="https://world4jason.github.io/2018/01/09/Deep-Learning-Recommended-Papers/index.html">
<meta property="og:site_name" content="world4jason">
<meta property="og:description" content="Image ClassificationMust Read : LeNet, AlexNet, VGG-16, GoogleNet, ResNet   Figure Title Authors Pub. Links">
<meta property="og:locale" content="zh-TW">
<meta property="og:image" content="http://yann.lecun.com/exdb/lenet/">
<meta property="og:image" content="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">
<meta property="og:image" content="https://arxiv.org/pdf/1409.1556.pdf">
<meta property="og:image" content="http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html">
<meta property="og:image" content="https://github.com/KaimingHe/deep-residual-networks">
<meta property="og:image" content="https://github.com/buptwangfei/residual-attention-network">
<meta property="og:image" content="https://github.com/facebookresearch/ResNeXt">
<meta property="og:image" content="https://github.com/liuzhuang13/DenseNet">
<meta property="og:image" content="https://github.com/jhkim89/PyramidNet">
<meta property="og:image" content="https://github.com/rbgirshick/rcnn">
<meta property="og:image" content="https://github.com/rbgirshick/fast-rcnn">
<meta property="og:image" content="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf">
<meta property="og:image" content="https://github.com/longcw/faster_rcnn_pytorch">
<meta property="og:image" content="http://arxiv.org/pdf/1506.02640.pdf">
<meta property="og:image" content="http://arxiv.org/pdf/1512.02325.pdf">
<meta property="og:image" content="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Dai_Convolutional_Feature_Masking_2015_CVPR_paper.pdf">
<meta property="og:image" content="https://github.com/daijifeng001/MNC">
<meta property="og:image" content="https://github.com/daijifeng001/R-FCN">
<meta property="og:image" content="https://arxiv.org/pdf/1612.03144.pdf">
<meta property="og:image" content="https://arxiv.org/abs/1703.06870">
<meta property="og:image" content="https://github.com/xiaolonw/adversarial-frcnn">
<meta property="og:image" content="https://arxiv.org/abs/1704.00138">
<meta property="og:image" content="https://arxiv.org/abs/1712.01802">
<meta property="og:image" content="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf">
<meta property="og:image" content="http://papers.nips.cc/paper/5852-learning-to-segment-object-candidates.pdf">
<meta property="og:image" content="https://arxiv.org/pdf/1603.08695.pdf">
<meta property="og:image" content="http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Zheng_Conditional_Random_Fields_ICCV_2015_paper.html">
<meta property="og:image" content="http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Noh_Learning_Deconvolution_Network_ICCV_2015_paper.html">
<meta property="og:image" content="https://arxiv.org/pdf/1505.04597.pdf">
<meta property="og:image" content="https://arxiv.org/abs/1603.08678">
<meta property="og:image" content="https://github.com/golnazghiasi/LRR">
<meta property="og:image" content="http://liangchiehchen.com/projects/DeepLab.html">
<meta property="og:image" content="https://github.com/guosheng/refinenet">
<meta property="og:image" content="https://github.com/hszhao/PSPNet">
<meta property="og:image" content="https://github.com/fyu/drn">
<meta property="og:image" content="https://github.com/msracver/FCIS">
<meta property="og:image" content="https://github.com/TobyPDE/FRRN">
<meta property="og:image" content="https://arxiv.org/abs/1703.08448">
<meta property="og:image" content="https://arxiv.org/abs/1704.01344">
<meta property="og:image" content="https://drive.google.com/drive/folders/0By2w_AaM8Rzbllnc3JCQjhHYnM?usp=sharing">
<meta property="og:image" content="https://thoth.inrialpes.fr/people/pluc/iccv2017">
<meta property="og:image" content="https://arxiv.org/abs/1711.10370">
<meta property="og:updated_time" content="2018-01-10T09:08:29.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Deep Learning and Computer Vision Recommended Paper">
<meta name="twitter:description" content="Image ClassificationMust Read : LeNet, AlexNet, VGG-16, GoogleNet, ResNet   Figure Title Authors Pub. Links">
<meta name="twitter:image" content="http://yann.lecun.com/exdb/lenet/">
    
        <link rel="alternate" type="application/atom+xml" title="world4jason" href="/atom.xml">
    
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="stylesheet" href="//unpkg.com/hexo-theme-material-indigo@latest/css/style.css">
    <script>window.lazyScripts=[]</script>

    <!-- custom head --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    

</head>

<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">Jason Yeh</h5>
          <a href="mailto:world4jason@gmail.com" title="world4jason@gmail.com" class="mail">world4jason@gmail.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-Home"></i>
                Home
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                Categories
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/world4jason" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/research"  >
                <i class="icon icon-lg icon-research"></i>
                Research
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about"  >
                <i class="icon icon-lg icon-about"></i>
                About
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/cv"  >
                <i class="icon icon-lg icon-CV"></i>
                CV
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">Deep Learning and Computer Vision Recommended Paper</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="搜尋">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">Deep Learning and Computer Vision Recommended Paper</h1>
        <h5 class="subtitle">
            
                <time datetime="2018-01-09T13:50:49.000Z" itemprop="datePublished" class="page-time">
  2018-01-09
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/Deep-Learning/">Deep Learning</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Image-Classification"><span class="post-toc-number">1.</span> <span class="post-toc-text">Image Classification</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Must-Read-LeNet-AlexNet-VGG-16-GoogleNet-ResNet"><span class="post-toc-number">1.0.1.</span> <span class="post-toc-text">Must Read : LeNet, AlexNet, VGG-16, GoogleNet, ResNet</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Object-Detection"><span class="post-toc-number">2.</span> <span class="post-toc-text">Object Detection</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Must-Read-R-CNN-Fast-R-CNN-Faster-R-CNN-YOLO-SSD"><span class="post-toc-number">2.0.1.</span> <span class="post-toc-text">Must Read : R-CNN, Fast R-CNN, Faster R-CNN, YOLO, SSD</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Semantic-Segmentation-and-Scene-Parsing"><span class="post-toc-number">3.</span> <span class="post-toc-text">Semantic Segmentation and Scene Parsing</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Must-Read-FCN-Learning-Deconvolution-Network-for-Semantic-Segmentation-U-Net"><span class="post-toc-number">3.0.1.</span> <span class="post-toc-text">Must Read : FCN, Learning Deconvolution Network for Semantic Segmentation, U-Net</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Regularization"><span class="post-toc-number">4.</span> <span class="post-toc-text">Regularization</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#RNN"><span class="post-toc-number">5.</span> <span class="post-toc-text">RNN</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Image-captioning"><span class="post-toc-number">6.</span> <span class="post-toc-text">Image captioning</span></a></li></ol>
        </nav>
    </aside>
    
<article id="post-Deep-Learning-Recommended-Papers"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">Deep Learning and Computer Vision Recommended Paper</h1>
        <div class="post-meta">
            <time class="post-time" title="2018-01-09 21:50:49" datetime="2018-01-09T13:50:49.000Z"  itemprop="datePublished">2018-01-09</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/Deep-Learning/">Deep Learning</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <h2 id="Image-Classification"><a href="#Image-Classification" class="headerlink" title="Image Classification"></a>Image Classification</h2><h4 id="Must-Read-LeNet-AlexNet-VGG-16-GoogleNet-ResNet"><a href="#Must-Read-LeNet-AlexNet-VGG-16-GoogleNet-ResNet" class="headerlink" title="Must Read : LeNet, AlexNet, VGG-16, GoogleNet, ResNet"></a>Must Read : LeNet, AlexNet, VGG-16, GoogleNet, ResNet</h4><table>
<thead>
<tr>
<th style="text-align:center">Figure</th>
<th style="text-align:center">Title</th>
<th style="text-align:center">Authors</th>
<th style="text-align:center">Pub.</th>
<th style="text-align:center">Links</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://yann.lecun.com/exdb/lenet/" alt="LeNet](/media/data/LeNet.png)|LeNet-5, convolutional neural networks|Y. LeCun|??? 199X|[`Web`" title="">
                </div>
                <div class="image-caption">LeNet](/media/data/LeNet.png)|LeNet-5, convolutional neural networks|Y. LeCun|??? 199X|[`Web`</div>
            </figure></td>
</tr>
<tr>
<td style="text-align:center"><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" alt="AlexNet](/media/data/AlexNet.jpg)|Very Deep Convolutional Networks for Large-Scale Image Recognition|Karen Simonyan, Andrew Zisserman|NIPS 2014|[paper" title="">
                </div>
                <div class="image-caption">AlexNet](/media/data/AlexNet.jpg)|Very Deep Convolutional Networks for Large-Scale Image Recognition|Karen Simonyan, Andrew Zisserman|NIPS 2014|[paper</div>
            </figure></td>
</tr>
<tr>
<td style="text-align:center"><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://arxiv.org/pdf/1409.1556.pdf" alt="GoogLeNet](/media/data/VGG16.png)|Very Deep Convolutional Networks for Large-Scale Image Recognition|Karen Simonyan, Andrew Zisserman|ICLR 2014|[paper" title="">
                </div>
                <div class="image-caption">GoogLeNet](/media/data/VGG16.png)|Very Deep Convolutional Networks for Large-Scale Image Recognition|Karen Simonyan, Andrew Zisserman|ICLR 2014|[paper</div>
            </figure></td>
</tr>
<tr>
<td style="text-align:center"><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html" alt="GoogLeNet](/media/data/GoogLeNet.png)|Going Deeper with Convolutions|Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed|CVPR 2015|[paper" title="">
                </div>
                <div class="image-caption">GoogLeNet](/media/data/GoogLeNet.png)|Going Deeper with Convolutions|Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed|CVPR 2015|[paper</div>
            </figure></td>
</tr>
<tr>
<td style="text-align:center"><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://github.com/KaimingHe/deep-residual-networks" alt="ResNet](/media/data/ResNet.png)|Deep Residual Learning for Image Recognition|Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun|CVPR 2016 *`best`*|[paper](https://arxiv.org/abs/1512.03385) [github" title="">
                </div>
                <div class="image-caption">ResNet](/media/data/ResNet.png)|Deep Residual Learning for Image Recognition|Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun|CVPR 2016 *`best`*|[paper](https://arxiv.org/abs/1512.03385) [github</div>
            </figure></td>
</tr>
<tr>
<td style="text-align:center"><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://github.com/buptwangfei/residual-attention-network" alt="Res-Attention-Network](/media/data/Res-Attention-Network.png)|Residual Attention Network for Image Classification|Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, Xiaoou Tang|CVPR 2017|[paper](https://arxiv.org/abs/1704.06904) [github" title="">
                </div>
                <div class="image-caption">Res-Attention-Network](/media/data/Res-Attention-Network.png)|Residual Attention Network for Image Classification|Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, Xiaoou Tang|CVPR 2017|[paper](https://arxiv.org/abs/1704.06904) [github</div>
            </figure></td>
</tr>
<tr>
<td style="text-align:center"><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://github.com/facebookresearch/ResNeXt" alt="ResNeXt](/media/data/ResNeXt.png)|Aggregated Residual Transformations for Deep Neural Networks|Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He|CVPR 2017|[paper](https://arxiv.org/abs/1611.05431) [github" title="">
                </div>
                <div class="image-caption">ResNeXt](/media/data/ResNeXt.png)|Aggregated Residual Transformations for Deep Neural Networks|Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He|CVPR 2017|[paper](https://arxiv.org/abs/1611.05431) [github</div>
            </figure></td>
</tr>
<tr>
<td style="text-align:center"><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://github.com/liuzhuang13/DenseNet" alt="DenseNet](/media/data/DenseNet.png)|Densely Connected Convolutional Networks|Gao Huang, Zhuang Liu, Kilian Q. Weinberger|CVPR 2017 *`best`*|[paper](https://arxiv.org/abs/1608.06993) [github" title="">
                </div>
                <div class="image-caption">DenseNet](/media/data/DenseNet.png)|Densely Connected Convolutional Networks|Gao Huang, Zhuang Liu, Kilian Q. Weinberger|CVPR 2017 *`best`*|[paper](https://arxiv.org/abs/1608.06993) [github</div>
            </figure></td>
</tr>
<tr>
<td style="text-align:center"><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://github.com/jhkim89/PyramidNet" alt="PyramidNet](/media/data/PyramidNet.png)|Deep Pyramidal Residual Networks|Dongyoon Han, Jiwhan Kim, Junmo Kim|CVPR 2017|[paper](https://arxiv.org/pdf/1610.02915.pdf) [github" title="">
                </div>
                <div class="image-caption">PyramidNet](/media/data/PyramidNet.png)|Deep Pyramidal Residual Networks|Dongyoon Han, Jiwhan Kim, Junmo Kim|CVPR 2017|[paper](https://arxiv.org/pdf/1610.02915.pdf) [github</div>
            </figure></td>
</tr>
</tbody>
</table>
<h2 id="Object-Detection"><a href="#Object-Detection" class="headerlink" title="Object Detection"></a>Object Detection</h2><h4 id="Must-Read-R-CNN-Fast-R-CNN-Faster-R-CNN-YOLO-SSD"><a href="#Must-Read-R-CNN-Fast-R-CNN-Faster-R-CNN-YOLO-SSD" class="headerlink" title="Must Read : R-CNN, Fast R-CNN, Faster R-CNN, YOLO, SSD"></a>Must Read : R-CNN, Fast R-CNN, Faster R-CNN, YOLO, SSD</h4><table>
<thead>
<tr>
<th style="text-align:center">Figure</th>
<th style="text-align:center">Title</th>
<th style="text-align:center">Authors</th>
<th style="text-align:center">Pub.</th>
<th style="text-align:center">Links</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://github.com/rbgirshick/rcnn" alt="R-CNN](/media/data/R-CNN.png)|Rich feature hierarchies for accurate object detection and semantic segmentation|Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik|CVPR 2014|[paper](http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf) [github" title="">
                </div>
                <div class="image-caption">R-CNN](/media/data/R-CNN.png)|Rich feature hierarchies for accurate object detection and semantic segmentation|Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik|CVPR 2014|[paper](http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf) [github</div>
            </figure></td>
</tr>
<tr>
<td style="text-align:center"><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://github.com/rbgirshick/fast-rcnn" alt="Fast-R-CNN](/media/data/Fast-R-CNN.png)|Fast R-CNN|Ross Girshick|ICCV 2015|[paper](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf) [github" title="">
                </div>
                <div class="image-caption">Fast-R-CNN](/media/data/Fast-R-CNN.png)|Fast R-CNN|Ross Girshick|ICCV 2015|[paper](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf) [github</div>
            </figure></td>
</tr>
<tr>
<td style="text-align:center"><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf" alt="SPP Net](/media/data/SPP-Net.png)|Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition|Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun|TPAMI 2015|[paper" title="">
                </div>
                <div class="image-caption">SPP Net](/media/data/SPP-Net.png)|Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition|Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun|TPAMI 2015|[paper</div>
            </figure> </td>
</tr>
<tr>
<td style="text-align:center"><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://github.com/longcw/faster_rcnn_pytorch" alt="Faster-R-CNN](/media/data/Faster-R-CNN.png)|Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks|Shaoqing Ren, [Kaiming He](http://kaiminghe.com/), Ross Girshick, Jian Sun|NIPS 2015|[paper](http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf) [`matlab`](https://github.com/ShaoqingRen/faster_rcnn) [`python`](https://github.com/rbgirshick/py-faster-rcnn) [`pytorch`" title="">
                </div>
                <div class="image-caption">Faster-R-CNN](/media/data/Faster-R-CNN.png)|Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks|Shaoqing Ren, [Kaiming He](http://kaiminghe.com/), Ross Girshick, Jian Sun|NIPS 2015|[paper](http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf) [`matlab`](https://github.com/ShaoqingRen/faster_rcnn) [`python`](https://github.com/rbgirshick/py-faster-rcnn) [`pytorch`</div>
            </figure></td>
</tr>
<tr>
<td style="text-align:center"><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://arxiv.org/pdf/1506.02640.pdf" alt="YOLO](/media/data/YOLO.jpg)|You Only Look Once: Unified, Real-Time Object Detection|Joseph Redmon,Santosh Divvala,Ross Girshick, Ali Farhadi|CVPR 2016|[paper" title="">
                </div>
                <div class="image-caption">YOLO](/media/data/YOLO.jpg)|You Only Look Once: Unified, Real-Time Object Detection|Joseph Redmon,Santosh Divvala,Ross Girshick, Ali Farhadi|CVPR 2016|[paper</div>
            </figure></td>
</tr>
<tr>
<td style="text-align:center"><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://arxiv.org/pdf/1512.02325.pdf" alt="YOLO](/media/data/YOLO.jpg)|SSD: Single Shot MultiBox Detector|Wei Liu1, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg|CVPR 2016|[paper" title="">
                </div>
                <div class="image-caption">YOLO](/media/data/YOLO.jpg)|SSD: Single Shot MultiBox Detector|Wei Liu1, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg|CVPR 2016|[paper</div>
            </figure></td>
</tr>
<tr>
<td style="text-align:center"><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Dai_Convolutional_Feature_Masking_2015_CVPR_paper.pdf" alt="CFM](/media/data/CFM.png)|Convolutional Feature Masking for Joint Object and Stuff Segmentation|Jifeng Dai, [Kaiming He](http://kaiminghe.com/), Jian Sun|CVPR 2015|[paper" title="">
                </div>
                <div class="image-caption">CFM](/media/data/CFM.png)|Convolutional Feature Masking for Joint Object and Stuff Segmentation|Jifeng Dai, [Kaiming He](http://kaiminghe.com/), Jian Sun|CVPR 2015|[paper</div>
            </figure></td>
</tr>
<tr>
<td style="text-align:center"><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://github.com/daijifeng001/MNC" alt="MNC](/media/data/MNC.png)|Instance-aware Semantic Segmentation via Multi-task Network Cascades|Jifeng Dai, [Kaiming He](http://kaiminghe.com/), Jian Sun|CVPR 2016|[paper](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Dai_Instance-Aware_Semantic_Segmentation_CVPR_2016_paper.pdf) [github" title="">
                </div>
                <div class="image-caption">MNC](/media/data/MNC.png)|Instance-aware Semantic Segmentation via Multi-task Network Cascades|Jifeng Dai, [Kaiming He](http://kaiminghe.com/), Jian Sun|CVPR 2016|[paper](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Dai_Instance-Aware_Semantic_Segmentation_CVPR_2016_paper.pdf) [github</div>
            </figure></td>
</tr>
<tr>
<td style="text-align:center"><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://github.com/daijifeng001/R-FCN" alt="Region-FCN](/media/data/Region-FCN.png)|R-FCN: Object Detection via Region-based Fully Convolutional Networks|Jifeng Dai, Yi Li, [Kaiming He](http://kaiminghe.com/), Jian Sun|NIPS 2016|[paper](https://arxiv.org/abs/1605.06409) [github" title="">
                </div>
                <div class="image-caption">Region-FCN](/media/data/Region-FCN.png)|R-FCN: Object Detection via Region-based Fully Convolutional Networks|Jifeng Dai, Yi Li, [Kaiming He](http://kaiminghe.com/), Jian Sun|NIPS 2016|[paper](https://arxiv.org/abs/1605.06409) [github</div>
            </figure></td>
</tr>
<tr>
<td style="text-align:center"><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://arxiv.org/pdf/1612.03144.pdf" alt="FPN](/media/data/FPN.png)|Feature Pyramid Networks for Object Detection|Tsung-Yi Lin, Piotr Dollár, Ross Girshick, [Kaiming He](http://kaiminghe.com/), Bharath Hariharan, and Serge Belongie|CVPR 2017|[paper" title="">
                </div>
                <div class="image-caption">FPN](/media/data/FPN.png)|Feature Pyramid Networks for Object Detection|Tsung-Yi Lin, Piotr Dollár, Ross Girshick, [Kaiming He](http://kaiminghe.com/), Bharath Hariharan, and Serge Belongie|CVPR 2017|[paper</div>
            </figure></td>
</tr>
<tr>
<td style="text-align:center"><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://arxiv.org/abs/1703.06870" alt="Mask-R-CNN](/media/data/Mask-R-CNN.png)|Mask R-CNN|Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick|ICCV 2017|[paper" title="">
                </div>
                <div class="image-caption">Mask-R-CNN](/media/data/Mask-R-CNN.png)|Mask R-CNN|Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick|ICCV 2017|[paper</div>
            </figure></td>
</tr>
<tr>
<td style="text-align:center"><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://github.com/xiaolonw/adversarial-frcnn" alt="A-Fast-R-CNN](/media/data/A-Fast-R-CNN.png)|A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection|Xiaolong Wang, Abhinav Shrivastava, Abhinav Gupta|CVPR 2017|[paper](https://arxiv.org/abs/1704.03414)  [github" title="">
                </div>
                <div class="image-caption">A-Fast-R-CNN](/media/data/A-Fast-R-CNN.png)|A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection|Xiaolong Wang, Abhinav Shrivastava, Abhinav Gupta|CVPR 2017|[paper](https://arxiv.org/abs/1704.03414)  [github</div>
            </figure></td>
</tr>
<tr>
<td style="text-align:center"><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://arxiv.org/abs/1704.00138" alt="MIDN](/media/data/MIDN.png)|Multiple Instance Detection Network with Online Instance Classifier Refinement|Peng Tang, Xinggang Wang, Xiang Bai, Wenyu Liu|CVPR 2017|[paper" title="">
                </div>
                <div class="image-caption">MIDN](/media/data/MIDN.png)|Multiple Instance Detection Network with Online Instance Classifier Refinement|Peng Tang, Xinggang Wang, Xiang Bai, Wenyu Liu|CVPR 2017|[paper</div>
            </figure></td>
</tr>
<tr>
<td style="text-align:center"><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://arxiv.org/abs/1712.01802" alt="R-FCN-3000](/media/data/R-FCN-3000.png)|R-FCN-3000 at 30fps: Decoupling Detection and Classification|Bharat Singh, Hengdou Li, Abhishek Sharma and Larry S. Davis|Tech Report|[paper" title="">
                </div>
                <div class="image-caption">R-FCN-3000](/media/data/R-FCN-3000.png)|R-FCN-3000 at 30fps: Decoupling Detection and Classification|Bharat Singh, Hengdou Li, Abhishek Sharma and Larry S. Davis|Tech Report|[paper</div>
            </figure></td>
</tr>
</tbody>
</table>
<h2 id="Semantic-Segmentation-and-Scene-Parsing"><a href="#Semantic-Segmentation-and-Scene-Parsing" class="headerlink" title="Semantic Segmentation and Scene Parsing"></a>Semantic Segmentation and Scene Parsing</h2><h4 id="Must-Read-FCN-Learning-Deconvolution-Network-for-Semantic-Segmentation-U-Net"><a href="#Must-Read-FCN-Learning-Deconvolution-Network-for-Semantic-Segmentation-U-Net" class="headerlink" title="Must Read : FCN, Learning Deconvolution Network for Semantic Segmentation, U-Net"></a>Must Read : FCN, Learning Deconvolution Network for Semantic Segmentation, U-Net</h4><table>
<thead>
<tr>
<th>No.</th>
<th style="text-align:center">Figure</th>
<th style="text-align:center">Title</th>
<th style="text-align:center">Authors</th>
<th style="text-align:center">Pub.</th>
<th style="text-align:center">Links</th>
</tr>
</thead>
<tbody>
<tr>
<td><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf" alt="FCN](/media/data/FCN.png)|Fully Convolutional Networks for Semantic Segmentation|Jonathan Long, Evan Shelhamer, Trevor Darrell|CVPR 2015|[paper" title="">
                </div>
                <div class="image-caption">FCN](/media/data/FCN.png)|Fully Convolutional Networks for Semantic Segmentation|Jonathan Long, Evan Shelhamer, Trevor Darrell|CVPR 2015|[paper</div>
            </figure> :star:</td>
</tr>
<tr>
<td><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://papers.nips.cc/paper/5852-learning-to-segment-object-candidates.pdf" alt="LSOC](/media/data/LSOC.png)|Learning to Segment Object Candidates|Pedro O. Pinheiro, Ronan Collobert, Piotr Dollar|NIPS 2015|[paper" title="">
                </div>
                <div class="image-caption">LSOC](/media/data/LSOC.png)|Learning to Segment Object Candidates|Pedro O. Pinheiro, Ronan Collobert, Piotr Dollar|NIPS 2015|[paper</div>
            </figure></td>
</tr>
<tr>
<td><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://arxiv.org/pdf/1603.08695.pdf" alt="LROS](/media/data/LROS.png)|Learning to Refine Object Segments|Pedro O. Pinheiro , Tsung-Yi Lin , Ronan Collobert, Piotr Doll ́ar|arXiv 1603.08695|[paper" title="">
                </div>
                <div class="image-caption">LROS](/media/data/LROS.png)|Learning to Refine Object Segments|Pedro O. Pinheiro , Tsung-Yi Lin , Ronan Collobert, Piotr Doll ́ar|arXiv 1603.08695|[paper</div>
            </figure></td>
</tr>
<tr>
<td><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Zheng_Conditional_Random_Fields_ICCV_2015_paper.html" alt="CRFRNN](/media/data/CRFRNN.png)|Conditional Random Fields as Recurrent Neural Networks|Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, ZhiZhong Su, Dalong Du, Chang Huang, and Philip H. S. Torr|ICCV 2015|[paper" title="">
                </div>
                <div class="image-caption">CRFRNN](/media/data/CRFRNN.png)|Conditional Random Fields as Recurrent Neural Networks|Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, ZhiZhong Su, Dalong Du, Chang Huang, and Philip H. S. Torr|ICCV 2015|[paper</div>
            </figure></td>
</tr>
<tr>
<td><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Noh_Learning_Deconvolution_Network_ICCV_2015_paper.html" alt="LDN](/media/data/LDN.png)|Learning Deconvolution Network for Semantic Segmentation|Heonwoo Noh, Seunghoon Hong, Bohyung Han|ICCV 2015|[paper" title="">
                </div>
                <div class="image-caption">LDN](/media/data/LDN.png)|Learning Deconvolution Network for Semantic Segmentation|Heonwoo Noh, Seunghoon Hong, Bohyung Han|ICCV 2015|[paper</div>
            </figure></td>
</tr>
<tr>
<td><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://arxiv.org/pdf/1505.04597.pdf" alt="U-Net](/media/data/U-Net.png)|U-Net: Convolutional Networks for Biomedical Image Segmentation|Olaf Ronneberger, Philipp Fischer, Thomas Brox|MICCAI 2015|[paper" title="">
                </div>
                <div class="image-caption">U-Net](/media/data/U-Net.png)|U-Net: Convolutional Networks for Biomedical Image Segmentation|Olaf Ronneberger, Philipp Fischer, Thomas Brox|MICCAI 2015|[paper</div>
            </figure></td>
</tr>
<tr>
<td><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://arxiv.org/abs/1603.08678" alt="ISFCN](/media/data/ISFCN.png)|Instance-sensitive Fully Convolutional Networks|Jifeng Dai, Kaiming He, Yi Li, Shaoqing Ren, Jian Sun|ECCV 2016|[paper" title="">
                </div>
                <div class="image-caption">ISFCN](/media/data/ISFCN.png)|Instance-sensitive Fully Convolutional Networks|Jifeng Dai, Kaiming He, Yi Li, Shaoqing Ren, Jian Sun|ECCV 2016|[paper</div>
            </figure></td>
</tr>
<tr>
<td><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://github.com/golnazghiasi/LRR" alt="LPRR](/media/data/LPRR.png)|Laplacian Pyramid Reconstruction and Refinement for Semantic Segmentation|Golnaz Ghiasi, Charless C. Fowlkes|ECCV 2016|[paper](https://link.springer.com/chapter/10.1007/978-3-319-46487-9_32)  [github" title="">
                </div>
                <div class="image-caption">LPRR](/media/data/LPRR.png)|Laplacian Pyramid Reconstruction and Refinement for Semantic Segmentation|Golnaz Ghiasi, Charless C. Fowlkes|ECCV 2016|[paper](https://link.springer.com/chapter/10.1007/978-3-319-46487-9_32)  [github</div>
            </figure></td>
</tr>
<tr>
<td><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://liangchiehchen.com/projects/DeepLab.html" alt="Attention-to-scale](/media/data/Attention-to-scale.png)|Attention to Scale: Scale-aware Semantic Image Segmentation|Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu|CVPR 2016|[paper](http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Chen_Attention_to_Scale_CVPR_2016_paper.html) [`DeepLab`" title="">
                </div>
                <div class="image-caption">Attention-to-scale](/media/data/Attention-to-scale.png)|Attention to Scale: Scale-aware Semantic Image Segmentation|Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu|CVPR 2016|[paper](http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Chen_Attention_to_Scale_CVPR_2016_paper.html) [`DeepLab`</div>
            </figure> :star:</td>
</tr>
<tr>
<td><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://github.com/guosheng/refinenet" alt="RefineNet](/media/data/RefineNet.png)|RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation|Guosheng Lin, Anton Milan, Chunhua Shen, Ian Reid|CVPR 2017|[paper](https://arxiv.org/abs/1611.06612)  [github" title="">
                </div>
                <div class="image-caption">RefineNet](/media/data/RefineNet.png)|RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation|Guosheng Lin, Anton Milan, Chunhua Shen, Ian Reid|CVPR 2017|[paper](https://arxiv.org/abs/1611.06612)  [github</div>
            </figure></td>
</tr>
<tr>
<td><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://github.com/hszhao/PSPNet" alt="PSPNet](/media/data/PSPNet.png)|Pyramid Scene Parsing Network|Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia|CVPR 2017|[paper](https://arxiv.org/abs/1612.01105)  [github" title="">
                </div>
                <div class="image-caption">PSPNet](/media/data/PSPNet.png)|Pyramid Scene Parsing Network|Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia|CVPR 2017|[paper](https://arxiv.org/abs/1612.01105)  [github</div>
            </figure></td>
</tr>
<tr>
<td><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://github.com/fyu/drn" alt="DRN](/media/data/DRN.png)|Dilated Residual Networks|Fisher Yu, Vladlen Koltun, Thomas Funkhouser|CVPR 2017|[paper](https://arxiv.org/abs/1705.09914) [github" title="">
                </div>
                <div class="image-caption">DRN](/media/data/DRN.png)|Dilated Residual Networks|Fisher Yu, Vladlen Koltun, Thomas Funkhouser|CVPR 2017|[paper](https://arxiv.org/abs/1705.09914) [github</div>
            </figure></td>
</tr>
<tr>
<td><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://github.com/msracver/FCIS" alt="FCIS](/media/data/FCIS.png)|Fully Convolutional Instance-aware Semantic Segmentation|Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, Yichen Wei|CVPR 2017|[paper](https://arxiv.org/abs/1611.07709) [github" title="">
                </div>
                <div class="image-caption">FCIS](/media/data/FCIS.png)|Fully Convolutional Instance-aware Semantic Segmentation|Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, Yichen Wei|CVPR 2017|[paper](https://arxiv.org/abs/1611.07709) [github</div>
            </figure></td>
</tr>
<tr>
<td><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://github.com/TobyPDE/FRRN" alt="FRRN](/media/data/FRRN.png)|Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes|Tobias Pohlen, Alexander Hermans, Markus Mathias, Bastian Leibe|CVPR 2017|[paper](https://arxiv.org/abs/1611.08323) [github" title="">
                </div>
                <div class="image-caption">FRRN](/media/data/FRRN.png)|Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes|Tobias Pohlen, Alexander Hermans, Markus Mathias, Bastian Leibe|CVPR 2017|[paper](https://arxiv.org/abs/1611.08323) [github</div>
            </figure></td>
</tr>
<tr>
<td><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://arxiv.org/abs/1703.08448" alt="A-Erasing](/media/data/A-Erasing.png)|Object Region Mining with Adversarial Erasing: A Simple Classification toSemantic Segmentation Approach|Yunchao Wei, Jiashi Feng, Xiaodan Liang, Ming-Ming Cheng, Yao Zhao, Shuicheng Yan|CVPR 2017|[paper" title="">
                </div>
                <div class="image-caption">A-Erasing](/media/data/A-Erasing.png)|Object Region Mining with Adversarial Erasing: A Simple Classification toSemantic Segmentation Approach|Yunchao Wei, Jiashi Feng, Xiaodan Liang, Ming-Ming Cheng, Yao Zhao, Shuicheng Yan|CVPR 2017|[paper</div>
            </figure> :star:</td>
</tr>
<tr>
<td><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://arxiv.org/abs/1704.01344" alt="Not-All-Pixels-Are-Equal](/media/data/Not-All-Pixels-Are-Equal.png)|Not All Pixels Are Equal: Difficulty-Aware Semantic Segmentation via Deep Layer Cascade|Xiaoxiao Li, Ziwei Liu, Ping Luo, Chen Change Loy, Xiaoou Tang|CVPR 2017|[paper" title="">
                </div>
                <div class="image-caption">Not-All-Pixels-Are-Equal](/media/data/Not-All-Pixels-Are-Equal.png)|Not All Pixels Are Equal: Difficulty-Aware Semantic Segmentation via Deep Layer Cascade|Xiaoxiao Li, Ziwei Liu, Ping Luo, Chen Change Loy, Xiaoou Tang|CVPR 2017|[paper</div>
            </figure></td>
</tr>
<tr>
<td><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://drive.google.com/drive/folders/0By2w_AaM8Rzbllnc3JCQjhHYnM?usp=sharing" alt="Rev-Attention](/media/data/Rev-Attention.png)|Semantic Segmentation with Reverse Attention|Qin Huang, Chunyang Xia, Wuchi Hao, Siyang Li, Ye Wang, Yuhang Song and C.-C. Jay Kuo|BMVC 2017|[paper](https://arxiv.org/abs/1707.06426) [`code`" title="">
                </div>
                <div class="image-caption">Rev-Attention](/media/data/Rev-Attention.png)|Semantic Segmentation with Reverse Attention|Qin Huang, Chunyang Xia, Wuchi Hao, Siyang Li, Ye Wang, Yuhang Song and C.-C. Jay Kuo|BMVC 2017|[paper](https://arxiv.org/abs/1707.06426) [`code`</div>
            </figure></td>
</tr>
<tr>
<td><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://thoth.inrialpes.fr/people/pluc/iccv2017" alt="Deeper-into-Future](/media/data/Deeper-into-Future.png)|Predicting Deeper into the Future of Semantic Segmentation|Pauline Luc, Natalia Neverova, Camille Couprie, Jakob Verbeek and Yann LeCun|ICCV 2017|[paper](https://arxiv.org/abs/1703.07684) [`project page`" title="">
                </div>
                <div class="image-caption">Deeper-into-Future](/media/data/Deeper-into-Future.png)|Predicting Deeper into the Future of Semantic Segmentation|Pauline Luc, Natalia Neverova, Camille Couprie, Jakob Verbeek and Yann LeCun|ICCV 2017|[paper](https://arxiv.org/abs/1703.07684) [`project page`</div>
            </figure></td>
</tr>
<tr>
<td><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://arxiv.org/abs/1711.10370" alt="Seg-Everything](/media/data/Seg-Everything.png)|Learning to Segment Every Thing|Ronghang Hu, Piotr Dollar, Kaiming He, Trevor Darrell, Ross Girshick|Tech Report|[paper" title="">
                </div>
                <div class="image-caption">Seg-Everything](/media/data/Seg-Everything.png)|Learning to Segment Every Thing|Ronghang Hu, Piotr Dollar, Kaiming He, Trevor Darrell, Ross Girshick|Tech Report|[paper</div>
            </figure></td>
</tr>
</tbody>
</table>
<h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><ul>
<li>Dropout- A Simple Way to Prevent Neural Networks from Overfitting</li>
<li>Batch Normalization- Accelerating Deep Network Training by Reducing Internal Covariate Shift</li>
</ul>
<h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><ul>
<li>Generating Sequences With Recurrent Neural Networks</li>
<li>Word embedding</li>
<li>Distributed Representations of Words and Phrases and their Compositionality</li>
</ul>
<h2 id="Image-captioning"><a href="#Image-captioning" class="headerlink" title="Image captioning"></a>Image captioning</h2><p>Show and Tell: A Neural Image Caption Generator<br>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</p>

        </div>

        <blockquote class="post-copyright">
    <div class="content">
        
<span class="post-time">
    最後更新時間：<time datetime="2018-01-10T09:08:29.000Z" itemprop="dateUpdated">2018-01-10 17:08:29</time>
</span><br>


        
    </div>
    <footer>
        <a href="https://world4jason.github.io">
            <img src="/img/avatar.jpg" alt="Jason Yeh">
            Jason Yeh
        </a>
    </footer>
</blockquote>

        


        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Computer-Vision/">Computer Vision</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Paper-List/">Paper List</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Self-Study/">Self-Study</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://world4jason.github.io/2018/01/09/Deep-Learning-Recommended-Papers/&title=《Deep Learning and Computer Vision Recommended Paper》 — world4jason&pic=https://world4jason.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://world4jason.github.io/2018/01/09/Deep-Learning-Recommended-Papers/&title=《Deep Learning and Computer Vision Recommended Paper》 — world4jason&source=菜鳥搬磚日常" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://world4jason.github.io/2018/01/09/Deep-Learning-Recommended-Papers/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《Deep Learning and Computer Vision Recommended Paper》 — world4jason&url=https://world4jason.github.io/2018/01/09/Deep-Learning-Recommended-Papers/&via=https://world4jason.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://world4jason.github.io/2018/01/09/Deep-Learning-Recommended-Papers/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between flex-row-reverse">
  

  
    <div class="waves-block waves-effect next">
      <a href="/2018/01/02/person_re-ID_summary/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">Person re-ID Summary</h4>
      </a>
    </div>
  
</nav>



    


<section class="comments" id="comments">
    <div id="disqus_thread"></div>
    <script>
    var disqus_shortname = 'true';
    lazyScripts.push('//' + disqus_shortname + '.disqus.com/embed.js')
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>




<section class="comments" id="comments">
    <!-- UY BEGIN -->
    <div id="uyan_frame"></div>
    <script src="http://v2.uyan.cc/code/uyan.js?uid=true"></script>
    <!-- UY END -->
</section>




<section class="comments" id="comments">
    <div id="gitment_thread"></div>
    <link rel="stylesheet" href="//unpkg.com/gitment/style/default.css">
    <script src="//unpkg.com/gitment/dist/gitment.browser.js"></script>
    <script>
        var gitment = new Gitment({
            owner: '',
            repo: '',
            oauth: {
                client_id: '',
                client_secret: '',
            },
        })
        gitment.render('comments')
    </script>
</section>




</article>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        總訪客數：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        總訪問量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>本部落格係採用<a rel="license" href="https://creativecommons.org/licenses/by/4.0/deed.zh_TW">創用 CC 姓名標示 4.0 國際 授權條款授權</a></span>
        </p>
    </div>
    <div class="bottom">
        <p><span>Jason Yeh &copy; 2015 - 2018</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://world4jason.github.io/2018/01/09/Deep-Learning-Recommended-Papers/&title=《Deep Learning and Computer Vision Recommended Paper》 — world4jason&pic=https://world4jason.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://world4jason.github.io/2018/01/09/Deep-Learning-Recommended-Papers/&title=《Deep Learning and Computer Vision Recommended Paper》 — world4jason&source=菜鳥搬磚日常" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://world4jason.github.io/2018/01/09/Deep-Learning-Recommended-Papers/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《Deep Learning and Computer Vision Recommended Paper》 — world4jason&url=https://world4jason.github.io/2018/01/09/Deep-Learning-Recommended-Papers/&via=https://world4jason.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://world4jason.github.io/2018/01/09/Deep-Learning-Recommended-Papers/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="//api.qrserver.com/v1/create-qr-code/?data=https://world4jason.github.io/2018/01/09/Deep-Learning-Recommended-Papers/" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: false };


</script>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/main.min.js"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/search.min.js" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = '持續搬磚中';
            clearTimeout(titleTime);
        } else {
            document.title = '(つェ⊂)咦!又回來了!';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



</body>
</html>
