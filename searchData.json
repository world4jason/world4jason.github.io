[{"title":"Mask RCNNCode Reading - Detection Layer","url":"/2018/06/04/Mask-RCNN-Code-Reading-DetectionLayer/","content":"\n\n\n這邊其實主要是因為程式碼撰寫方便 所以才拆成Detection Target Layer與Detection Layer 前者是在訓練時用到的,後者是在測試時用的, \n\nDetection Layer\n\n## refine_detections_graph\n根據rois和probs(每個ROI都有總類別個數的probs)和deltas進行檢測的優化，得到固定數量的優化目標。  \n\n```python\ndef refine_detections_graph(rois, probs, deltas, window, config):\n    \"\"\"Refine classified proposals and filter overlaps and return final\n    detections.\n    # 輸入為N個rois、N個具有num_classes的probs，scores由probs得出 \n    Inputs:\n        rois: [N, (y1, x1, y2, x2)] in normalized coordinates\n        probs: [N, num_classes]. Class probabilities.\n        deltas: [N, num_classes, (dy, dx, log(dh), log(dw))]. Class-specific\n                bounding box deltas.\n        window: (y1, x1, y2, x2) in image coordinates. The part of the image\n            that contains the image excluding the padding.\n\n    Returns detections shaped: [N, (y1, x1, y2, x2, class_id, score)] where\n        coordinates are normalized.\n    \"\"\"\n    # Class IDs per ROI\n    class_ids = tf.argmax(probs, axis=1, output_type=tf.int32)\n    # Class probability of the top class of each ROI\n    indices = tf.stack([tf.range(probs.shape[0]), class_ids], axis=1)\n    class_scores = tf.gather_nd(probs, indices)\n    # Class-specific bounding box deltas\n    deltas_specific = tf.gather_nd(deltas, indices)\n    # Apply bounding box deltas\n    # Shape: [boxes, (y1, x1, y2, x2)] in normalized coordinates\n    refined_rois = apply_box_deltas_graph(\n        rois, deltas_specific * config.BBOX_STD_DEV)\n    # Clip boxes to image window\n    refined_rois = clip_boxes_graph(refined_rois, window)\n\n    # TODO: Filter out boxes with zero area\n\n    # Filter out background boxes\n    keep = tf.where(class_ids > 0)[:, 0]\n    # Filter out low confidence boxes\n    if config.DETECTION_MIN_CONFIDENCE:\n        conf_keep = tf.where(class_scores >= config.DETECTION_MIN_CONFIDENCE)[:, 0]\n        keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),\n                                        tf.expand_dims(conf_keep, 0))\n        keep = tf.sparse_tensor_to_dense(keep)[0]\n    \n    # 留下既滿足是前景又滿足scores大於MIN_CONFIDENCE的  \n    # Apply per-class NMS\n    # 1. Prepare variables\n    pre_nms_class_ids = tf.gather(class_ids, keep)\n    pre_nms_scores = tf.gather(class_scores, keep)\n    pre_nms_rois = tf.gather(refined_rois,   keep)\n    unique_pre_nms_class_ids = tf.unique(pre_nms_class_ids)[0]\n\n    def nms_keep_map(class_id):\n        \"\"\"Apply Non-Maximum Suppression on ROIs of the given class.\"\"\"\n        # Indices of ROIs of the given class\n        ixs = tf.where(tf.equal(pre_nms_class_ids, class_id))[:, 0]\n        # Apply NMS\n        class_keep = tf.image.non_max_suppression(\n                tf.gather(pre_nms_rois, ixs),\n                tf.gather(pre_nms_scores, ixs),\n                max_output_size=config.DETECTION_MAX_INSTANCES,\n                iou_threshold=config.DETECTION_NMS_THRESHOLD)\n        # Map indicies\n        class_keep = tf.gather(keep, tf.gather(ixs, class_keep))\n        # Pad with -1 so returned tensors have the same shape\n        gap = config.DETECTION_MAX_INSTANCES - tf.shape(class_keep)[0]\n        class_keep = tf.pad(class_keep, [(0, gap)],\n                            mode='CONSTANT', constant_values=-1)\n        # Set shape so map_fn() can infer result shape\n        class_keep.set_shape([config.DETECTION_MAX_INSTANCES])\n        return class_keep\n\n    # 2. Map over class IDs\n    # 在nms_keep_map內分類別的進行NMS。 \n    nms_keep = tf.map_fn(nms_keep_map, unique_pre_nms_class_ids,\n                         dtype=tf.int64)\n    # 3. Merge results into one list, and remove -1 padding\n    nms_keep = tf.reshape(nms_keep, [-1])\n    nms_keep = tf.gather(nms_keep, tf.where(nms_keep > -1)[:, 0])\n    # 4. Compute intersection between keep and nms_keep\n    keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),\n                                    tf.expand_dims(nms_keep, 0))\n    keep = tf.sparse_tensor_to_dense(keep)[0]\n    # Keep top detections\n    roi_count = config.DETECTION_MAX_INSTANCES\n    class_scores_keep = tf.gather(class_scores, keep)\n    num_keep = tf.minimum(tf.shape(class_scores_keep)[0], roi_count)\n    top_ids = tf.nn.top_k(class_scores_keep, k=num_keep, sorted=True)[1]\n    keep = tf.gather(keep, top_ids)\n\n    # Arrange output as [N, (y1, x1, y2, x2, class_id, score)]\n    # Coordinates are normalized.\n    detections = tf.concat([\n        tf.gather(refined_rois, keep),\n        tf.to_float(tf.gather(class_ids, keep))[..., tf.newaxis],\n        tf.gather(class_scores, keep)[..., tf.newaxis]\n        ], axis=1)\n\n    # Pad with zeros if detections < DETECTION_MAX_INSTANCES\n    gap = config.DETECTION_MAX_INSTANCES - tf.shape(detections)[0]\n    detections = tf.pad(detections, [(0, gap), (0, 0)], \"CONSTANT\")\n    return detections\n```\n\n\n```python\nclass DetectionLayer(KE.Layer):\n    \"\"\"Takes classified proposal boxes and their bounding box deltas and\n    returns the final detection boxes.\n\n    Returns:\n    [batch, num_detections, (y1, x1, y2, x2, class_id, class_score)] where\n    coordinates are normalized.\n    \"\"\"\n\n    def __init__(self, config=None, **kwargs):\n        super(DetectionLayer, self).__init__(**kwargs)\n        self.config = config\n\n    def call(self, inputs):\n        rois = inputs[0]\n        mrcnn_class = inputs[1]\n        mrcnn_bbox = inputs[2]\n        image_meta = inputs[3]\n\n        # Get windows of images in normalized coordinates. Windows are the area\n        # in the image that excludes the padding.\n        # Use the shape of the first image in the batch to normalize the window\n        # because we know that all images get resized to the same size.\n        m = parse_image_meta_graph(image_meta)\n        image_shape = m['image_shape'][0]\n        window = norm_boxes_graph(m['window'], image_shape[:2])\n        \n        # Run detection refinement graph on each item in the batch\n        detections_batch = utils.batch_slice(\n            [rois, mrcnn_class, mrcnn_bbox, window],\n            lambda x, y, w, z: refine_detections_graph(x, y, w, z, self.config),\n            self.config.IMAGES_PER_GPU)\n\n        # Reshape output\n        # [batch, num_detections, (y1, x1, y2, x2, class_score)] in\n        # normalized coordinates\n        return tf.reshape(\n            detections_batch,\n            [self.config.BATCH_SIZE, self.config.DETECTION_MAX_INSTANCES, 6])\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.config.DETECTION_MAX_INSTANCES, 6)\n```\n\n"},{"title":"Batch Normalization and Group Normalization","url":"/2018/06/04/Batch-Normalization-and-Group-Normalization/","content":"\n\n#Batch Normalization   \nBatch Normalization 在深度學習上算是不可或缺的一部分，基本上所有的框架中都會用到它，我記得比較清楚的是，在YOLOV2中作者採用了Batch Normalization 從而提高了4個百分點的Map吧。\n\n##為何要提出Batch Normalization？\n在每次給network輸入數據時，都需要進行預處理，比如歸一化之類的，為什麼需要歸一化呢？神經網絡學習過程本質就是為了學習數據分佈，一旦訓練數據與測試數據的分佈不同，那麼網絡的泛化能力也大大降低；另外一方面，一旦每批訓練數據的分佈各不相同(batch 梯度下降)，那麼網絡就要在每次迭代都去學習適應不同的分佈，這樣將會大大降低網絡的訓練速度，這也正是為什麼我們需要對數據都要做一個歸一化預處理的原因。\n\n而且在訓練的過程中，經過一層層的網絡運算，中間層的學習到的數據分佈也是發生著挺大的變化，這就要求我們必須使用一個很小的學習率和對參數很好的初始化，但是這麼做會讓訓練過程變得慢而且複雜m在論文中，這種現象被稱為Internal Covariate Shift。為瞭解決這個問題，作者提出了Batch Normalization。\n\n##Batch Normalization原理\n為了降低Internal Covariate Shift帶來的影響，其實只要進行歸一化就可以的。比如，我們把network每一層的輸出都整為方差為1，均值為0的正態分佈，這樣看起來是可以解決問題，但是想想，network好不容易學習到的數據特徵，被你這樣一弄又回到瞭解放前了，相當於沒有學習了。所以這樣是不行的，大神想到了一個大招：變換重構，引入了兩個可以學習的參數γ、β，當然，這也是算法的靈魂所在：\n\n具體的算法流程如下：\n\n![](/media/15280580743313.jpg)\n\n\n Batch Normalization 是對一個batch來進行normalization的，例如我們的輸入的一個batch為：β=x_(1...m)，輸出為：y_i=BN(x)。具體的完整流程如下：\n\n1.求出該batch數據x的均值\n\n![](/media/15280580994379.jpg)\n2.求出該batch數據的方差\n![](/media/15280581211439.jpg)\n3.對輸入數據x做歸一化處理，得到：\n![](/media/15280581373757.jpg)\n\n4.最後加入可訓練的兩個參數：縮放變量γ和平移變量β，計算歸一化後的值：\n![](/media/15280581605011.jpg)\n加入了這兩個參數之後，網絡就可以更加容易的學習到更多的東西了。先想想極端的情況，當縮放變量γ和平移變量β分別等於batch數據的方差和均值時，最後得到的yi就和原來的xi一模一樣了，相當於batch normalization沒有起作用了。這樣就保證了每一次數據經過歸一化後還保留的有學習來的特徵，同時又能完成歸一化這個操作，加速訓練。\n\n引入參數的更新過程，也就是微積分的Chain Rule：\n![](/media/15280581942881.jpg)\n\nExample\n\n```python\ndef Batchnorm_simple_for_train(x, gamma,beta, bn_param):\"\"\"  \nparam:x   : 輸入數據，設shape(B,L)  \nparam:gama : 縮放因子  γ  \nparam:beta : 平移因子  β  \nparam:bn_param   : batchnorm所需要的一些參數  \n   eps      : 接近0的數，防止分母出現0  \n   momentum : 動量參數，一般為0.9，0.99， 0.999  \n   running_mean ：滑動平均的方式計算新的均值，訓練時計算，為測試數據做準備  \n   running_var  : 滑動平均的方式計算新的方差，訓練時計算，為測試數據做準備  \n\"\"\"  \n   running_mean = bn_param['running_mean'] #shape = [B]  \n   running_var = bn_param['running_var']   #shape = [B]  \n   results = 0. # 建立一個新的變量  \n   x_mean=x.mean(axis=0)  # 計算x的均值  \n   x_var=x.var(axis=0)    # 計算方差  \n   x_normalized=(x-x_mean)/np.sqrt(x_var+eps)       # 歸一化  \n   results = gamma * x_normalized + beta            # 縮放平移  \n   running_mean = momentum * running_mean + (1 - momentum) * x_mean  \n   running_var = momentum * running_var + (1 - momentum) * x_var    #記錄新的值  \n   bn_param['running_mean'] = running_mean  \n   bn_param['running_var'] = running_var     \n   return results , bn_param \n```\n\n這份code首先計算均值和方差，然後歸一化，然後縮放和平移就結束了！但是這是在訓練中完成的任務，每次訓練給一個批量，然後計算批量的均值方差，但是在測試的時候可不是這樣，測試的時候每次只輸入一張圖片，這怎麼計算批量的均值和方差，於是，就有了代碼中下面兩行，在訓練的時候實現計算好mean var測試的時候直接拿來用就可以了，不用計算均值和方差。\n\n```python\nrunning_mean = momentum * running_mean + (1- momentum) * x_mean  \nrunning_var = momentum * running_var + (1 -momentum) * x_var  \n```\n\n所以，測試的時候是這樣的：\n\n```python\ndef Batchnorm_simple_for_test(x, gamma,beta, bn_param):\"\"\"  \nparam:x   : 輸入數據，設shape(B,L)  \nparam:gama : 縮放因子  γ  \nparam:beta : 平移因子  β  \nparam:bn_param   : batchnorm所需要的一些參數  \n   eps      : 接近0的數，防止分母出現0  \n   momentum : 動量參數，一般為0.9，0.99， 0.999  \n   running_mean ：滑動平均的方式計算新的均值，訓練時計算，為測試數據做準備  \n   running_var  : 滑動平均的方式計算新的方差，訓練時計算，為測試數據做準備  \n\"\"\"  \n   running_mean = bn_param['running_mean'] #shape = [B]  \n   running_var = bn_param['running_var']   #shape = [B]  \n   results = 0. # 建立一個新的變量  \n   x_normalized=(x-running_mean )/np.sqrt(running_var +eps)       # 歸一化  \n   results = gamma * x_normalized + beta            # 縮放平移  \n   return results , bn_param  \n```\n\n整個過程還是很順的，很好理解的。這部分的內容摘抄自微信公眾號：機器學習算法工程師。一個很好的公眾號，推薦一波。\n\nBatch Normalization 的TensorFlow 源碼解讀，來自知乎：\n\n```python\ndef batch_norm_layer(x, train_phase,scope_bn):  \n   with tf.variable_scope(scope_bn):  \n        # 新建兩個變量，平移、縮放因子  \n       beta = tf.Variable(tf.constant(0.0, shape=[x.shape[-1]]), name='beta',trainable=True)  \n       gamma = tf.Variable(tf.constant(1.0, shape=[x.shape[-1]]), name='gamma',trainable=True)  \n       # 計算此次批量的均值和方差  \n       axises = np.arange(len(x.shape) - 1)  \n       batch_mean, batch_var = tf.nn.moments(x, axises, name='moments')  \n       # 滑動平均做衰減  \n       ema = tf.train.ExponentialMovingAverage(decay=0.5)  \n       def mean_var_with_update():  \n           ema_apply_op = ema.apply([batch_mean, batch_var])  \n           with tf.control_dependencies([ema_apply_op]):  \n                return tf.identity(batch_mean),tf.identity(batch_var)  \n       # train_phase 訓練還是測試的flag  \n       # 訓練階段計算runing_mean和runing_var，使用mean_var_with_update（）函數  \n       # 測試的時候直接把之前計算的拿去用 ema.average(batch_mean)  \n       mean, var = tf.cond(train_phase, mean_var_with_update,  \n                            lambda:(ema.average(batch_mean), ema.average(batch_var)))  \n       normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)  \n   return normed  \n```\n至於此行代碼tf.nn.batch_normalization()就是簡單的計算batchnorm過程，這個函數所實現的功能就如此公式：\n\n![](/media/15280583334189.jpg)\n\n```python\ndef batch_normalization(x, mean, variance, offset,scale, variance_epsilon, name=None):  \n   with ops.name_scope(name, \"batchnorm\", [x, mean, variance,scale, offset]):  \n       inv = math_ops.rsqrt(variance + variance_epsilon)  \n    if scale is not None:  \n           inv *= scale        \n       return x * inv + (offset - mean * inv  \n                       if offset is not Noneelse -mean * inv)  \n```\n\n##Batch Normalization的帶來的優勢：\n\n沒有它之前，需要小心的調整學習率和權重初始化，但是有了BN可以放心的使用大學習率，但是使用了BN，就不用小心的調參了，較大的學習率極大的提高了學習速度，\n\nBatchnorm本身上也是一種正則的方式，可以代替其他正則方式如dropout等\n\n另外，個人認為，batchnorm降低了數據之間的絕對差異，有一個去相關的性質，更多的考慮相對差異性，因此在分類任務上具有更好的效果\n\n\n#Group Normalization\ngroup normalization是2018年3月份何愷明大神的又一力作，優化了batch normalization在比較小的batch size 情況下表現不太好的劣勢。批量維度進行歸一化會帶來一些問題——批量統計估算不準確導致批量變小時，BN 的誤差會迅速增加。在訓練大型網絡和將特徵轉移到計算機視覺任務中（包括檢測、分割和視頻），內存消耗限制了只能使用小批量的BN。尤其是在我的破電腦裡面，batch的大小一般都是使用的1，相當於不存在BN。\n\n下圖是論文中給出BN和GN的對比：\n\n![](/media/15280584129198.jpg)\n  可以看出在bath size比較小的情況下，BN的性能十分地差，而GN的性能基本上沒有太大改變。\n\n##Group Normalization 原理：\n先給出他目前出現比較多的幾種normalization的示意圖：\n\n![](/media/15280584563207.jpg)\nBatchNorm：batch方向做歸一化，算N*H*W的均值\n\nLayerNorm：channel方向做歸一化，算C*H*W的均值\n\nInstanceNorm：一個channel內做歸一化，算H*W的均值\n\nGroupNorm：將channel方向分group，然後每個group內做歸一化，算(C//G)*H*W的均值\n\n從示意圖中看，也可以看出其實沒有太大的變化，所以代碼中也沒有需要太大的變動，只需要稍微修改一下就好了。\n\nGN程式碼範例：\n\n```python\ndef GroupNorm(x,G=16,eps=1e-5):    \n    N,H,W,C=x.shape         \n    x=tf.reshape(x,[tf.cast(N,tf.int32),tf.cast(H,tf.int32),tf.cast(W,tf.int32),tf.cast(G,tf.int32),tf.cast(C//G,tf.int32)])    \n    mean,var=tf.nn.moments(x,[1,2,4],keep_dims=True)    \n    x=(x-mean)/tf.sqrt(var+eps)    \n    x=tf.reshape(x,[tf.cast(N,tf.int32),tf.cast(H,tf.int32),tf.cast(W,tf.int32),tf.cast(C,tf.int32)])    \n    gamma = tf.Variable(tf.ones(shape=[1,1,1,tf.cast(C,tf.int32)]), name=\"gamma\")    \n    beta = tf.Variable(tf.zeros(shape=[1,1,1,tf.cast(C,tf.int32)]), name=\"beta\")    \n    return x*gamma+beta    \n\n```\n\n## Group Normalization in Keras\n其實也是在keras中的BatchNormalization層上進行一定的修改就得到了GroupNormalization層。正常和batchnormalization一樣的調用即可。但注意需要保持channel數是group的整數倍。\n\n\n```python\nfrom keras.engine import Layer, InputSpec\nfrom keras import initializers\nfrom keras import regularizers\nfrom keras import constraints\nfrom keras import backend as K\n\nfrom keras.utils.generic_utils import get_custom_objects\n\n\nclass GroupNormalization(Layer):\n    \"\"\"Group normalization layer\n\n    Group Normalization divides the channels into groups and computes within each group\n    the mean and variance for normalization. GN's computation is independent of batch sizes,\n    and its accuracy is stable in a wide range of batch sizes\n\n    # Arguments\n        groups: Integer, the number of groups for Group Normalization.\n        axis: Integer, the axis that should be normalized\n            (typically the features axis).\n            For instance, after a `Conv2D` layer with\n            `data_format=\"channels_first\"`,\n            set `axis=1` in `BatchNormalization`.\n        epsilon: Small float added to variance to avoid dividing by zero.\n        center: If True, add offset of `beta` to normalized tensor.\n            If False, `beta` is ignored.\n        scale: If True, multiply by `gamma`.\n            If False, `gamma` is not used.\n            When the next layer is linear (also e.g. `nn.relu`),\n            this can be disabled since the scaling\n            will be done by the next layer.\n        beta_initializer: Initializer for the beta weight.\n        gamma_initializer: Initializer for the gamma weight.\n        beta_regularizer: Optional regularizer for the beta weight.\n        gamma_regularizer: Optional regularizer for the gamma weight.\n        beta_constraint: Optional constraint for the beta weight.\n        gamma_constraint: Optional constraint for the gamma weight.\n\n    # Input shape\n        Arbitrary. Use the keyword argument `input_shape`\n        (tuple of integers, does not include the samples axis)\n        when using this layer as the first layer in a model.\n\n    # Output shape\n        Same shape as input.\n\n    # References\n        - [Group Normalization](https://arxiv.org/abs/1803.08494)\n    \"\"\"\n\n    def __init__(self,\n                 groups=32,\n                 axis=-1,\n                 epsilon=1e-5,\n                 center=True,\n                 scale=True,\n                 beta_initializer='zeros',\n                 gamma_initializer='ones',\n                 beta_regularizer=None,\n                 gamma_regularizer=None,\n                 beta_constraint=None,\n                 gamma_constraint=None,\n                 **kwargs):\n        super(GroupNormalization, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.groups = groups\n        self.axis = axis\n        self.epsilon = epsilon\n        self.center = center\n        self.scale = scale\n        self.beta_initializer = initializers.get(beta_initializer)\n        self.gamma_initializer = initializers.get(gamma_initializer)\n        self.beta_regularizer = regularizers.get(beta_regularizer)\n        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n        self.beta_constraint = constraints.get(beta_constraint)\n        self.gamma_constraint = constraints.get(gamma_constraint)\n\n    def build(self, input_shape):\n        dim = input_shape[self.axis]\n\n        if dim is None:\n            raise ValueError('Axis ' + str(self.axis) + ' of '\n                                                        'input tensor should have a defined dimension '\n                                                        'but the layer received an input with shape ' +\n                             str(input_shape) + '.')\n\n        if dim < self.groups:\n            raise ValueError('Number of groups (' + str(self.groups) + ') cannot be '\n                                                                       'more than the number of channels (' +\n                             str(dim) + ').')\n\n        if dim % self.groups != 0:\n            raise ValueError('Number of groups (' + str(self.groups) + ') must be a '\n                                                                       'multiple of the number of channels (' +\n                             str(dim) + ').')\n\n        self.input_spec = InputSpec(ndim=len(input_shape),\n                                    axes={self.axis: dim})\n        shape = (dim,)\n\n        if self.scale:\n            self.gamma = self.add_weight(shape=shape,\n                                         name='gamma',\n                                         initializer=self.gamma_initializer,\n                                         regularizer=self.gamma_regularizer,\n                                         constraint=self.gamma_constraint)\n        else:\n            self.gamma = None\n        if self.center:\n            self.beta = self.add_weight(shape=shape,\n                                        name='beta',\n                                        initializer=self.beta_initializer,\n                                        regularizer=self.beta_regularizer,\n                                        constraint=self.beta_constraint)\n        else:\n            self.beta = None\n        self.built = True\n\n    def call(self, inputs, **kwargs):\n        input_shape = K.int_shape(inputs)\n        # Prepare broadcasting shape.\n        ndim = len(input_shape)\n        reduction_axes = list(range(len(input_shape)))\n        del reduction_axes[self.axis]\n        broadcast_shape = [1] * len(input_shape)\n        broadcast_shape[self.axis] = input_shape[self.axis]\n\n        reshape_group_shape = list(input_shape)\n        reshape_group_shape[self.axis] = input_shape[self.axis] // self.groups\n        group_shape = [-1, self.groups]\n        group_shape.extend(reshape_group_shape[1:])\n        group_reduction_axes = list(range(len(group_shape)))\n\n        # Determines whether broadcasting is needed.\n        needs_broadcasting = (sorted(reduction_axes) != list(range(ndim))[:-1])\n\n        inputs = K.reshape(inputs, group_shape)\n\n        mean = K.mean(inputs, axis=group_reduction_axes[2:], keepdims=True)\n        variance = K.var(inputs, axis=group_reduction_axes[2:], keepdims=True)\n\n        inputs = (inputs - mean) / (K.sqrt(variance + self.epsilon))\n\n        original_shape = [-1] + list(input_shape[1:])\n        inputs = K.reshape(inputs, original_shape)\n\n        if needs_broadcasting:\n            outputs = inputs\n\n            # In this case we must explicitly broadcast all parameters.\n            if self.scale:\n                broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n                outputs = outputs * broadcast_gamma\n\n            if self.center:\n                broadcast_beta = K.reshape(self.beta, broadcast_shape)\n                outputs = outputs + broadcast_beta\n        else:\n            outputs = inputs\n\n            if self.scale:\n                outputs = outputs * self.gamma\n\n            if self.center:\n                outputs = outputs + self.beta\n\n        return outputs\n\n    def get_config(self):\n        config = {\n            'groups': self.groups,\n            'axis': self.axis,\n            'epsilon': self.epsilon,\n            'center': self.center,\n            'scale': self.scale,\n            'beta_initializer': initializers.serialize(self.beta_initializer),\n            'gamma_initializer': initializers.serialize(self.gamma_initializer),\n            'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n            'beta_constraint': constraints.serialize(self.beta_constraint),\n            'gamma_constraint': constraints.serialize(self.gamma_constraint)\n        }\n        base_config = super(GroupNormalization, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n```\n\n"},{"title":"Mask RCNNCode Reading for ROI Align Layer","url":"/2018/06/03/Mask-RCNN-Code-Reading-ROI_Align/","content":"\n\n##  ROIAlign Layer\n\n這class在兩個地方會被呼叫到\n\n```python\ndef build_fpn_mask_graph(rois, feature_maps, image_meta,\n                         pool_size, num_classes, train_bn=True):\n\n    # ROI Pooling\n    # Shape: [batch, num_boxes, pool_height, pool_width, channels]\n    x = PyramidROIAlign([pool_size, pool_size],\n                        name=\"roi_align_mask\")([rois, image_meta] + feature_maps)\n```\n\n```python\ndef fpn_classifier_graph(rois, feature_maps, image_meta,\n                         pool_size, num_classes, train_bn=True):\n\n    # ROI Pooling\n    # Shape: [batch, num_boxes, pool_height, pool_width, channels]\n    x = PyramidROIAlign([pool_size, pool_size],\n                        name=\"roi_align_classifier\")([rois, image_meta] + feature_maps)\n\n```\n\nROI獲得的可能會是從P2~P5, 而他們基本上anchors也不太依樣\n\n![](/media/15280551744304.jpg)\n功能型Layer, log2_graph存粹是因為tf 沒有這項功能所以只好自己寫一個.\n\n\n```python\n\ndef log2_graph(x):\n    \"\"\"Implementatin of Log2. TF doesn't have a native implemenation.\"\"\"\n    return tf.log(x) / tf.log(2.0)\n\n\nclass PyramidROIAlign(KE.Layer):\n    \"\"\"Implements ROI Pooling on multiple levels of the feature pyramid.\n\n    Params:\n    - pool_shape: [height, width] of the output pooled regions. Usually [7, 7]\n\n    Inputs:\n    - boxes: [batch, num_boxes, (y1, x1, y2, x2)] in normalized\n             coordinates. Possibly padded with zeros if not enough\n             boxes to fill the array.\n    - image_meta: [batch, (meta data)] Image details. See compose_image_meta()\n    - Feature maps: List of feature maps from different levels of the pyramid.\n                    Each is [batch, height, width, channels]\n\n    Output:\n    Pooled regions in the shape: [batch, num_boxes, height, width, channels].\n    The width and height are those specific in the pool_shape in the layer\n    constructor.\n    \"\"\"\n\n    def __init__(self, pool_shape, **kwargs):\n        super(PyramidROIAlign, self).__init__(**kwargs)\n        self.pool_shape = tuple(pool_shape)\n\n    def call(self, inputs):\n        # Crop boxes [batch, num_boxes, (y1, x1, y2, x2)] in normalized coords\n        # inputs[0 is ROIs]\n        boxes = inputs[0]\n\n        # Image meta\n        # Holds details about the image. See compose_image_meta()\n        image_meta = inputs[1]\n\n        # Feature Maps. List of feature maps from different level of the\n        # feature pyramid. Each is [batch, height, width, channels]\n        feature_maps = inputs[2:]\n\n        # Assign each ROI to a level in the pyramid based on the ROI area.\n        y1, x1, y2, x2 = tf.split(boxes, 4, axis=2)\n        h = y2 - y1\n        w = x2 - x1\n        # Use shape of first image. Images in a batch must have the same size.\n        image_shape = parse_image_meta_graph(image_meta)['image_shape'][0]\n        # Equation 1 in the Feature Pyramid Networks paper. Account for\n        # the fact that our coordinates are normalized here.\n        # e.g. a 224x224 ROI (in pixels) maps to P4\n        image_area = tf.cast(image_shape[0] * image_shape[1], tf.float32)\n        roi_level = log2_graph(tf.sqrt(h * w) / (224.0 / tf.sqrt(image_area)))\n        roi_level = tf.minimum(5, tf.maximum(\n            2, 4 + tf.cast(tf.round(roi_level), tf.int32)))\n        roi_level = tf.squeeze(roi_level, 2)\n\n        # Loop through levels and apply ROI pooling to each. P2 to P5.\n        pooled = []\n        box_to_level = []\n        for i, level in enumerate(range(2, 6)):\n            ##應該是一個二維的array，存儲這哪一層的哪些box的indicies  \n            ix = tf.where(tf.equal(roi_level, level))\n            level_boxes = tf.gather_nd(boxes, ix)\n\n            # Box indicies for crop_and_resize.\n            box_indices = tf.cast(ix[:, 0], tf.int32)\n\n            # Keep track of which box is mapped to which level\n            ##應該是一個二維的array，存儲這哪一層的哪些box的indicies  \n\n            box_to_level.append(ix)\n\n            # Stop gradient propogation to ROI proposals\n            level_boxes = tf.stop_gradient(level_boxes)\n            box_indices = tf.stop_gradient(box_indices)\n\n            # Crop and Resize\n            # From Mask R-CNN paper: \"We sample four regular locations, so\n            # that we can evaluate either max or average pooling. In fact,\n            # interpolating only a single value at each bin center (without\n            # pooling) is nearly as effective.\"\n            #\n            # Here we use the simplified approach of a single value per bin,\n            # which is how it's done in tf.crop_and_resize()\n            # Result: [batch * num_boxes, pool_height, pool_width, channels]\n            # 因為插值一個點和四個點的性能影響不大故插一個點 \n            pooled.append(tf.image.crop_and_resize(\n                feature_maps[i], level_boxes, box_indices, self.pool_shape,\n                method=\"bilinear\"))\n\n        # Pack pooled features into one tensor\n        pooled = tf.concat(pooled, axis=0)\n\n        # Pack box_to_level mapping into one array and add another\n        # column representing the order of pooled boxes\n        box_to_level = tf.concat(box_to_level, axis=0)\n        box_range = tf.expand_dims(tf.range(tf.shape(box_to_level)[0]), 1)\n        box_to_level = tf.concat([tf.cast(box_to_level, tf.int32), box_range],\n                                 axis=1)\n\n        # Rearrange pooled features to match the order of the original boxes\n        # Sort box_to_level by batch then box index\n        # TF doesn't have a way to sort by two columns, so merge them and sort.\n        sorting_tensor = box_to_level[:, 0] * 100000 + box_to_level[:, 1]\n        ix = tf.nn.top_k(sorting_tensor, k=tf.shape(\n            box_to_level)[0]).indices[::-1]\n        ix = tf.gather(box_to_level[:, 2], ix)\n        pooled = tf.gather(pooled, ix)\n\n        # Re-add the batch dimension\n        pooled = tf.expand_dims(pooled, 0)\n        return pooled\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0][:2] + self.pool_shape + (input_shape[2][-1], )\n```\n\n"},{"title":"Mask RCNNCode Reading - Detection Target Layer","url":"/2018/06/03/Mask-RCNN-Code-Reading-DetectionTargetLayer/","content":"\nDetection Target Layer與Detection Layer 前者是在訓練時用到的\n後者是在測試時用的\n\n其實對於inference來說,Proposal_Layer已經拿到要的結果了, DetectionTargetLayer最主要是訓練用途需要 所以會有ground truth (gt_box, gt_class_id, gt_mask)\n\n### overlaps_graph\n這邊顧名思義是計算IOU用的,\n\n```python\ndef overlaps_graph(boxes1, boxes2):\n    \"\"\"Computes IoU overlaps between two sets of boxes.\n    boxes1, boxes2: [N, (y1, x1, y2, x2)].\n    \"\"\"\n    # 1. Tile boxes2 and repeat boxes1. This allows us to compare\n    # every boxes1 against every boxes2 without loops.\n    # TF doesn't have an equivalent to np.repeat() so simulate it\n    # using tf.tile() and tf.reshape.\n    b1 = tf.reshape(tf.tile(tf.expand_dims(boxes1, 1),\n                            [1, 1, tf.shape(boxes2)[0]]), [-1, 4])\n    b2 = tf.tile(boxes2, [tf.shape(boxes1)[0], 1])\n    # 2. Compute intersections\n    b1_y1, b1_x1, b1_y2, b1_x2 = tf.split(b1, 4, axis=1)\n    b2_y1, b2_x1, b2_y2, b2_x2 = tf.split(b2, 4, axis=1)\n    y1 = tf.maximum(b1_y1, b2_y1)\n    x1 = tf.maximum(b1_x1, b2_x1)\n    y2 = tf.minimum(b1_y2, b2_y2)\n    x2 = tf.minimum(b1_x2, b2_x2)\n    intersection = tf.maximum(x2 - x1, 0) * tf.maximum(y2 - y1, 0)\n    # 3. Compute unions\n    b1_area = (b1_y2 - b1_y1) * (b1_x2 - b1_x1)\n    b2_area = (b2_y2 - b2_y1) * (b2_x2 - b2_x1)\n    union = b1_area + b2_area - intersection\n    # 4. Compute IoU and reshape to [boxes1, boxes2]\n    iou = intersection / union\n    overlaps = tf.reshape(iou, [tf.shape(boxes1)[0], tf.shape(boxes2)[0]])\n    return overlaps\n\n```\n### Detection Target Layer\n\n\n```python\nclass DetectionTargetLayer(KE.Layer):\n    \"\"\"Subsamples proposals and generates target box refinement, class_ids,\n    and masks for each.\n\n    Inputs:\n    proposals: [batch, N, (y1, x1, y2, x2)] in normalized coordinates. Might\n               be zero padded if there are not enough proposals.\n    gt_class_ids: [batch, MAX_GT_INSTANCES] Integer class IDs.\n    gt_boxes: [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)] in normalized\n              coordinates.\n    gt_masks: [batch, height, width, MAX_GT_INSTANCES] of boolean type\n\n    Returns: Target ROIs and corresponding class IDs, bounding box shifts,\n    and masks.\n    rois: [batch, TRAIN_ROIS_PER_IMAGE, (y1, x1, y2, x2)] in normalized\n          coordinates\n    target_class_ids: [batch, TRAIN_ROIS_PER_IMAGE]. Integer class IDs.\n    target_deltas: [batch, TRAIN_ROIS_PER_IMAGE, NUM_CLASSES,\n                    (dy, dx, log(dh), log(dw), class_id)]\n                   Class-specific bbox refinements.\n    target_mask: [batch, TRAIN_ROIS_PER_IMAGE, height, width)\n                 Masks cropped to bbox boundaries and resized to neural\n                 network output size.\n\n    Note: Returned arrays might be zero padded if not enough target ROIs.\n    \"\"\"\n\n    def __init__(self, config, **kwargs):\n        super(DetectionTargetLayer, self).__init__(**kwargs)\n        self.config = config\n\n    def call(self, inputs):\n        proposals = inputs[0]\n        gt_class_ids = inputs[1]\n        gt_boxes = inputs[2]\n        gt_masks = inputs[3]\n\n        # Slice the batch and run a graph for each slice\n        # TODO: Rename target_bbox to target_deltas for clarity\n        names = [\"rois\", \"target_class_ids\", \"target_bbox\", \"target_mask\"]\n        outputs = utils.batch_slice(\n            [proposals, gt_class_ids, gt_boxes, gt_masks],\n            lambda w, x, y, z: detection_targets_graph(\n                w, x, y, z, self.config),\n            self.config.IMAGES_PER_GPU, names=names)\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        return [\n            (None, self.config.TRAIN_ROIS_PER_IMAGE, 4),  # rois\n            (None, 1),  # class_ids\n            (None, self.config.TRAIN_ROIS_PER_IMAGE, 4),  # deltas\n            (None, self.config.TRAIN_ROIS_PER_IMAGE, self.config.MASK_SHAPE[0],\n             self.config.MASK_SHAPE[1])  # masks\n        ]\n\n    def compute_mask(self, inputs, mask=None):\n        return [None, None, None, None]\n```\n\n### detection_targets_graph\ndetection_targets_graph函數負責處理輸入的[proposals, gt_class_ids, gt_boxes, gt_masks]\n\nCode有點長，首先計算proposals和gt_boxes的覆蓋度矩陣proposals*gt_boxes，然後獲得每個proposals一個與gt_boxes最大覆蓋度值roi_iou_max，如果roi_iou_max>0.5，則認為該proposals是positive_roi，最後再把對應gt_box分配給對應的positive_roi並進行框體微調獲得對應偏移。最終返回的rois包含positive_roi和negative_roi。\n\n根據proposal和gt_box的overlap來確定正樣本和負樣本，並按照sample_ratio和train_anchor_per_image的大小進行sample，最終得出rois, class_id,delta,masks，其中進行了padding  \n\n```python\ndef detection_targets_graph(proposals, gt_class_ids, gt_boxes, gt_masks, config):\n    \"\"\"Generates detection targets for one image. Subsamples proposals and\n    generates target class IDs, bounding box deltas, and masks for each.\n\n    Inputs:\n    proposals: [N, (y1, x1, y2, x2)] in normalized coordinates. Might\n               be zero padded if there are not enough proposals.\n    gt_class_ids: [MAX_GT_INSTANCES] int class IDs\n    gt_boxes: [MAX_GT_INSTANCES, (y1, x1, y2, x2)] in normalized coordinates.\n    gt_masks: [height, width, MAX_GT_INSTANCES] of boolean type.\n\n    Returns: Target ROIs and corresponding class IDs, bounding box shifts,\n    and masks.\n    rois: [TRAIN_ROIS_PER_IMAGE, (y1, x1, y2, x2)] in normalized coordinates\n    class_ids: [TRAIN_ROIS_PER_IMAGE]. Integer class IDs. Zero padded.\n    deltas: [TRAIN_ROIS_PER_IMAGE, NUM_CLASSES, (dy, dx, log(dh), log(dw))]\n            Class-specific bbox refinements.\n    masks: [TRAIN_ROIS_PER_IMAGE, height, width). Masks cropped to bbox\n           boundaries and resized to neural network output size.\n\n    Note: Returned arrays might be zero padded if not enough target ROIs.\n    \"\"\"\n    # Assertions\n    asserts = [ tf.Assert(tf.greater(tf.shape(proposals)[0], 0), [proposals],name=\"roi_assertion\"),]\n    with tf.control_dependencies(asserts):\n        proposals = tf.identity(proposals)\n\n    # Remove zero padding\n    # 去除0的padding\n    proposals, _ = trim_zeros_graph(proposals, name=\"trim_proposals\")\n    gt_boxes, non_zeros = trim_zeros_graph(gt_boxes, name=\"trim_gt_boxes\")\n    gt_class_ids = tf.boolean_mask(gt_class_ids, non_zeros,\n                                   name=\"trim_gt_class_ids\")\n    gt_masks = tf.gather(gt_masks, tf.where(non_zeros)[:, 0], axis=2,\n                         name=\"trim_gt_masks\")\n    \n    # 有點看不懂 反正是處理coco的問題的樣子\n    # Handle COCO crowds\n    # A crowd box in COCO is a bounding box around several instances. Exclude\n    # them from training. A crowd box is given a negative class ID.\n    crowd_ix = tf.where(gt_class_ids < 0)[:, 0]\n    non_crowd_ix = tf.where(gt_class_ids > 0)[:, 0]\n    crowd_boxes = tf.gather(gt_boxes, crowd_ix)\n    crowd_masks = tf.gather(gt_masks, crowd_ix, axis=2)\n    gt_class_ids = tf.gather(gt_class_ids, non_crowd_ix)\n    gt_boxes = tf.gather(gt_boxes, non_crowd_ix)\n    gt_masks = tf.gather(gt_masks, non_crowd_ix, axis=2)\n\n    # Compute overlaps matrix [proposals, gt_boxes]\n        overlaps = overlaps_graph(proposals, gt_boxes)\n\n    # Compute overlaps with crowd boxes [anchors, crowds]\n    crowd_overlaps = overlaps_graph(proposals, crowd_boxes)\n    crowd_iou_max = tf.reduce_max(crowd_overlaps, axis=1)\n    no_crowd_bool = (crowd_iou_max < 0.001)\n\n    # Determine postive and negative ROIs\n    # 獲得proposals和gt_box最大overlaps的值[n_proposals,1]\n    roi_iou_max = tf.reduce_max(overlaps, axis=1)\n    # 1. Positive ROIs are those with >= 0.5 IoU with a GT box\n    # roi覆蓋度值>0.5則認為其是positive_roi bool[n_proposals,1]\n    positive_roi_bool = (roi_iou_max >= 0.5)\n    # 拿positive_roi的index [filter_n_proposals,1]\n    positive_indices = tf.where(positive_roi_bool)[:, 0]\n    # 2. Negative ROIs are those with < 0.5 with every GT box. Skip crowds.\n    negative_indices = tf.where(tf.logical_and(roi_iou_max < 0.5, no_crowd_bool))[:, 0]\n    \n    # 對positive_roi和negative_roi進行了subsample\n    # Subsample ROIs. Aim for 33% positive\n    # Positive ROIs\n    positive_count = int(config.TRAIN_ROIS_PER_IMAGE *\n                         config.ROI_POSITIVE_RATIO)\n    positive_indices = tf.random_shuffle(positive_indices)[:positive_count]\n    positive_count = tf.shape(positive_indices)[0]\n    # Negative ROIs. Add enough to maintain positive:negative ratio.\n    r = 1.0 / config.ROI_POSITIVE_RATIO\n    negative_count = tf.cast(r * tf.cast(positive_count, tf.float32), tf.int32) - positive_count\n    negative_indices = tf.random_shuffle(negative_indices)[:negative_count]\n    # Gather selected ROIs\n    positive_rois = tf.gather(proposals, positive_indices)\n    negative_rois = tf.gather(proposals, negative_indices)\n\n    # Assign positive ROIs to GT boxes.\n    # 把sample後的positive_roi分配給gt_box\n    positive_overlaps = tf.gather(overlaps, positive_indices)\n    # 每個positive_rois對應gt_box overlaps最大值的下標[filter_n_proposals,1]\n    roi_gt_box_assignment = tf.argmax(positive_overlaps, axis=1)\n    roi_gt_boxes = tf.gather(gt_boxes, roi_gt_box_assignment)\n    roi_gt_class_ids = tf.gather(gt_class_ids, roi_gt_box_assignment)\n\n    # Compute bbox refinement for positive ROIs\n    deltas = utils.box_refinement_graph(positive_rois, roi_gt_boxes)\n    deltas /= config.BBOX_STD_DEV\n\n    # Assign positive ROIs to GT masks\n    # Permute masks to [N, height, width, 1]\n    transposed_masks = tf.expand_dims(tf.transpose(gt_masks, [2, 0, 1]), -1)\n    # Pick the right mask for each ROI\n    roi_masks = tf.gather(transposed_masks, roi_gt_box_assignment)\n\n    # Compute mask targets\n    boxes = positive_rois\n    if config.USE_MINI_MASK:\n        # Transform ROI corrdinates from normalized image space\n        # to normalized mini-mask space.\n        y1, x1, y2, x2 = tf.split(positive_rois, 4, axis=1)\n        gt_y1, gt_x1, gt_y2, gt_x2 = tf.split(roi_gt_boxes, 4, axis=1)\n        gt_h = gt_y2 - gt_y1\n        gt_w = gt_x2 - gt_x1\n        y1 = (y1 - gt_y1) / gt_h\n        x1 = (x1 - gt_x1) / gt_w\n        y2 = (y2 - gt_y1) / gt_h\n        x2 = (x2 - gt_x1) / gt_w\n        boxes = tf.concat([y1, x1, y2, x2], 1)\n    box_ids = tf.range(0, tf.shape(roi_masks)[0])\n    masks = tf.image.crop_and_resize(tf.cast(roi_masks, tf.float32), boxes,\n                                     box_ids,\n                                     config.MASK_SHAPE)\n    # Remove the extra dimension from masks.\n    masks = tf.squeeze(masks, axis=3)\n\n    # Threshold mask pixels at 0.5 to have GT masks be 0 or 1 to use with\n    # binary cross entropy loss.\n    masks = tf.round(masks)\n\n    # Append negative ROIs and pad bbox deltas and masks that\n    # are not used for negative ROIs with zeros.\n    rois = tf.concat([positive_rois, negative_rois], axis=0)\n    N = tf.shape(negative_rois)[0]\n    P = tf.maximum(config.TRAIN_ROIS_PER_IMAGE - tf.shape(rois)[0], 0)\n    rois = tf.pad(rois, [(0, P), (0, 0)])\n    roi_gt_boxes = tf.pad(roi_gt_boxes, [(0, N + P), (0, 0)])\n    roi_gt_class_ids = tf.pad(roi_gt_class_ids, [(0, N + P)])\n    deltas = tf.pad(deltas, [(0, N + P), (0, 0)])\n    masks = tf.pad(masks, [[0, N + P], (0, 0), (0, 0)])\n\n    return rois, roi_gt_class_ids, deltas, masks\n```\n\n\n\n"},{"title":"Mask RCNNCode Reading for Proposal Layer","url":"/2018/06/02/Mask-RCNN-Code-Reading-Propsal-Layer/","content":"\n\n通過下面這段程式碼拿到anchors以後\n\n```python\n# Anchors\nif mode == \"training\":\n  anchors = self.get_anchors(config.IMAGE_SHAPE)\n  # Duplicate across the batch dimension because Keras requires it\n  # TODO: can this be optimized to avoid duplicating the anchors?\n  anchors = np.broadcast_to(anchors, (config.BATCH_SIZE,) + anchors.shape)\n  # A hack to get around Keras's bad support for constants\n  anchors = KL.Lambda(lambda x: tf.Variable(anchors), name=\"anchors\")(input_image)\nelse:\n  anchors = input_anchors\n```\n\n這裡總算要進行proposal layer計算了\n將所有anchors還有相應對的分數跟偏移量當作輸入\n而前面的東西都是class init的時候的設定\n\n```python\nrpn_class_logits, rpn_class, rpn_bbox = outputs\n\n# Generate proposals\n# Proposals are [batch, N, (y1, x1, y2, x2)] in normalized coordinates\n# and zero padded.\n\n# POST_NMS_ROIS_INFERENCE = 1000\n# POST_NMS_ROIS_TRAINING = 2000\n\n    proposal_count = config.POST_NMS_ROIS_TRAINING if mode == \"training\"\\\n            else config.POST_NMS_ROIS_INFERENCE\n        \nrpn_rois = ProposalLayer(\n              proposal_count=proposal_count,\n              nms_threshold=config.RPN_NMS_THRESHOLD,\n              name=\"ROI\",\n              config=config)([rpn_class, rpn_bbox, anchors]\n              )\n\n\nif mode == \"training\":\n      # Class ID mask to mark class IDs supported by the dataset the image\n      # came from.\n      active_class_ids = KL.Lambda(\n          lambda x: parse_image_meta_graph(x)[\"active_class_ids\"]\n          )(input_image_meta)\n    \n      if not config.USE_RPN_ROIS:\n          # Ignore predicted ROIs and use ROIs provided as an input.\n          input_rois = KL.Input(shape=[config.POST_NMS_ROIS_TRAINING, 4],\n                                name=\"input_roi\", dtype=np.int32)\n          # Normalize coordinates\n          target_rois = KL.Lambda(lambda x: norm_boxes_graph(\n              x, K.shape(input_image)[1:3]))(input_rois)\n      else:\n          target_rois = rpn_rois\n\n```\n其中parse_image_meta_graph\n\n```python\nreturn {\n        \"image_id\": image_id,\n        \"original_image_shape\": original_image_shape,\n        \"image_shape\": image_shape,\n        \"window\": window,\n        \"scale\": scale,\n        \"active_class_ids\": active_class_ids,\n    }\n```\n\n\n### Proposal Layer\n\nrpn_class：所有像素點BG/FG的機率值。\nrpn_bbox：所有像素點對應anchor上的4個偏移值[dy, dx, log(dh), log(dw)]。\nanchors:  剛剛通過預先生成的有序anchor列表，注意這裡有序表示feature_map上像素點生成的anchor以及該像素點生成的rpn_class和rpn_bbox是對應的（看paper看起來是這樣,但這樣理解對嗎, 有點不確定)\nscores和deltas都是RPN中得到的\n\n\n最終ProposalLayer會return一個經過bbox regression以及NMS過濾後anchor boxes set(稱為roi或proposal)，至此已經完成了RPN啦~\ninit內super的用法可參考 [你不知道的 super\n](http://funhacks.net/explore-python/Class/super.html)\n\n要注意的事情是, 其實對於inference來說 這裡已經有RPN的結果了, 但對於training來說還少了一些東西, 就是跟grondtruth的比較還有等等, 所以才有後續的DetectionTargetLayer\n\n\n另外Python中，如果在創建class的時候寫了call()， 那麼該class實例化出實例後， 實例名()就是調用call()。\n\n```python\nclass ProposalLayer(KE.Layer):\n    \"\"\"Receives anchor scores and selects a subset to pass as proposals\n    to the second stage. Filtering is done based on anchor scores and\n    non-max suppression to remove overlaps. It also applies bounding\n    box refinement deltas to anchors.\n\n    Inputs:\n        rpn_probs: [batch, anchors, (bg prob, fg prob)]\n        rpn_bbox: [batch, anchors, (dy, dx, log(dh), log(dw))]\n        anchors: [batch, (y1, x1, y2, x2)] anchors in normalized coordinates\n\n    Returns:\n        Proposals in normalized coordinates [batch, rois, (y1, x1, y2, x2)]\n    \"\"\"\n\n    def __init__(self, proposal_count, nms_threshold, config=None, **kwargs):\n        super(ProposalLayer, self).__init__(**kwargs)\n        self.config = config\n        self.proposal_count = proposal_count\n        self.nms_threshold = nms_threshold\n\n    def call(self, inputs):\n        ###實現了將傳入的anchors，及其scores、deltas進行topK的推薦和nms的推薦，最終輸出  \n        ###數量為proposal_counts的proposals。其中的scores和deltas都是RPN網絡中得到的\n        # Box Scores. Use the foreground class confidence. [Batch, num_rois, 1]\n        scores = inputs[0][:, :, 1]\n        # Box deltas [batch, num_rois, 4]\n        deltas = inputs[1]\n        deltas = deltas * np.reshape(self.config.RPN_BBOX_STD_DEV, [1, 1, 4])\n        # Anchors\n        anchors = inputs[2]\n\n        # Improve performance by trimming to top anchors by score\n        # and doing the rest on the smaller subset.\n        pre_nms_limit = tf.minimum(6000, tf.shape(anchors)[1])\n        ix = tf.nn.top_k(scores, pre_nms_limit, sorted=True,\n                         name=\"top_anchors\").indices\n        scores = utils.batch_slice([scores, ix], lambda x, y: tf.gather(x, y),\n                                   self.config.IMAGES_PER_GPU)\n        deltas = utils.batch_slice([deltas, ix], lambda x, y: tf.gather(x, y),\n                                   self.config.IMAGES_PER_GPU)\n        pre_nms_anchors = utils.batch_slice([anchors, ix], lambda a, x: tf.gather(a, x),\n                                    self.config.IMAGES_PER_GPU,\n                                    names=[\"pre_nms_anchors\"])\n\n        # Apply deltas to anchors to get refined anchors.\n        # [batch, N, (y1, x1, y2, x2)]\n        ##利用deltas在anchors上，得到精煉後的boxs  \n\n        boxes = utils.batch_slice([pre_nms_anchors, deltas],\n                                  lambda x, y: apply_box_deltas_graph(x, y),\n                                  self.config.IMAGES_PER_GPU,\n                                  names=[\"refined_anchors\"])\n                                  \n        # normalized coordinates就是對應原圖的百分比坐標\n        # 下面的作用：防止修正後的anchor坐標超出了邊界即0<=x,y<=1\n        # Clip to image boundaries. Since we're in normalized coordinates,\n        # clip to 0..1 range. [batch, N, (y1, x1, y2, x2)]\n        window = np.array([0, 0, 1, 1], dtype=np.float32)\n        boxes = utils.batch_slice(boxes,\n                                  lambda x: clip_boxes_graph(x, window),\n                                  self.config.IMAGES_PER_GPU,\n                                  names=[\"refined_anchors_clipped\"])\n\n        # Filter out small boxes\n        # According to Xinlei Chen's paper, this reduces detection accuracy\n        # for small objects, so we're skipping it.\n\n        # Non-max suppression\n        def nms(boxes, scores):\n            indices = tf.image.non_max_suppression(\n                boxes, scores, self.proposal_count,\n                self.nms_threshold, name=\"rpn_non_max_suppression\")\n            proposals = tf.gather(boxes, indices)\n            # Pad if needed\n            padding = tf.maximum(self.proposal_count - tf.shape(proposals)[0], 0)\n            ##利用deltas在anchors上，得到精化的boxs \n            proposals = tf.pad(proposals, [(0, padding), (0, 0)])\n            return proposals\n             \n\n        proposals = utils.batch_slice([boxes, scores], nms,\n                                      self.config.IMAGES_PER_GPU)\n        return proposals\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.proposal_count, 4)\n```\n\n"},{"title":"Mask RCNNCode Reading for Anchor boxes generate","url":"/2018/06/02/Mask-RCNN-Code-Reading-Anchor-boxes-Generate/","content":"\n\n接續上面的RPN output\n\n```python\nrpn_class_logits, rpn_class, rpn_bbox = outputs\n\n# Generate proposals\n# Proposals are [batch, N, (y1, x1, y2, x2)] in normalized coordinates\n# and zero padded.\n\n# POST_NMS_ROIS_INFERENCE = 1000\n# POST_NMS_ROIS_TRAINING = 2000\n\n    proposal_count = config.POST_NMS_ROIS_TRAINING if mode == \"training\"\\\n            else config.POST_NMS_ROIS_INFERENCE\n        \nrpn_rois = ProposalLayer(\n              proposal_count=proposal_count,\n              nms_threshold=config.RPN_NMS_THRESHOLD,\n              name=\"ROI\",\n              config=config)([rpn_class, rpn_bbox, anchors]\n              )\n```\n\n# Anchor boxes generate\n而 anchors 的來源呢\n在主程式碼中有這段\n\n```python\n# Anchors\nif mode == \"training\":\n  anchors = self.get_anchors(config.IMAGE_SHAPE)\n  # Duplicate across the batch dimension because Keras requires it\n  # TODO: can this be optimized to avoid duplicating the anchors?\n  anchors = np.broadcast_to(anchors, (config.BATCH_SIZE,) + anchors.shape)\n  # A hack to get around Keras's bad support for constants\n  anchors = KL.Lambda(lambda x: tf.Variable(anchors), name=\"anchors\")(input_image)\nelse:\n  anchors = input_anchors\n```\n\n\n\n### def get_anchors\n\ncompute_backbone_shapes:\n\n```\nComputes the width and height of each stage of the backbone network.\nReturns:\n  [N, (height, width)]. Where N is the number of stages\n```\n\nutils.generate_pyramid_anchors這邊看起來相對於single scale的backbone, 這裡多了backbone_shapes這項變數, 是因為需要知道該層feature_map大小才能回推anchor boxes\n\n```python\nRPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)\nRPN_ANCHOR_RATIOS = [0.5, 1 ,2]\n\ndef get_anchors(self, image_shape):\n   \"\"\"Returns anchor pyramid for the given image size.\"\"\"\n   backbone_shapes = compute_backbone_shapes(self.config, image_shape)\n   # Cache anchors and reuse if image shape is the same\n   if not hasattr(self, \"_anchor_cache\"):\n       self._anchor_cache = {}\n   if not tuple(image_shape) in self._anchor_cache:\n       # Generate Anchors\n       a = utils.generate_pyramid_anchors(\n           self.config.RPN_ANCHOR_SCALES,\n           self.config.RPN_ANCHOR_RATIOS,\n           backbone_shapes,\n           self.config.BACKBONE_STRIDES,\n           self.config.RPN_ANCHOR_STRIDE)\n       # Keep a copy of the latest anchors in pixel coordinates because\n       # it's used in inspect_model notebooks.\n       # TODO: Remove this after the notebook are refactored to not use it\n       self.anchors = a\n       # Normalize coordinates\n       self._anchor_cache[tuple(image_shape)] = utils.norm_boxes(a, image_shape[:2])\n   return self._anchor_cache[tuple(image_shape)]\n```\n\n### utils.generate_pyramid_anchors\n這邊很明顯就是包裝一層, 讓各個stage (P2~P6)的各自去計算以後append\n後方操作...舉個例子, 順便了解一下IO, 其實只是會了整形而已\n\n```python\n>>> a=np.arange(5)\n>>> b=np.array([11,22,33])\n>>> a\narray([0, 1, 2, 3, 4])\n>>> b\narray([11, 22, 33])\n\n>>> c.append(a)\n>>> c\n[array([0, 1, 2, 3, 4])]\n>>> c.append(b)\n>>> c\n[array([0, 1, 2, 3, 4]), array([11, 22, 33])]\n>>> np.concatenate(c,axis=0)\narray([ 0,  1,  2,  3,  4, 11, 22, 33])\n```\n\n\n\n```python\ndef generate_pyramid_anchors(scales, ratios, feature_shapes, feature_strides,anchor_stride):\n    \"\"\"Generate anchors at different levels of a feature pyramid. Each scale\n    is associated with a level of the pyramid, but each ratio is used in\n    all levels of the pyramid.\n\n    Returns:\n    anchors: [N, (y1, x1, y2, x2)]. All generated anchors in one array. Sorted\n        with the same order of the given scales. So, anchors of scale[0] come\n        first, then anchors of scale[1], and so on.\n    \"\"\"\n    # Anchors\n    # [anchor_count, (y1, x1, y2, x2)]\n    anchors = []\n    for i in range(len(scales)):\n        anchors.append(\n            generate_anchors(\n                    scales[i],  # self.config.RPN_ANCHOR_SCALES\n                    ratios,     # self.config.RPN_ANCHOR_RATIOS\n                    feature_shapes[i],  # backbone_shapes\n                    feature_strides[i], # self.config.BACKBONE_STRIDES [4, 8, 16, 32, 64]\n                    anchor_stride       # self.config.RPN_ANCHOR_STRIDE\n            ))\n    return np.concatenate(anchors, axis=0)\n```\n\n### utils.generate_anchors\nCode內其實有參數說明了\nscales就是anchor boxes邊長了\nratio就是anchor boxes的比例\nshapes是原圖\nfeature_stride 其實就是原圖與該feature層(例如p3)的縮放倍率, 如果是resnet或是VGG這種就一律都是縮放16倍了 \nanchor_stride 這通常是1吧XD 就是以縮放比切割原圖後可以得到(w/feature_stride)x(h/feature_stride)個grid, 那在此多少區間弄一個anchor?\n\n```python\ndef generate_anchors(scales, ratios, shape, feature_stride, anchor_stride):\n    \"\"\"\n    scales: 1D array of anchor sizes in pixels. Example: [32, 64, 128]\n    ratios: 1D array of anchor ratios of width/height. Example: [0.5, 1, 2]\n    shape: [height, width] spatial shape of the feature map over which\n            to generate anchors.\n    feature_stride: Stride of the feature map relative to the image in pixels.\n    anchor_stride: Stride of anchors on the feature map. For example, if the\n        value is 2 then generate anchors for every other feature map pixel.\n    \"\"\"\n    # Get all combinations of scales and ratios\n    scales, ratios = np.meshgrid(np.array(scales), np.array(ratios))\n    scales = scales.flatten()\n    ratios = ratios.flatten()\n\n    # Enumerate heights and widths from scales and ratios\n    heights = scales / np.sqrt(ratios)\n    widths = scales * np.sqrt(ratios)\n\n    # Enumerate shifts in feature space\n    shifts_y = np.arange(0, shape[0], anchor_stride) * feature_stride\n    shifts_x = np.arange(0, shape[1], anchor_stride) * feature_stride\n    shifts_x, shifts_y = np.meshgrid(shifts_x, shifts_y)\n\n    # Enumerate combinations of shifts, widths, and heights\n    box_widths, box_centers_x = np.meshgrid(widths, shifts_x)\n    box_heights, box_centers_y = np.meshgrid(heights, shifts_y)\n\n    # Reshape to get a list of (y, x) and a list of (h, w)\n    box_centers = np.stack(\n        [box_centers_y, box_centers_x], axis=2).reshape([-1, 2])\n    box_sizes = np.stack([box_heights, box_widths], axis=2).reshape([-1, 2])\n\n    # Convert to corner coordinates (y1, x1, y2, x2)\n    boxes = np.concatenate([box_centers - 0.5 * box_sizes,\n                            box_centers + 0.5 * box_sizes], axis=1)\n    return boxes\n\n```\n\n\n下面用一組參數示範一下, 到最後widths, heights 就可以得到anchor boxes的所有邊長了\nnp.meshgrid的部分, 要注意傳入參數得需要是一維的, 接著再用flatten()把它變成1xN的array\nnp.stack我覺得挺難理解, 可以看這邊 [numpy.stack最通俗的理解](https://blog.csdn.net/qq_17550379/article/details/78934529)\n\n```python\n# Get all combinations of scales and ratios\n\n>>> scales = [32, 64, 128, 256, 512]\n>>> ratios = [0.5,1,2]\n>>> np.meshgrid(np.array(scales), np.array(ratios))\n[\n    array([\n        [ 32,  64, 128, 256, 512],\n        [ 32,  64, 128, 256, 512],\n        [ 32,  64, 128, 256, 512]\n    ]), \n    array([\n        [0.5, 0.5, 0.5, 0.5, 0.5],\n        [1. , 1. , 1. , 1. , 1. ],\n        [2. , 2. , 2. , 2. , 2. ]\n    ])\n]\n\n### 反過來傳結果有點不一樣\n>>> np.meshgrid(np.array(ratios), np.array(scales))\n[array([[0.5, 1. , 2. ],\n       [0.5, 1. , 2. ],\n       [0.5, 1. , 2. ],\n       [0.5, 1. , 2. ],\n       [0.5, 1. , 2. ]]), \n       \n array([[ 32,  32,  32],\n       [ 64,  64,  64],\n       [128, 128, 128],\n       [256, 256, 256],\n       [512, 512, 512]])]\n\n>>> scales = scales.flatten()\n>>> ratios = ratios.flatten()\n\n# Enumerate heights and widths from scales and ratios       \n>>> heights = scales / np.sqrt(ratios)\n>>> widths = scales * np.sqrt(ratios)\n>>> heights\narray([ 45.254834  ,  90.50966799, 181.01933598, 362.03867197,\n       724.07734394,  32.        ,  64.        , 128.        ,\n       256.        , 512.        ,  22.627417  ,  45.254834  ,\n        90.50966799, 181.01933598, 362.03867197])\n>>> widths\narray([ 22.627417  ,  45.254834  ,  90.50966799, 181.01933598,\n       362.03867197,  32.        ,  64.        , 128.        ,\n       256.        , 512.        ,  45.254834  ,  90.50966799,\n       181.01933598, 362.03867197, 724.07734394])\n>>> heights.shape, widths.shape\n((15,), (15,))\n\n# Enumerate shifts in feature space\n>>> anchor_stride=1\n>>> shape=[1024,2048]\n>>> feature_strides = [4, 8, 16, 32, 64]\n>>> feature_stride = feature_strides[2]\n\n\n>>> shifts_x = np.arange(0, shape[1], anchor_stride) * feature_stride\n>>> shifts_y = np.arange(0, shape[0], anchor_stride) * feature_stride\n>>> shifts_x\narray([    0,    16,    32, ..., 32720, 32736, 32752])\n>>> shifts_y\narray([    0,    16,    32, ..., 16336, 16352, 16368])\n>>> len(shifts_x), len(shifts_y)\n2048,1024\n>>> shifts_x, shifts_y = np.meshgrid(shifts_x, shifts_y)\n>>> shifts_y\narray([[    0,     0,     0, ...,     0,     0,     0],\n       [   16,    16,    16, ...,    16,    16,    16],\n       [   32,    32,    32, ...,    32,    32,    32],\n       ...,\n       [16336, 16336, 16336, ..., 16336, 16336, 16336],\n       [16352, 16352, 16352, ..., 16352, 16352, 16352],\n       [16368, 16368, 16368, ..., 16368, 16368, 16368]])\n>>> shifts_x\narray([[    0,    16,    32, ..., 32720, 32736, 32752],\n       [    0,    16,    32, ..., 32720, 32736, 32752],\n       [    0,    16,    32, ..., 32720, 32736, 32752],\n       ...,\n       [    0,    16,    32, ..., 32720, 32736, 32752],\n       [    0,    16,    32, ..., 32720, 32736, 32752],\n       [    0,    16,    32, ..., 32720, 32736, 32752]])\n>>> shifts_x.shape, shifts_y.shape\n((1024, 2048), (1024, 2048))\n       \n# Enumerate combinations of shifts, widths, and heights\n>>> box_widths, box_centers_x = np.meshgrid(widths, shifts_x)\n>>> box_heights, box_centers_y = np.meshgrid(heights, shifts_y)\n>>> box_heights\narray([[ 45.254834  ,  90.50966799, 181.01933598, ...,  90.50966799,\n        181.01933598, 362.03867197],\n       [ 45.254834  ,  90.50966799, 181.01933598, ...,  90.50966799,\n        181.01933598, 362.03867197],\n       [ 45.254834  ,  90.50966799, 181.01933598, ...,  90.50966799,\n        181.01933598, 362.03867197],\n       ...,\n       [ 45.254834  ,  90.50966799, 181.01933598, ...,  90.50966799,\n        181.01933598, 362.03867197],\n       [ 45.254834  ,  90.50966799, 181.01933598, ...,  90.50966799,\n        181.01933598, 362.03867197],\n       [ 45.254834  ,  90.50966799, 181.01933598, ...,  90.50966799,\n        181.01933598, 362.03867197]])\n>>> box_centers_y\narray([[    0,     0,     0, ...,     0,     0,     0],\n       [    0,     0,     0, ...,     0,     0,     0],\n       [    0,     0,     0, ...,     0,     0,     0],\n       ...,\n       [16368, 16368, 16368, ..., 16368, 16368, 16368],\n       [16368, 16368, 16368, ..., 16368, 16368, 16368],\n       [16368, 16368, 16368, ..., 16368, 16368, 16368]])\n>>> box_centers_x\narray([[    0,     0,     0, ...,     0,     0,     0],\n       [   16,    16,    16, ...,    16,    16,    16],\n       [   32,    32,    32, ...,    32,    32,    32],\n       ...,\n       [32720, 32720, 32720, ..., 32720, 32720, 32720],\n       [32736, 32736, 32736, ..., 32736, 32736, 32736],\n       [32752, 32752, 32752, ..., 32752, 32752, 32752]])\n>>> box_widths\narray([[ 22.627417  ,  45.254834  ,  90.50966799, ..., 181.01933598,\n        362.03867197, 724.07734394],\n       [ 22.627417  ,  45.254834  ,  90.50966799, ..., 181.01933598,\n        362.03867197, 724.07734394],\n       [ 22.627417  ,  45.254834  ,  90.50966799, ..., 181.01933598,\n        362.03867197, 724.07734394],\n       ...,\n       [ 22.627417  ,  45.254834  ,  90.50966799, ..., 181.01933598,\n        362.03867197, 724.07734394],\n       [ 22.627417  ,  45.254834  ,  90.50966799, ..., 181.01933598,\n        362.03867197, 724.07734394],\n       [ 22.627417  ,  45.254834  ,  90.50966799, ..., 181.01933598,\n        362.03867197, 724.07734394]])\n\n3867197,>>> box_widths.shape, box_centers_x.shape, box_heights.shape , box_centers_y.shape\n((2097152, 15), (2097152, 15), (2097152, 15), (2097152, 15))\n\n# Reshape to get a list of (y, x) and a list of (h, w)\n>>> box_centers\narray([[    0,     0],\n       [    0,     0],\n       [    0,     0],\n       ...,\n       [16368, 32752],\n       [16368, 32752],\n       [16368, 32752]])\n>>> box_sizes\narray([[ 45.254834  ,  22.627417  ],\n       [ 90.50966799,  45.254834  ],\n       [181.01933598,  90.50966799],\n       ...,\n       [ 90.50966799, 181.01933598],\n       [181.01933598, 362.03867197],\n       [362.03867197, 724.07734394]])\n>>> len(box_centers),len(box_sizes)\n(31457280, 31457280)\n>>> box_centers.shape, box_sizes.shape\n((31457280, 2), (31457280, 2))\n\n# Convert to corner coordinates (y1, x1, y2, x2)\n\n>>> boxes = np.concatenate([box_centers - 0.5 * box_sizes,\n...                             box_centers + 0.5 * box_sizes], axis=1)\n>>> boxes\narray([[-2.26274170e+01, -1.13137085e+01,  2.26274170e+01,\n         1.13137085e+01],\n       [-4.52548340e+01, -2.26274170e+01,  4.52548340e+01,\n         2.26274170e+01],\n       [-9.05096680e+01, -4.52548340e+01,  9.05096680e+01,\n         4.52548340e+01],\n       ...,\n       [ 1.63227452e+04,  3.26614903e+04,  1.64132548e+04,\n         3.28425097e+04],\n       [ 1.62774903e+04,  3.25709807e+04,  1.64585097e+04,\n         3.29330193e+04],\n       [ 1.61869807e+04,  3.23899613e+04,  1.65490193e+04,\n         3.31140387e+04]])\n         \n>>> boxes.shape\n(31457280, 4)\n\n```\n```python\n>>> np.broadcast_to(boxes, (4,) + boxes.shape)\narray([[[-2.26274170e+01, -1.13137085e+01,  2.26274170e+01,\n          1.13137085e+01],\n        [-4.52548340e+01, -2.26274170e+01,  4.52548340e+01,\n          2.26274170e+01],\n        [-9.05096680e+01, -4.52548340e+01,  9.05096680e+01,\n          4.52548340e+01],\n        ...,\n        [ 1.63227452e+04,  3.26614903e+04,  1.64132548e+04,\n          3.28425097e+04],\n        [ 1.62774903e+04,  3.25709807e+04,  1.64585097e+04,\n          3.29330193e+04],\n        [ 1.61869807e+04,  3.23899613e+04,  1.65490193e+04,\n          3.31140387e+04]],\n\n       [[-2.26274170e+01, -1.13137085e+01,  2.26274170e+01,\n          1.13137085e+01],\n        [-4.52548340e+01, -2.26274170e+01,  4.52548340e+01,\n          2.26274170e+01],\n        [-9.05096680e+01, -4.52548340e+01,  9.05096680e+01,\n          4.52548340e+01],\n        ...,\n        [ 1.63227452e+04,  3.26614903e+04,  1.64132548e+04,\n          3.28425097e+04],\n        [ 1.62774903e+04,  3.25709807e+04,  1.64585097e+04,\n          3.29330193e+04],\n        [ 1.61869807e+04,  3.23899613e+04,  1.65490193e+04,\n          3.31140387e+04]],\n\n       [[-2.26274170e+01, -1.13137085e+01,  2.26274170e+01,\n          1.13137085e+01],\n        [-4.52548340e+01, -2.26274170e+01,  4.52548340e+01,\n          2.26274170e+01],\n        [-9.05096680e+01, -4.52548340e+01,  9.05096680e+01,\n          4.52548340e+01],\n        ...,\n        [ 1.63227452e+04,  3.26614903e+04,  1.64132548e+04,\n          3.28425097e+04],\n        [ 1.62774903e+04,  3.25709807e+04,  1.64585097e+04,\n          3.29330193e+04],\n        [ 1.61869807e+04,  3.23899613e+04,  1.65490193e+04,\n          3.31140387e+04]],\n\n       [[-2.26274170e+01, -1.13137085e+01,  2.26274170e+01,\n          1.13137085e+01],\n        [-4.52548340e+01, -2.26274170e+01,  4.52548340e+01,\n          2.26274170e+01],\n        [-9.05096680e+01, -4.52548340e+01,  9.05096680e+01,\n          4.52548340e+01],\n        ...,\n        [ 1.63227452e+04,  3.26614903e+04,  1.64132548e+04,\n          3.28425097e+04],\n        [ 1.62774903e+04,  3.25709807e+04,  1.64585097e+04,\n          3.29330193e+04],\n        [ 1.61869807e+04,  3.23899613e+04,  1.65490193e+04,\n          3.31140387e+04]]])\n\n>>> np.broadcast_to(boxes, (4,) + boxes.shape).shape\n(4, 31457280, 4)\n```\n\n"},{"title":"Mask RCNN Code Reading for RPN","url":"/2018/05/30/Mask-RCNN-Code-Reading-RPN/","content":"Mask_RCNN V2.1版本\n\n檔案：[Model.py](https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py#L822)\n\n![](/media/15276733083036.jpg)\n\n\n\n### Class MaskRCNN():\n\n這部分是在程式碼主體所使用到RPN的部分, 其中傳入參數預設為此, for loop那邊可以看到有幾個從fpn的output就會有幾個rpn\n    \n    RPN_ANCHOR_RATIOS = [0.5, 1, 2]\n    RPN_ANCHOR_STRIDE = 1\n\n```python\n # Note that P6 is used in RPN, but not in the classifier heads.\n        rpn_feature_maps = [P2, P3, P4, P5, P6]\n        mrcnn_feature_maps = [P2, P3, P4, P5]\n\n        # Anchors\n        if mode == \"training\":\n            anchors = self.get_anchors(config.IMAGE_SHAPE)\n            # Duplicate across the batch dimension because Keras requires it\n            # TODO: can this be optimized to avoid duplicating the anchors?\n            anchors = np.broadcast_to(anchors, (config.BATCH_SIZE,) + anchors.shape)\n            # A hack to get around Keras's bad support for constants\n            anchors = KL.Lambda(lambda x: tf.Variable(anchors), name=\"anchors\")(input_image)\n        else:\n            anchors = input_anchors\n\n        # RPN Model\n        rpn = build_rpn_model(config.RPN_ANCHOR_STRIDE,\n                              len(config.RPN_ANCHOR_RATIOS), 256)\n```\n其中layer_output會長這樣, 就是一堆input是rpn_feature_maps = [P2, P3, P4, P5, P6] 輸出是[“rpn_class_logits”, “rpn_class”, “rpn_bbox”]的東西\n        \n```python\n[[<tf.Tensor 'rpn_model/lambda_2/Reshape:0' shape=(?, ?, 2) dtype=float32>, <tf.Tensor 'rpn_model/rpn_class_xxx/truediv:0' shape=(?, ?, 2) dtype=float32>, <tf.Tensor 'rpn_model/lambda_3/Reshape:0' shape=(?, ?, 4) dtype=float32>], [<tf.Tensor 'rpn_model_1/lambda_2/Reshape:0' shape=(?, ?, 2) dtype=float32>, <tf.Tensor 'rpn_model_1/rpn_class_xxx/truediv:0' shape=(?, ?, 2) dtype=float32>, <tf.Tensor 'rpn_model_1/lambda_3/Reshape:0' shape=(?, ?, 4) dtype=float32>], [<tf.Tensor 'rpn_model_2/lambda_2/Reshape:0' shape=(?, ?, 2) dtype=float32>, <tf.Tensor 'rpn_model_2/rpn_class_xxx/truediv:0' shape=(?, ?, 2) dtype=float32>, <tf.Tensor 'rpn_model_2/lambda_3/Reshape:0' shape=(?, ?, 4) dtype=float32>], [<tf.Tensor 'rpn_model_3/lambda_2/Reshape:0' shape=(?, ?, 2) dtype=float32>, <tf.Tensor 'rpn_model_3/rpn_class_xxx/truediv:0' shape=(?, ?, 2) dtype=float32>, <tf.Tensor 'rpn_model_3/lambda_3/Reshape:0' shape=(?, ?, 4) dtype=float32>], [<tf.Tensor 'rpn_model_4/lambda_2/Reshape:0' shape=(?, ?, 2) dtype=float32>, <tf.Tensor 'rpn_model_4/rpn_class_xxx/truediv:0' shape=(?, ?, 2) dtype=float32>, <tf.Tensor 'rpn_model_4/lambda_3/Reshape:0' shape=(?, ?, 4) dtype=float32>]]\n```\n        \n```python\n        # Loop through pyramid layers\n            layer_outputs = []  # list of lists\n            for p in rpn_feature_maps:\n                layer_outputs.append(rpn([p]))\n        # Concatenate layer outputs\n        # Convert from list of lists of level outputs to list of lists\n        # of outputs across levels.\n        # e.g. [[a1, b1, c1], [a2, b2, c2]] => [[a1, a2], [b1, b2], [c1, c2]]\n        output_names = [\"rpn_class_logits\", \"rpn_class\", \"rpn_bbox\"]\n        outputs = list(zip(*layer_outputs))\n        outputs = [KL.Concatenate(axis=1, name=n)(list(o))\n                   for o, n in zip(outputs, output_names)]\n\n        rpn_class_logits, rpn_class, rpn_bbox = outputs\n```\n        \n### RPN_GRAPH\n\nrpn_logits: [batch, H, W, 2] Anchor classifier logits (before softmax)\nrpn_probs: [batch, W, W, 2] Anchor classifier probabilities.\nrpn_bbox: [batch, H, W, (dy, dx, log(dh), log(dw))] Deltas to be applied to anchors.\n\n在上方的時候有執行了這行, 三個參數對應到anchor_stride, anchors_per_location, depth, 為啥要這樣包的原因是因為可以用同樣的weight好幾次\n\n```python\n##RPN_ANCHOR_RATIOS = [0.5, 1, 2]\n##RPN_ANCHOR_STRIDE = 1\nrpn = build_rpn_model(config.RPN_ANCHOR_STRIDE,\n                              len(config.RPN_ANCHOR_RATIOS), 256)\nInput() is used to instantiate a Keras tensor.   # https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/contrib/keras/layers/Input\nFor instance, if a, b and c and Keras tensors, it becomes possible to do: model = Model(input=[a, b], output=c)\n\n\n\ndef build_rpn_model(anchor_stride, anchors_per_location, depth):\n    \"\"\"\n    Builds a Keras model of the Region Proposal Network.\n    It wraps the RPN graph so it can be used multiple times with shared\n    weights.\n    anchors_per_location: number of anchors per pixel in the feature map\n    anchor_stride: Controls the density of anchors. Typically 1 (anchors for\n                   every pixel in the feature map), or 2 (every other pixel).\n    depth: Depth of the backbone feature map.\n    Returns a Keras Model object. The model outputs, when called, are:\n    rpn_logits: [batch, H, W, 2] Anchor classifier logits (before softmax)\n    rpn_probs: [batch, W, W, 2] Anchor classifier probabilities.\n    rpn_bbox: [batch, H, W, (dy, dx, log(dh), log(dw))] Deltas to be\n                applied to anchors.\n    \"\"\"\n    input_feature_map = KL.Input(shape=[None, None, depth],\n                                 name=\"input_rpn_feature_map\")\n    outputs = rpn_graph(input_feature_map, anchors_per_location, anchor_stride)\n    return KM.Model([input_feature_map], outputs, name=\"rpn_model\")\n\n```\n\n\nfeature map->Conv(3x3,512)->RELU 後得到shared feature map\n接下來就兵分兩路\n\n這邊有點奇怪的是 下面這邊 \n理論上來說應該是要 len(RATIOS)*len(RPN_ANCHOR_SCALES) # 3 x 5\n但實質上只有3而已\n\n```python\nRPN_ANCHOR_RATIOS = [0.5, 1, 2]\nRPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)\nanchors_per_location = len(config.RPN_ANCHOR_RATIOS)\n```\n\n計算分數\nshared feature map->Conv(1x1,2*3)->linear_activation\n接著得到的東西會reshape成2xN的樣式的到rpn_class_logits, 接著才做softmax\n\n    # Reshape to [batch, anchors, 2]\n        rpn_class_logits = KL.Lambda(\n            lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n計算BBOX 偏移\nshared feature map->Conv(1x1,4*3)->linear_activation\n接著得到的東西會reshape成2xN的樣式的到rpn_class_logits\n   \n    # Reshape to [batch, anchors, 4]\n        rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n\n\n```python\ndef rpn_graph(feature_map, anchors_per_location, anchor_stride):\n    \"\"\"Builds the computation graph of Region Proposal Network.\n    feature_map: backbone features [batch, height, width, depth]\n    anchors_per_location: number of anchors per pixel in the feature map\n    anchor_stride: Controls the density of anchors. Typically 1 (anchors for\n                   every pixel in the feature map), or 2 (every other pixel).\n    Returns:\n        rpn_logits: [batch, H, W, 2] Anchor classifier logits (before softmax)\n        rpn_probs: [batch, H, W, 2] Anchor classifier probabilities.\n        rpn_bbox: [batch, H, W, (dy, dx, log(dh), log(dw))] Deltas to be\n                  applied to anchors.\n    \"\"\"\n    # TODO: check if stride of 2 causes alignment issues if the featuremap\n    #       is not even.\n    # Shared convolutional base of the RPN\n    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu',\n                       strides=anchor_stride,\n                       name='rpn_conv_shared')(feature_map)\n\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding='valid',\n                  activation='linear', name='rpn_class_raw')(shared)\n\n    # Reshape to [batch, anchors, 2]\n    rpn_class_logits = KL.Lambda(\n        lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n\n    # Softmax on last dimension of BG/FG.\n    rpn_probs = KL.Activation(\n        \"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n\n    # Bounding box refinement. [batch, H, W, anchors per location, depth]\n    # where depth is [x, y, log(w), log(h)]\n    x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\",\n                  activation='linear', name='rpn_bbox_pred')(shared)\n\n    # Reshape to [batch, anchors, 4]\n    rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n```\n\n"},{"title":"Mask RCNN Code Reading for ResNet & FPN","url":"/2018/05/30/Mask-RCNN-Code-Reading-ResNet/","content":"Mask_RCNN V2.1版本\n\n檔案：[Model.py](https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py#L85)\n\n\n\nResnet的程式碼是由下面這專案參考改來的\nhttps://github.com/fchollet/deep-learning-models/blob/master/resnet50.py\n\n\n### identity_block\n這function主要是為了後面的resnet_graph做準備的, 主要是為了重複利用,對應到的就是Resnet裡面的identity_block,然後這是純粹的Resnet版本,在shortcut上沒有操作\n\ninput_tensor: 其實就是接上一層的output\nkernel_size : 基本上是3,\nfilters:程式碼中filters通常是一個list, 而通常是三個, 像是[128,128,512],代表三層分別的filter數量\nstage: 命名用,知道是哪一個stage的\nblock: 命名用,知道是stage中的第幾個idenity_block\nuse_bias: conv layer需不需要bias\ntrain_bn: 這個identity_block要freeze還是train\n\n從裡面可以看到一個完整的Identity_Block\nInput->(Conv->BN->RELU)->(Conv->BN->RELU)->(Conv->BN)->ADD(prev,Input)->RELU\n\n```python\ndef identity_block(input_tensor, kernel_size, filters, stage, block,\n                   use_bias=True, train_bn=True):\n    \"\"\"The identity_block is the block that has no conv layer at shortcut\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n        filters: list of integers, the nb_filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n        use_bias: Boolean. To use or not use a bias in conv layers.\n        train_bn: Boolean. Train or freeze Batch Norm layres\n    \"\"\"\n    nb_filter1, nb_filter2, nb_filter3 = filters\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n\n    x = KL.Conv2D(nb_filter1, (1, 1), name=conv_name_base + '2a',\n                  use_bias=use_bias)(input_tensor)\n    x = BatchNorm(name=bn_name_base + '2a')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same',\n                  name=conv_name_base + '2b', use_bias=use_bias)(x)\n    x = BatchNorm(name=bn_name_base + '2b')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    x = KL.Conv2D(nb_filter3, (1, 1), name=conv_name_base + '2c',\n                  use_bias=use_bias)(x)\n    x = BatchNorm(name=bn_name_base + '2c')(x, training=train_bn)\n\n    x = KL.Add()([x, input_tensor])\n    x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x)\n    return x\n```\n\n### Conv_block\n這邊就是純粹的Conv_block, input參數與上方相仿, 除了從stage3開始第一個Conv有subsample=(2,2), 而shortcut因此也需要subsample=(2,2)\n\n```python\ndef conv_block(input_tensor, kernel_size, filters, stage, block,\n               strides=(2, 2), use_bias=True, train_bn=True):\n    \"\"\"conv_block is the block that has a conv layer at shortcut\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n        filters: list of integers, the nb_filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n        use_bias: Boolean. To use or not use a bias in conv layers.\n        train_bn: Boolean. Train or freeze Batch Norm layres\n    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n    And the shortcut should have subsample=(2,2) as well\n    \"\"\"\n    nb_filter1, nb_filter2, nb_filter3 = filters\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n\n    x = KL.Conv2D(nb_filter1, (1, 1), strides=strides,\n                  name=conv_name_base + '2a', use_bias=use_bias)(input_tensor)\n    x = BatchNorm(name=bn_name_base + '2a')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same',\n                  name=conv_name_base + '2b', use_bias=use_bias)(x)\n    x = BatchNorm(name=bn_name_base + '2b')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n\n    x = KL.Conv2D(nb_filter3, (1, 1), name=conv_name_base +\n                  '2c', use_bias=use_bias)(x)\n    x = BatchNorm(name=bn_name_base + '2c')(x, training=train_bn)\n\n    shortcut = KL.Conv2D(nb_filter3, (1, 1), strides=strides,\n                         name=conv_name_base + '1', use_bias=use_bias)(input_tensor)\n    shortcut = BatchNorm(name=bn_name_base + '1')(shortcut, training=train_bn)\n\n    x = KL.Add()([x, shortcut])\n    x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x)\n    return x\n\n```\n\n### Resnet_Graph\nResnet主體程式碼 剩下就是call上面的function\n而ResNet50與ResNet101差異就在於中間使用for來判斷要增加幾個identity_block\n\n```python\ndef resnet_graph(input_image, architecture, stage5=False, train_bn=True):\n    \"\"\"Build a ResNet graph.\n        architecture: Can be resnet50 or resnet101\n        stage5: Boolean. If False, stage5 of the network is not created\n        train_bn: Boolean. Train or freeze Batch Norm layres\n    \"\"\"\n    assert architecture in [\"resnet50\", \"resnet101\"]\n    # Stage 1\n    x = KL.ZeroPadding2D((3, 3))(input_image)\n    x = KL.Conv2D(64, (7, 7), strides=(2, 2), name='conv1', use_bias=True)(x)\n    x = BatchNorm(name='bn_conv1')(x, training=train_bn)\n    x = KL.Activation('relu')(x)\n    C1 = x = KL.MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\")(x)\n    # Stage 2\n    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1), train_bn=train_bn)\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b', train_bn=train_bn)\n    C2 = x = identity_block(x, 3, [64, 64, 256], stage=2, block='c', train_bn=train_bn)\n    # Stage 3\n    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a', train_bn=train_bn)\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b', train_bn=train_bn)\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c', train_bn=train_bn)\n    C3 = x = identity_block(x, 3, [128, 128, 512], stage=3, block='d', train_bn=train_bn)\n    # Stage 4\n    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a', train_bn=train_bn)\n    block_count = {\"resnet50\": 5, \"resnet101\": 22}[architecture]\n    for i in range(block_count):\n        x = identity_block(x, 3, [256, 256, 1024], stage=4, block=chr(98 + i), train_bn=train_bn)\n    C4 = x\n    # Stage 5\n    if stage5:\n        x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a', train_bn=train_bn)\n        x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b', train_bn=train_bn)\n        C5 = x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c', train_bn=train_bn)\n    else:\n        C5 = None\n    return [C1, C2, C3, C4, C5]\n```\n\n\n### FPN Feature Pyramid Network\nFPN被寫在了下面 並沒有獨立一個function, 但這邊順便連帶一起講\n\nC5先通過了一個1x1的conv layer調整channel數量,得到P5\n\n```python\nP5 = KL.Conv2D(256, (1, 1), name='fpn_c5p5')(C5)\n```\n接下來P5為了要跟P4層進行運算,所以得resize, 這裡用單純的upsampling, 沒有dialated conv或是transposed conv或是bilinear interpolation.\n\n```python\nKL.UpSampling2D(size=(2, 2),name=\"fpn_p5upsampled\")(P5)\n```\n所以P4層就是把原本的C4調整過channel數量以後 跟upsamplinge過後的P5相加\n\n```python\nP4 = KL.Add(name=\"fpn_p4add\")\n    ([KL.UpSampling2D(size=(2, 2), name=\"fpn_p5upsampled\")(P5),\n      KL.Conv2D(256,(1, 1), name='fpn_c4p4')(C4)])\n```\n\nFPN完整程式碼如下\nP6是用來給RPN用的, 不是給FPN用的\n另外P2~P5又做了一次3x3的conv 是為了消除upsampling的混疊效應 \n\n\n```python\n        # Build the shared convolutional layers.\n        # Bottom-up Layers\n        # Returns a list of the last layers of each stage, 5 in total.\n        # Don't create the thead (stage 5), so we pick the 4th item in the list.\n        _, C2, C3, C4, C5 = resnet_graph(input_image, config.BACKBONE,\n                                         stage5=True, train_bn=config.TRAIN_BN)\n        # Top-down Layers\n        # TODO: add assert to varify feature map sizes match what's in config\n        P5 = KL.Conv2D(256, (1, 1), name='fpn_c5p5')(C5)\n        P4 = KL.Add(name=\"fpn_p4add\")([\n            KL.UpSampling2D(size=(2, 2), name=\"fpn_p5upsampled\")(P5),\n            KL.Conv2D(256, (1, 1), name='fpn_c4p4')(C4)])\n        P3 = KL.Add(name=\"fpn_p3add\")([\n            KL.UpSampling2D(size=(2, 2), name=\"fpn_p4upsampled\")(P4),\n            KL.Conv2D(256, (1, 1), name='fpn_c3p3')(C3)])\n        P2 = KL.Add(name=\"fpn_p2add\")([\n            KL.UpSampling2D(size=(2, 2), name=\"fpn_p3upsampled\")(P3),\n            KL.Conv2D(256, (1, 1), name='fpn_c2p2')(C2)])\n        # Attach 3x3 conv to all P layers to get the final feature maps.\n        P2 = KL.Conv2D(256, (3, 3), padding=\"SAME\", name=\"fpn_p2\")(P2)\n        P3 = KL.Conv2D(256, (3, 3), padding=\"SAME\", name=\"fpn_p3\")(P3)\n        P4 = KL.Conv2D(256, (3, 3), padding=\"SAME\", name=\"fpn_p4\")(P4)\n        P5 = KL.Conv2D(256, (3, 3), padding=\"SAME\", name=\"fpn_p5\")(P5)\n        # P6 is used for the 5th anchor scale in RPN. Generated by\n        # subsampling from P5 with stride of 2.\n        P6 = KL.MaxPooling2D(pool_size=(1, 1), strides=2, name=\"fpn_p6\")(P5)\n\n        # Note that P6 is used in RPN, but not in the classifier heads.\n        rpn_feature_maps = [P2, P3, P4, P5, P6]\n        mrcnn_feature_maps = [P2, P3, P4, P5]\n\n```\n\n"},{"title":"Mask RCNN Code Reading","url":"/2018/05/30/Mask-RCNN-Code-Reading/","content":"\nBackBone :\n\n- [ResNet and FPN]() FPN的code其實作者寫在了model.py的Class MaskRCNN裡面並沒有獨立出來, 而其餘ResNet有\n\nRegion Proposal Network:\n\n- [Anchor Boxes Generate]() 產生anchor boxes的code\n- [RPN]() RPN網路架構的code\n- [Proposal Layer]() 將Anchor Boxes Generate與RPN結合的部分並會經過NMS\n\n\nTASK_HEAD:\n\n- [FPN Classifier Graph]() 有關於[ROI Align Layer]()還有分類的網路架構的部分\n- [Build FPN Mask Graph]() 有關於[ROI Align Layer]()還有Mask的網路架構的部分\n- [Detection Layer]() FPN Classifier Graph後,按照confidence還有數量篩選, 算是後處理了\n- [Detection Target Layer]() Proposal Layer後,Training的code\n- [Mask RCNN Loss]()有關於所有的loss\n\n\n\n\n\n\n如果是training的話\n\n```python\n if mode == \"training\":\n    # Class ID mask to mark class IDs supported by the dataset the image\n    # came from.\n    active_class_ids = KL.Lambda(\n     lambda x: parse_image_meta_graph(x)[\"active_class_ids\"]\n     )(input_image_meta)\n    \n    if not config.USE_RPN_ROIS:\n     # Ignore predicted ROIs and use ROIs provided as an input.\n     input_rois = KL.Input(shape=[config.POST_NMS_ROIS_TRAINING, 4],\n                           name=\"input_roi\", dtype=np.int32)\n     # Normalize coordinates\n     target_rois = KL.Lambda(lambda x: norm_boxes_graph(\n         x, K.shape(input_image)[1:3]))(input_rois)\n    else:\n     target_rois = rpn_rois\n    \n    # Generate detection targets\n    # Subsamples proposals and generates target outputs for training\n    # Note that proposal class IDs, gt_boxes, and gt_masks are zero\n    # padded. Equally, returned rois and targets are zero padded.\n    rois, target_class_ids, target_bbox, target_mask =\\\n     DetectionTargetLayer(config, name=\"proposal_targets\")([\n         target_rois, input_gt_class_ids, gt_boxes, input_gt_masks])\n    \n    # Network Heads\n    # TODO: verify that this handles zero padded ROIs\n    mrcnn_class_logits, mrcnn_class, mrcnn_bbox =\\\n     fpn_classifier_graph(rois, mrcnn_feature_maps, input_image_meta,\n                          config.POOL_SIZE, config.NUM_CLASSES,\n                          train_bn=config.TRAIN_BN)\n    \n    mrcnn_mask = build_fpn_mask_graph(rois, mrcnn_feature_maps,\n                                   input_image_meta,\n                                   config.MASK_POOL_SIZE,\n                                   config.NUM_CLASSES,\n                                   train_bn=config.TRAIN_BN)\n    \n    # TODO: clean up (use tf.identify if necessary)\n    output_rois = KL.Lambda(lambda x: x * 1, name=\"output_rois\")(rois)\n    \n    # Losses\n    rpn_class_loss = KL.Lambda(lambda x: rpn_class_loss_graph(*x), name=\"rpn_class_loss\")(\n     [input_rpn_match, rpn_class_logits])\n    rpn_bbox_loss = KL.Lambda(lambda x: rpn_bbox_loss_graph(config, *x), name=\"rpn_bbox_loss\")(\n     [input_rpn_bbox, input_rpn_match, rpn_bbox])\n    class_loss = KL.Lambda(lambda x: mrcnn_class_loss_graph(*x), name=\"mrcnn_class_loss\")(\n     [target_class_ids, mrcnn_class_logits, active_class_ids])\n    bbox_loss = KL.Lambda(lambda x: mrcnn_bbox_loss_graph(*x), name=\"mrcnn_bbox_loss\")(\n     [target_bbox, target_class_ids, mrcnn_bbox])\n    mask_loss = KL.Lambda(lambda x: mrcnn_mask_loss_graph(*x), name=\"mrcnn_mask_loss\")(\n     [target_mask, target_class_ids, mrcnn_mask])\n    \n    # Model\n    inputs = [input_image, input_image_meta,\n           input_rpn_match, input_rpn_bbox, input_gt_class_ids, input_gt_boxes, input_gt_masks]\n    if not config.USE_RPN_ROIS:\n     inputs.append(input_rois)\n    outputs = [rpn_class_logits, rpn_class, rpn_bbox,\n            mrcnn_class_logits, mrcnn_class, mrcnn_bbox, mrcnn_mask,\n            rpn_rois, output_rois,\n            rpn_class_loss, rpn_bbox_loss, class_loss, bbox_loss, mask_loss]\n    model = KM.Model(inputs, outputs, name='mask_rcnn')\n\n```\n\n如果是測試的話\n\n```python\nelse:\n    # Network Heads\n    # Proposal classifier and BBox regressor heads\n    mrcnn_class_logits, mrcnn_class, mrcnn_bbox =\\\n     fpn_classifier_graph(rpn_rois, mrcnn_feature_maps, input_image_meta,\n                          config.POOL_SIZE, config.NUM_CLASSES,\n                          train_bn=config.TRAIN_BN)\n    \n    # Detections\n    # output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in \n    # normalized coordinates\n    detections = DetectionLayer(config, name=\"mrcnn_detection\")(\n     [rpn_rois, mrcnn_class, mrcnn_bbox, input_image_meta])\n    \n    # Create masks for detections\n    detection_boxes = KL.Lambda(lambda x: x[..., :4])(detections)\n    mrcnn_mask = build_fpn_mask_graph(detection_boxes, mrcnn_feature_maps,\n                                   input_image_meta,\n                                   config.MASK_POOL_SIZE,\n                                   config.NUM_CLASSES,\n                                   train_bn=config.TRAIN_BN)\n    \n    model = KM.Model([input_image, input_image_meta, input_anchors],\n                  [detections, mrcnn_class, mrcnn_bbox,\n                      mrcnn_mask, rpn_rois, rpn_class, rpn_bbox],\n                  name='mask_rcnn')      \n```     \n\n\n\n以下是別人網誌上的舊圖 不太依樣了 build_classfier_grpah拿掉了 上圖是inference下圖是train\n![](/media/15280598887927.jpg)\n![](/media/15280601089173.jpg)\n\n\n"},{"title":"Optimize /","url":"/2018/04/07/Optimize/","content":"\n\n","categories":["Optimize"]},{"title":"Deep Residual Network","url":"/2018/04/07/Resnet-Deep-Residual-Network/","content":"\n\n","tags":["CNN","Model Architecture","Convolution","Classification"],"categories":["Model Architecture","Classification"]},{"title":"VGG-VERY DEEP CONVOLUTIONAL NETWORK SFOR LARGE-SCALE IMAGE RECOGNITION","url":"/2018/04/07/VGG-VERY-DEEP-CONVOLUTIONAL-NETWORK-SFOR-LARGE-SCALE-IMAGE-RECOGNITION/","content":"\n## 前言\n這篇的摘要寫說針對AlexNet的基礎上主要做了兩方面的改進：\n1.使用了最小的3x3卷積核尺寸和最小間隔。\n2. 在整個圖片和multi-scale上訓練和測試圖片。\n論文原句：1. Use smaller receptive window size and smaller stride of the first convolutional layer.\n2.Training and testing the networks densely over the whole image and over multiple scales.\n\n```\nAlexNet做改進, 所以VGG本身的問題並不會在這篇討論, 會在ResNet那篇再探討。\n```\n![](/media/15231088858941.jpg)\n\n![](/media/15231088789196.jpg)\n\n\n## 架構\n![](/media/15231067749133.jpg)\n![](/media/15231137520393.jpg)\n\n採用較小的Filter尺寸-3x3，卷積的間隔s=1:\n1：3x3是最小的能夠捕獲上下左右和中心概念的尺寸。\n2：兩個3x3的捲基層的有限感受野是5x5；三個3x3的感受野是7x7，可以替代大的filter尺寸。\n3：多個3x3的捲基層比一個大尺寸filter卷基層有更多的非線性，使得判決函數更加具有判決性。\n4：多個3x3的捲積層比一個大尺寸的filter有更少的參數，假設卷基層的輸入和輸出的特徵圖大小相同為C，那麼三個3x3的捲積層參數個數3x（3x3xCxC）=27CC；一個7x7的捲積層參數為49CC；所以可以把三個3x3的filter看成是一個7x7filter的分解（中間層有非線性的分解）。\n也使用過1x1 filter:\n作用是在不影響輸入輸出維數的情況下，對輸入進行線性形變，然後通過Relu進行非線性處理，增加網絡的非線性表達能力。\nMax-Pooling：2x2，間隔s=2；\n\n論文中解釋關於僅使用3x3 conv kernel，因為兩個3x3的conv kernel疊合的reception field等效於一個5x5 conv kernel(亦即每個pixel可以correlate到周圍的5x5個pixel), 而三個3x3則可以等效於一個7x7，但兩層3x3的參數量僅有一層5x5的(3x3x2)/(5x5) = 0.72倍，而三層3x3參數量是一層7x7的(3x3x3)/(7x7)=0.55倍，對應到的範圍等效並且可使得需參數量更少，並且疊越多層Conv+ReLU的特徵學習能力比單一層Conv+ReLU來的更好。\n![](/media/15231135640564.jpg)\n\n1. Filter Size\n相對於AlexNet 每個Stage僅含有一個Conv層, Filter 7x7, VGG每個Stage有2~4個Conv層,而Filter只有3x3, 原論文內\"This can be seen as imposing a regularization on the 7 × 7 conv. filters, forcing them to have a decomposition through the 3 × 3 filters\"\n他說7x7 filter可以被分解成若干个3x3的filter的叠加。\n類比一下n維空間的向量x，x的正交分解\nx = x1(1, 0, 0, ....) + x2(0, 1, 0, ...) + x3(0, 0, 1,...) + ... + xn(0, 0, 0, ..., 1)\n\n每一組的每一層的filter被類比成n維歐幾里得空間的基底。\n若VGG的一組含有3層3x3的filter，則我們則假設一個7x7的filter可以被分解成3種“正交”的3x3的filter。\n\n作者原文：First, we incorporate three non-linearrectification layers instead of a single one, which makes the decision function more discriminative.Second, we decrease the number of parameters: assuming that both the input and the output of athree-layer 3 × 3 convolution stack has C channels, the stack is parametrised by 3 32C^2 = 27C^2weights; at the same time, a single 7 × 7 conv. layer would require 72C^2 = 49C^2\n\n\n\n後來googleNet\n將3×3的卷積分解成31和13的卷積，可以減少33％的計算量，如果將3×3分解為兩個2×2的，可以減少11％的計算量，而且利用非對稱卷積的效果還更好。\n實踐表明，不要過早的使用這種分解操作，在特徵映射大小為（12〜20）之間，使用它，效果是比較好的。 ![](/media/15231135906645.jpg)\n\n\n","tags":["CNN","Model Architecture","Convolution","Classification"],"categories":["Model Architecture","Classification"]},{"title":"CNN模型壓縮系列","url":"/2018/04/07/Model-Compression/","content":"\n## 網絡修剪\n網絡修剪，採用當網絡權重非常小的時候（小於某個設定的閾值），把它置0，就像二值網絡一般;然後屏蔽被設置為0的權重更新，繼續進行訓練;以此循環，每隔訓練幾輪過後，繼續進行修剪。\n例如Deep-Compression這篇paper\n## 權重共享\n對於每一層的參數，我們進行k-均值聚類，進行量化，對於歸屬於同一個聚類中心的權重，採用共享一個權重，進行重新訓練。需要注意的是這個權重共享並不是層之間的權重共享，這是對於每一層的單獨共享\n\n## 增加L2權重\n增加L2權重可以讓更多的權重，靠近0，這樣每次修剪的比例大大增加。\n\n## 從結構上，簡化網絡計算，\n這些需要自己閱讀比較多相關文獻，才能設計出合理，速度更快的網絡，比如引入消防模塊，NIN，除全連接層等一些設計思想，這邊不進行具體詳述。\nSqueezeNet\n\n## 其他\nHuffman Coding\n\n\n\n","tags":["CNN","Model Architecture","Convolution","Model Compression"],"categories":["Model Architecture"]},{"title":"Instance Segmentation Series","url":"/2018/03/16/2018-03-27/","content":"\nMNC\nFCIS\nMask R-CNN\nPANet\n\n\n","categories":["Series","Segmentation"]},{"title":"Light Head R-CNN","url":"/2018/03/16/Light Head R-CNN/","content":"\n\n\n\n\n\n\n[旷视face++研究院解读Light-Head R-CNN](/media/%E6%97%B7%E8%A7%86face++%E7%A0%94%E7%A9%B6%E9%99%A2%E8%A7%A3%E8%AF%BBLight-Head%20R-CNN.mp4)\n\n\n"},{"title":"Tensorflow Function Usage And Example","url":"/2018/03/16/Tensorflow-Function-Usage-And-Example/","content":"\nMini Tensorflow example\n```python\nimport tensorflow as tf\n a = tf.placeholder(\"float\")\n b = tf.placeholder(\"float\")\n\n y = tf.mul(a, b) \n sess = tf.Session()\nprint sess.run(y, feed_dict={a: 3, b: 3})\n sess.close()\n```\n\n\n| 操作組                        | 操作       |\n|----------------------------|------------------------------------------------------|  \n| Maths                      | Add, Sub, Mul, Div, Exp, Log, Greater, Less, Equal   | \n| Array                      | Concat, Slice, Split, Constant, Rank, Shape, Shuffle | \n| Matrix                     | MatMul, MatrixInverse, MatrixDeterminant             | \n| Neuronal Network           | SoftMax, Sigmoid, ReLU, Convolution2D, MaxPool       | \n| Checkpointing              | Save, Restore                                        | \n| Queues and syncronizations | Enqueue, Dequeue, MutexAcquire, MutexRelease         | \n| Flow control               | Merge, Switch, Enter, Leave, NextIteration           | \n\n\nTensorFlow的算術操作如下：\n\n| 操作組                        | 操作       |\n|----------------------------|------------------------------------------------------|  \n| tf.add(x, y, name=None)                  | 求和                                                       | \n| tf.sub(x, y, name=None)                  | 減法                                                       | \n| tf.mul(x, y, name=None)                  | 乘法                                                       | \n| tf.div(x, y, name=None)                  | 除法                                                       | \n| tf.mod(x, y, name=None)                  | 取模                                                       | \n| tf.abs(x, name=None)                     | 求絕對值                                                     | \n| tf.neg(x, name=None)                     | 取負 (y = -x).                                             | \n| tf.sign(x, name=None)                    | 返回符號 y = sign(x) = -1 if x < 0; 0 if x == 0; 1 if x > 0. | \n| tf.inv(x, name=None)                     | 取反                                                       | \n| tf.square(x, name=None)                  | 計算平方 (y = x * x = x^2).                                  | \n| tf.round(x, name=None)                   | 舍入最接近的整數      <br># ‘a’ is [0.9, 2.5, 2.3, -4.4]       <br> tf.round(a) ==> [ 1.0, 3.0, 2.0, -4.0 ]                                  | \n| tf.sqrt(x, name=None)|    <br>開根號 (y = \\sqrt{x} = x^{1/2}).     | \n|tf.pow(x, y, name=None)    |                     冪次方                                                     <br># tensor ‘x’ is [[2, 2], [3, 3]]          <br># tensor ‘y’ is [[8, 16], [2, 3]] <br>tf.pow(x, y) ==> [[256, 65536], [9, 27]] |                                                          | \n| tf.exp(x, name=None)                     | 計算e的次方                                                   | \n| tf.log(x, name=None)                     | 計算log，一個輸入計算e的ln，兩輸入以第二輸入為底                              | \n| tf.maximum(x, y, name=None)              | 返回最大值 (x > y ? x : y)                                    | \n| tf.minimum(x, y, name=None)              | 返回最小值 (x < y ? x : y)                                    | \n| tf.cos(x, name=None)                     | 三角函數cosine                                               | \n| tf.sin(x, name=None)                     | 三角函數sine                                                 | \n| tf.tan(x, name=None)                     | 三角函數tan                                                  | \n| tf.atan(x, name=None)                    |三角函數ctan                                            | \n\n## 張量操作Tensor Transformations\n####數據類型轉換Casting\n| 操作組                        | 操作       |\n|----------------------------|------------------------------------------------------|  \n| tf.string_to_number                              |                      | \n| (string_tensor, out_type=None, name=None)        | 字符串轉為數字              | \n| tf.to_double(x, name=’ToDouble’)                 | 轉為64位浮點類型–float64    | \n| tf.to_float(x, name=’ToFloat’)                   | 轉為32位浮點類型–float32    | \n| tf.to_int32(x, name=’ToInt32’)                   | 轉為32位整型–int32        | \n| tf.to_int64(x, name=’ToInt64’)                   | 轉為64位整型–int64        | \n| tf.cast(x, dtype, name=None)                     | 將x或者x.values轉換為dtype | \n| # tensor a is [1.8, 2.2], dtype=tf.float         |                      | \n| tf.cast(a, tf.int32) ==> [1, 2] # dtype=tf.int32 |                      | \n\n#### 形狀操作Shapes and Shaping\n| 操作組                        | 操作       |\n|----------------------------|------------------------------------------------------| \n| tf.shape(input, name=None)                                  | 返回數據的shape       <br> # ‘t’ is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]  <br> shape(t) ==> [2, 2, 3]                                      |                  | \n| tf.size(input, name=None)                                   | 返回數據的元素數量 <br> # ‘t’ is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]]  <br> size(t) ==> 12                                              |                  | \n| tf.rank(input, name=None)                                   | 返回tensor的rank    <br> 注意：此rank不同於矩陣的rank， <br> tensor的rank表示一個tensor需要的索引數目來唯一表示任何一個元素 <br> 也就是通常所説的 “order”, “degree”或”ndims”   <br> #’t’ is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]  <br> # shape of tensor ‘t’ is [2, 2, 3]                          <br> rank(t) ==> 3                                               |                  | \n| tf.reshape(tensor, shape, name=None)                        | 改變tensor的形狀      <br> # tensor ‘t’ is [1, 2, 3, 4, 5, 6, 7, 8, 9] <br> # tensor ‘t’ has shape [9]   <br> reshape(t, [3, 3]) ==>                                      <br> [[1, 2, 3],                                                 <br> [4, 5, 6],                                                  <br> [7, 8, 9]]                                                  <br> #如果shape有元素[-1],表示在該維度打平至一維                                 <br> # -1 將自動推導得為 9:                                             <br> reshape(t, [2, -1]) ==>                                     <br> [[1, 1, 1, 2, 2, 2, 3, 3, 3],                               <br> [4, 4, 4, 5, 5, 5, 6, 6, 6]]                                |                  | \n| tf.expand_dims(input, dim, name=None)                       | 插入維度1進入一個tensor中 <br> #該操作要求-1-input.dims()                                       <br> # ‘t’ is a tensor of shape [2]                              <br> shape(expand_dims(t, 0)) ==> [1, 2]                         <br> shape(expand_dims(t, 1)) ==> [2, 1]                         <br>shape(expand_dims(t, -1)) ==> [2, 1] <= dim <= input.dims() |                  | \n\t\nhttps://hk.saowen.com/a/2766b13f38b54ab09e6d478975f5feca8abbf01d842e2ca8f5111160fcbb0e36\n### tf.reshape\ntf.reshape(tensor,shape, name=None) \n\n重組\n\n```python\n# tensor \n't' is [1, 2, 3, 4, 5, 6, 7, 8, 9]\n# tensor \n't'has shape [9]\n\nreshape(t, [3, 3]) \n==> \n[[1, 2, 3],\n[4, 5, 6],\n[7, 8, 9]]\n```\n\n降為\n\n```python\n# tensor \n't'is \n[[[1, 1], [2, 2]],                \n[[3, 3], [4, 4]]]\n# tensor \n't'has shape [2, 2, 2]\nreshape(t, [2, 4]) \n==> \n[[1, 1, 2, 2],\n[3, 3, 4, 4]]\n```\n\n平坦\n\n```python\n# tensor \n't' has shape [3, 2, 3]\n# pass \n'[-1]'\n to flatten \n't'\nreshape(t, [-1]) \n==> [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6]\n```\n\n### tf.split\ntf.split(split_dim, num_split, value, name='split')\n將大的tensor分割成更小的tensor，第一個參數代表沿著那一維開始分割，第二個參數代表切成幾段\n\n\n\n### tf.nn.max_pool\nmax pooling 是CNN 當中的最大值池化操作，其實用法和卷積很類似\n\ntf.nn.max_pool(value,ksize, strides, padding, name=None)\n\n````\n參數是四個，和卷積很類似：\n\n第一個參數value ：需要池化的輸入，一般池化層接在卷積層後面，所以輸入通常是feature map ，依然是[batch, height, width, channels] 這樣的shape\n\n第二個參數ksize ：池化窗口的大小，取一個四維向量，一般是[1, height, width, 1] ，因為我們不想在batch 和channels 上做池化，所以這兩個維度設為了1\n\n第三個參數strides ：和卷積類似，窗口在每一個維度上滑動的步長，一般也是[1, stride, stride ,1]\n\n第四個參數padding ：和卷積類似，可以取'VALID' 或者'SAME'\n返回一個Tensor ，類型不變，shape 仍然是[batch, height, width, channels] 這種形式\n````\n\n假設有這樣一張圖，雙通道\n第一個通道：\n![](/media/15225714198554.jpg)\n第二個通道：\n![](/media/15225714248800.jpg)\n\n```python\nimport tensorflow as tf\n\na=tf.constant([\n\n        [[1.0,2.0,3.0,4.0],\n\n       [5.0,6.0,7.0,8.0],\n\n       [8.0,7.0,6.0,5.0],\n\n       [4.0,3.0,2.0,1.0]],\n\n       [[4.0,3.0,2.0,1.0],\n\n        [8.0,7.0,6.0,5.0],\n\n        [1.0,2.0,3.0,4.0],\n\n        [5.0,6.0,7.0,8.0]]\n\n    ])\n\na=tf.reshape(a,[1,4,4,2])\npooling=tf.nn.max_pool(a,[1,2,2,1],[1,1,1,1],padding='VALID')\n\nwith tf.Session() as sess:\n   print(\"image:\")\n   image=sess.run(a)\n   print (image)\n   print(\"reslut:\")\n   result=sess.run(pooling)\n   print (result)\n```\n\n這裡步長為1 ，窗口大小2×2 ，輸出結果：\n\nimage:\n\n```python\n[[\n[[ 1. 2.]\n[ 3. 4.]\n[ 5. 6.]\n[ 7. 8.]]\n\n[[ 8. 7.]\n[ 6. 5.]\n[ 4. 3.]\n[ 2. 1.]]\n\n[[ 4. 3.]\n[ 2. 1.]\n[ 8. 7.]\n[ 6. 5.]]\n\n[[ 1. 2.]\n[ 3. 4.]\n[ 5. 6.]\n[ 7. 8.]]]]\n\nreslut:\n\n[[\n[[ 8. 7.]\n[ 6. 6.]\n[ 7. 8.]]\n\n[[ 8. 7.]\n[ 8. 7.]\n[ 8. 7.]]\n\n[[ 4. 4.]\n[ 8. 7.]\n[ 8. 8.]]]]\n```\n池化後的圖就是：\n\n![](/media/15225714762104.jpg)\n\n![](/media/15225714795456.jpg)\n\n證明了程序的結果是正確的。\n我們還可以改變步長\n\n```python\npooling=tf.nn.max_pool(a,[1,2,2,1],[1,2,2,1],padding='VALID')\n\n最後的result 就變成：\n\nreslut:\n[[[[ 8. 7.]\n[ 7. 8.]]\n[[ 4. 4.]\n[ 8. 8.]]]]\n\n```\n\n\n"},{"title":"Mask R-CNN","url":"/2018/03/10/Mask R-CNN/","content":"\n## Overview\n![](/media/15222419162572.jpg)\n\n![](/media/15144439857026.jpg)\n\n## Concept \n\nMask R-CNN 利用了相當簡潔與彈性的方法進行實例分割, 主要跟 Faster R-CNN 不同的地方在於原架構有 2 個分支 \n\n1. Classification branch \n2. Bounding box branch \n\n而 Mask R-CNN 的方法則是多加了另一個分支  --- Mask branch。\n\n## Mask branch\n\n\n\n## Loss function \n\nL<sub>total</sub> = L<sub>cls</sub> + L<sub>box</sub>  + L<sub>mask</sub> \n\nL<sub>cls</sub> 跟 L<sub>box</sub>的可以參考Fast-RCNN\n\n````\nJ(θ)=−1m[∑mi=1y(i)loghθ(x(i))+(1−y(i))log(1−hθ(x(i)))]\n````\n\n## ROI Alignment\n### ROI POOLING\nROI POOLING 概念如下圖所示\n![](/media/15137535435975.png)\n\n![](/media/15137536007979.gif)\n\n```\n\\mathcal {F_{i}^{RoI}}_{(u^\\prime,v^\\prime)} = \\sum_{(u,v)}^{W \\times H} G(u,v;u^\\prime, v^\\prime|B_i) \\mathcal F_(u,v)\n\n```\n\n","tags":["Deep Learning","R-CNN","Segmentation","Pose Estimation"],"categories":["Segmentation","Detection"]},{"title":"Mask R-CNN","url":"/2018/03/10/Mask R-CNN Code Reading/","content":"\n\n\nhttps://github.com/matterport/Mask_RCNN\n\n\n\n#model.py\n\n1. import 需要的東西 tf ver>1.3 && keras ver>2.0.8\n\n```python\nimport os\nimport sys\nimport glob\nimport random\nimport math\nimport datetime\nimport itertools\nimport json\nimport re\nimport logging\nfrom collections import OrderedDict\nimport multiprocessing\nimport numpy as np\nimport skimage.transform\nimport tensorflow as tf\nimport keras\nimport keras.backend as K\nimport keras.layers as KL\nimport keras.initializers as KI\nimport keras.engine as KE\nimport keras.models as KM\n\nimport utils\n\n# Requires TensorFlow 1.3+ and Keras 2.0.8+.\nfrom distutils.version import LooseVersion\nassert LooseVersion(tf.__version__) >= LooseVersion(\"1.3\")\nassert LooseVersion(keras.__version__) >= LooseVersion('2.0.8')\n\n```\n\n2. 封裝Mask R-CNN \n\n```python\nclass MaskRCNN():\n    \"\"\"Encapsulates the Mask RCNN model functionality.\n\n    The actual Keras model is in the keras_model property.\n    \"\"\"\n    ## Config 設定是由外部傳入，像是ballon.py裡面有關於batch_size、Iteration等等，詳細內容去看config.py\n    \n    def __init__(self, mode, config, model_dir):\n        \"\"\"\n        mode: Either \"training\" or \"inference\"\n        config: A Sub-class of the Config class\n        model_dir: Directory to save training logs and trained weights\n        \"\"\"\n        assert mode in ['training', 'inference']\n        self.mode = mode\n        self.config = config\n        self.model_dir = model_dir\n        self.set_log_dir()\n        self.keras_model = self.build(mode=mode, config=config)\n\n    def build(self, mode, config):\n        \"\"\"Build Mask R-CNN architecture.\n            input_shape: The shape of the input image.\n            mode: Either \"training\" or \"inference\". The inputs and\n                outputs of the model differ accordingly.\n        \"\"\"\n        assert mode in ['training', 'inference']\n        \n        # 強迫要求一定要是 長寬一定要是32的倍數，不然downscaling跟upscaling會有問題\n        # 尤其是convultion到後面Resnet是縮放了32倍\n        # Image size must be dividable by 2 multiple times\n        h, w = config.IMAGE_SHAPE[:2]\n        if h / 2**6 != int(h / 2**6) or w / 2**6 != int(w / 2**6):\n            raise Exception(\"Image size must be dividable by 2 at least 6 times \"\n                            \"to avoid fractions when downscaling and upscaling.\"\n                            \"For example, use 256, 320, 384, 448, 512, ... etc. \")\n        \n        # config.IMAGE_SHAPE.tolist() = [1024,1024,3]\n        # input_image_meta有點意義不明\n        # 此段落是建立需要的輸入，都用KL.Input來轉化\n        # Inputs\n        input_image = KL.Input(\n            shape=config.IMAGE_SHAPE.tolist(), name=\"input_image\")\n        input_image_meta = KL.Input(shape=[None], name=\"input_image_meta\")\n        if mode == \"training\":\n            # RPN GT\n            input_rpn_match = KL.Input(\n                shape=[None, 1], name=\"input_rpn_match\", dtype=tf.int32)\n            input_rpn_bbox = KL.Input(\n                shape=[None, 4], name=\"input_rpn_bbox\", dtype=tf.float32)\n\n            # Detection GT (class IDs, bounding boxes, and masks)\n            # 1. GT Class IDs (zero padded)\n            input_gt_class_ids = KL.Input(\n                shape=[None], name=\"input_gt_class_ids\", dtype=tf.int32)\n            # 2. GT Boxes in pixels (zero padded)\n            # [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)] in image coordinates\n            input_gt_boxes = KL.Input(\n                shape=[None, 4], name=\"input_gt_boxes\", dtype=tf.float32)\n            # Normalize coordinates\n            h, w = K.shape(input_image)[1], K.shape(input_image)[2]\n            image_scale = K.cast(K.stack([h, w, h, w], axis=0), tf.float32)\n            gt_boxes = KL.Lambda(lambda x: x / image_scale)(input_gt_boxes)\n            # 3. GT Masks (zero padded)\n            # [batch, height, width, MAX_GT_INSTANCES]\n            if config.USE_MINI_MASK:\n                input_gt_masks = KL.Input(\n                    shape=[config.MINI_MASK_SHAPE[0],\n                           config.MINI_MASK_SHAPE[1], None],\n                    name=\"input_gt_masks\", dtype=bool)\n            else:\n                input_gt_masks = KL.Input(\n                    shape=[config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1], None],\n                    name=\"input_gt_masks\", dtype=bool)\n\n        # Build the shared convolutional layers.\n        # Bottom-up Layers\n        # Returns a list of the last layers of each stage, 5 in total.\n        # Don't create the thead (stage 5), so we pick the 4th item in the list.\n        _, C2, C3, C4, C5 = resnet_graph(input_image, config.BACKBONE, stage5=True)\n        # Top-down Layers\n        # TODO: add assert to varify feature map sizes match what's in config\n        P5 = KL.Conv2D(256, (1, 1), name='fpn_c5p5')(C5)\n        P4 = KL.Add(name=\"fpn_p4add\")([\n            KL.UpSampling2D(size=(2, 2), name=\"fpn_p5upsampled\")(P5),\n            KL.Conv2D(256, (1, 1), name='fpn_c4p4')(C4)])\n        P3 = KL.Add(name=\"fpn_p3add\")([\n            KL.UpSampling2D(size=(2, 2), name=\"fpn_p4upsampled\")(P4),\n            KL.Conv2D(256, (1, 1), name='fpn_c3p3')(C3)])\n        P2 = KL.Add(name=\"fpn_p2add\")([\n            KL.UpSampling2D(size=(2, 2), name=\"fpn_p3upsampled\")(P3),\n            KL.Conv2D(256, (1, 1), name='fpn_c2p2')(C2)])\n        # Attach 3x3 conv to all P layers to get the final feature maps.\n        P2 = KL.Conv2D(256, (3, 3), padding=\"SAME\", name=\"fpn_p2\")(P2)\n        P3 = KL.Conv2D(256, (3, 3), padding=\"SAME\", name=\"fpn_p3\")(P3)\n        P4 = KL.Conv2D(256, (3, 3), padding=\"SAME\", name=\"fpn_p4\")(P4)\n        P5 = KL.Conv2D(256, (3, 3), padding=\"SAME\", name=\"fpn_p5\")(P5)\n        # P6 is used for the 5th anchor scale in RPN. Generated by\n        # subsampling from P5 with stride of 2.\n        P6 = KL.MaxPooling2D(pool_size=(1, 1), strides=2, name=\"fpn_p6\")(P5)\n\n        # Note that P6 is used in RPN, but not in the classifier heads.\n        rpn_feature_maps = [P2, P3, P4, P5, P6]\n        mrcnn_feature_maps = [P2, P3, P4, P5]\n\n        # Generate Anchors\n        self.anchors = utils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES,\n                                                      config.RPN_ANCHOR_RATIOS,\n                                                      config.BACKBONE_SHAPES,\n                                                      config.BACKBONE_STRIDES,\n                                                      config.RPN_ANCHOR_STRIDE)\n\n        # RPN Model\n        rpn = build_rpn_model(config.RPN_ANCHOR_STRIDE,\n                              len(config.RPN_ANCHOR_RATIOS), 256)\n        # Loop through pyramid layers\n        layer_outputs = []  # list of lists\n        for p in rpn_feature_maps:\n            layer_outputs.append(rpn([p]))\n        # Concatenate layer outputs\n        # Convert from list of lists of level outputs to list of lists\n        # of outputs across levels.\n        # e.g. [[a1, b1, c1], [a2, b2, c2]] => [[a1, a2], [b1, b2], [c1, c2]]\n        output_names = [\"rpn_class_logits\", \"rpn_class\", \"rpn_bbox\"]\n        outputs = list(zip(*layer_outputs))\n        outputs = [KL.Concatenate(axis=1, name=n)(list(o))\n                   for o, n in zip(outputs, output_names)]\n\n        rpn_class_logits, rpn_class, rpn_bbox = outputs\n\n        # Generate proposals\n        # Proposals are [batch, N, (y1, x1, y2, x2)] in normalized coordinates\n        # and zero padded.\n        proposal_count = config.POST_NMS_ROIS_TRAINING if mode == \"training\"\\\n            else config.POST_NMS_ROIS_INFERENCE\n        rpn_rois = ProposalLayer(proposal_count=proposal_count,\n                                 nms_threshold=config.RPN_NMS_THRESHOLD,\n                                 name=\"ROI\",\n                                 anchors=self.anchors,\n                                 config=config)([rpn_class, rpn_bbox])\n\n        if mode == \"training\":\n            # Class ID mask to mark class IDs supported by the dataset the image\n            # came from.\n            _, _, _, active_class_ids = KL.Lambda(lambda x: parse_image_meta_graph(x),\n                                                  mask=[None, None, None, None])(input_image_meta)\n\n            if not config.USE_RPN_ROIS:\n                # Ignore predicted ROIs and use ROIs provided as an input.\n                input_rois = KL.Input(shape=[config.POST_NMS_ROIS_TRAINING, 4],\n                                      name=\"input_roi\", dtype=np.int32)\n                # Normalize coordinates to 0-1 range.\n                target_rois = KL.Lambda(lambda x: K.cast(\n                    x, tf.float32) / image_scale[:4])(input_rois)\n            else:\n                target_rois = rpn_rois\n\n            # Generate detection targets\n            # Subsamples proposals and generates target outputs for training\n            # Note that proposal class IDs, gt_boxes, and gt_masks are zero\n            # padded. Equally, returned rois and targets are zero padded.\n            rois, target_class_ids, target_bbox, target_mask =\\\n                DetectionTargetLayer(config, name=\"proposal_targets\")([\n                    target_rois, input_gt_class_ids, gt_boxes, input_gt_masks])\n\n            # Network Heads\n            # TODO: verify that this handles zero padded ROIs\n            mrcnn_class_logits, mrcnn_class, mrcnn_bbox =\\\n                fpn_classifier_graph(rois, mrcnn_feature_maps, config.IMAGE_SHAPE,\n                                     config.POOL_SIZE, config.NUM_CLASSES)\n\n            mrcnn_mask = build_fpn_mask_graph(rois, mrcnn_feature_maps,\n                                              config.IMAGE_SHAPE,\n                                              config.MASK_POOL_SIZE,\n                                              config.NUM_CLASSES)\n\n            # TODO: clean up (use tf.identify if necessary)\n            output_rois = KL.Lambda(lambda x: x * 1, name=\"output_rois\")(rois)\n\n            # Losses\n            rpn_class_loss = KL.Lambda(lambda x: rpn_class_loss_graph(*x), name=\"rpn_class_loss\")(\n                [input_rpn_match, rpn_class_logits])\n            rpn_bbox_loss = KL.Lambda(lambda x: rpn_bbox_loss_graph(config, *x), name=\"rpn_bbox_loss\")(\n                [input_rpn_bbox, input_rpn_match, rpn_bbox])\n            class_loss = KL.Lambda(lambda x: mrcnn_class_loss_graph(*x), name=\"mrcnn_class_loss\")(\n                [target_class_ids, mrcnn_class_logits, active_class_ids])\n            bbox_loss = KL.Lambda(lambda x: mrcnn_bbox_loss_graph(*x), name=\"mrcnn_bbox_loss\")(\n                [target_bbox, target_class_ids, mrcnn_bbox])\n            mask_loss = KL.Lambda(lambda x: mrcnn_mask_loss_graph(*x), name=\"mrcnn_mask_loss\")(\n                [target_mask, target_class_ids, mrcnn_mask])\n\n            # Model\n            inputs = [input_image, input_image_meta,\n                      input_rpn_match, input_rpn_bbox, input_gt_class_ids, input_gt_boxes, input_gt_masks]\n            if not config.USE_RPN_ROIS:\n                inputs.append(input_rois)\n            outputs = [rpn_class_logits, rpn_class, rpn_bbox,\n                       mrcnn_class_logits, mrcnn_class, mrcnn_bbox, mrcnn_mask,\n                       rpn_rois, output_rois,\n                       rpn_class_loss, rpn_bbox_loss, class_loss, bbox_loss, mask_loss]\n            model = KM.Model(inputs, outputs, name='mask_rcnn')\n        else:\n            # Network Heads\n            # Proposal classifier and BBox regressor heads\n            mrcnn_class_logits, mrcnn_class, mrcnn_bbox =\\\n                fpn_classifier_graph(rpn_rois, mrcnn_feature_maps, config.IMAGE_SHAPE,\n                                     config.POOL_SIZE, config.NUM_CLASSES)\n\n            # Detections\n            # output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in image coordinates\n            detections = DetectionLayer(config, name=\"mrcnn_detection\")(\n                [rpn_rois, mrcnn_class, mrcnn_bbox, input_image_meta])\n\n            # Convert boxes to normalized coordinates\n            # TODO: let DetectionLayer return normalized coordinates to avoid\n            #       unnecessary conversions\n            h, w = config.IMAGE_SHAPE[:2]\n            detection_boxes = KL.Lambda(\n                lambda x: x[..., :4] / np.array([h, w, h, w]))(detections)\n\n            # Create masks for detections\n            mrcnn_mask = build_fpn_mask_graph(detection_boxes, mrcnn_feature_maps,\n                                              config.IMAGE_SHAPE,\n                                              config.MASK_POOL_SIZE,\n                                              config.NUM_CLASSES)\n\n            model = KM.Model([input_image, input_image_meta],\n                             [detections, mrcnn_class, mrcnn_bbox,\n                                 mrcnn_mask, rpn_rois, rpn_class, rpn_bbox],\n                             name='mask_rcnn')\n\n        # Add multi-GPU support.\n        if config.GPU_COUNT > 1:\n            from parallel_model import ParallelModel\n            model = ParallelModel(model, config.GPU_COUNT)\n\n        return model\n\n    def find_last(self):\n        \"\"\"Finds the last checkpoint file of the last trained model in the\n        model directory.\n        Returns:\n            log_dir: The directory where events and weights are saved\n            checkpoint_path: the path to the last checkpoint file\n        \"\"\"\n        # Get directory names. Each directory corresponds to a model\n        dir_names = next(os.walk(self.model_dir))[1]\n        key = self.config.NAME.lower()\n        dir_names = filter(lambda f: f.startswith(key), dir_names)\n        dir_names = sorted(dir_names)\n        if not dir_names:\n            return None, None\n        # Pick last directory\n        dir_name = os.path.join(self.model_dir, dir_names[-1])\n        # Find the last checkpoint\n        checkpoints = next(os.walk(dir_name))[2]\n        checkpoints = filter(lambda f: f.startswith(\"mask_rcnn\"), checkpoints)\n        checkpoints = sorted(checkpoints)\n        if not checkpoints:\n            return dir_name, None\n        checkpoint = os.path.join(dir_name, checkpoints[-1])\n        return dir_name, checkpoint\n\n    def load_weights(self, filepath, by_name=False, exclude=None):\n        \"\"\"Modified version of the correspoding Keras function with\n        the addition of multi-GPU support and the ability to exclude\n        some layers from loading.\n        exlude: list of layer names to excluce\n        \"\"\"\n        import h5py\n        from keras.engine import topology\n\n        if exclude:\n            by_name = True\n\n        if h5py is None:\n            raise ImportError('`load_weights` requires h5py.')\n        f = h5py.File(filepath, mode='r')\n        if 'layer_names' not in f.attrs and 'model_weights' in f:\n            f = f['model_weights']\n\n        # In multi-GPU training, we wrap the model. Get layers\n        # of the inner model because they have the weights.\n        keras_model = self.keras_model\n        layers = keras_model.inner_model.layers if hasattr(keras_model, \"inner_model\")\\\n            else keras_model.layers\n\n        # Exclude some layers\n        if exclude:\n            layers = filter(lambda l: l.name not in exclude, layers)\n\n        if by_name:\n            topology.load_weights_from_hdf5_group_by_name(f, layers)\n        else:\n            topology.load_weights_from_hdf5_group(f, layers)\n        if hasattr(f, 'close'):\n            f.close()\n\n        # Update the log directory\n        self.set_log_dir(filepath)\n\n    def get_imagenet_weights(self):\n        \"\"\"Downloads ImageNet trained weights from Keras.\n        Returns path to weights file.\n        \"\"\"\n        from keras.utils.data_utils import get_file\n        TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/'\\\n                                 'releases/download/v0.2/'\\\n                                 'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n        weights_path = get_file('resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5',\n                                TF_WEIGHTS_PATH_NO_TOP,\n                                cache_subdir='models',\n                                md5_hash='a268eb855778b3df3c7506639542a6af')\n        return weights_path\n\n    def compile(self, learning_rate, momentum):\n        \"\"\"Gets the model ready for training. Adds losses, regularization, and\n        metrics. Then calls the Keras compile() function.\n        \"\"\"\n        # Optimizer object\n        optimizer = keras.optimizers.SGD(lr=learning_rate, momentum=momentum,\n                                         clipnorm=self.config.GRADIENT_CLIP_NORM)\n        # Add Losses\n        # First, clear previously set losses to avoid duplication\n        self.keras_model._losses = []\n        self.keras_model._per_input_losses = {}\n        loss_names = [\"rpn_class_loss\", \"rpn_bbox_loss\",\n                      \"mrcnn_class_loss\", \"mrcnn_bbox_loss\", \"mrcnn_mask_loss\"]\n        for name in loss_names:\n            layer = self.keras_model.get_layer(name)\n            if layer.output in self.keras_model.losses:\n                continue\n            self.keras_model.add_loss(\n                tf.reduce_mean(layer.output, keep_dims=True))\n\n        # Add L2 Regularization\n        # Skip gamma and beta weights of batch normalization layers.\n        reg_losses = [keras.regularizers.l2(self.config.WEIGHT_DECAY)(w) / tf.cast(tf.size(w), tf.float32)\n                      for w in self.keras_model.trainable_weights\n                      if 'gamma' not in w.name and 'beta' not in w.name]\n        self.keras_model.add_loss(tf.add_n(reg_losses))\n\n        # Compile\n        self.keras_model.compile(optimizer=optimizer, loss=[\n                                 None] * len(self.keras_model.outputs))\n\n        # Add metrics for losses\n        for name in loss_names:\n            if name in self.keras_model.metrics_names:\n                continue\n            layer = self.keras_model.get_layer(name)\n            self.keras_model.metrics_names.append(name)\n            self.keras_model.metrics_tensors.append(tf.reduce_mean(\n                layer.output, keep_dims=True))\n\n    def set_trainable(self, layer_regex, keras_model=None, indent=0, verbose=1):\n        \"\"\"Sets model layers as trainable if their names match\n        the given regular expression.\n        \"\"\"\n        # Print message on the first call (but not on recursive calls)\n        if verbose > 0 and keras_model is None:\n            log(\"Selecting layers to train\")\n\n        keras_model = keras_model or self.keras_model\n\n        # In multi-GPU training, we wrap the model. Get layers\n        # of the inner model because they have the weights.\n        layers = keras_model.inner_model.layers if hasattr(keras_model, \"inner_model\")\\\n            else keras_model.layers\n\n        for layer in layers:\n            # Is the layer a model?\n            if layer.__class__.__name__ == 'Model':\n                print(\"In model: \", layer.name)\n                self.set_trainable(\n                    layer_regex, keras_model=layer, indent=indent + 4)\n                continue\n\n            if not layer.weights:\n                continue\n            # Is it trainable?\n            trainable = bool(re.fullmatch(layer_regex, layer.name))\n            # Update layer. If layer is a container, update inner layer.\n            if layer.__class__.__name__ == 'TimeDistributed':\n                layer.layer.trainable = trainable\n            else:\n                layer.trainable = trainable\n            # Print trainble layer names\n            if trainable and verbose > 0:\n                log(\"{}{:20}   ({})\".format(\" \" * indent, layer.name,\n                                            layer.__class__.__name__))\n\n    def set_log_dir(self, model_path=None):\n        \"\"\"Sets the model log directory and epoch counter.\n\n        model_path: If None, or a format different from what this code uses\n            then set a new log directory and start epochs from 0. Otherwise,\n            extract the log directory and the epoch counter from the file\n            name.\n        \"\"\"\n        # Set date and epoch counter as if starting a new model\n        self.epoch = 0\n        now = datetime.datetime.now()\n\n        # If we have a model path with date and epochs use them\n        if model_path:\n            # Continue from we left of. Get epoch and date from the file name\n            # A sample model path might look like:\n            # /path/to/logs/coco20171029T2315/mask_rcnn_coco_0001.h5\n            regex = r\".*/\\w+(\\d{4})(\\d{2})(\\d{2})T(\\d{2})(\\d{2})/mask\\_rcnn\\_\\w+(\\d{4})\\.h5\"\n            m = re.match(regex, model_path)\n            if m:\n                now = datetime.datetime(int(m.group(1)), int(m.group(2)), int(m.group(3)),\n                                        int(m.group(4)), int(m.group(5)))\n                self.epoch = int(m.group(6)) + 1\n\n        # Directory for training logs\n        self.log_dir = os.path.join(self.model_dir, \"{}{:%Y%m%dT%H%M}\".format(\n            self.config.NAME.lower(), now))\n\n        # Path to save after each epoch. Include placeholders that get filled by Keras.\n        self.checkpoint_path = os.path.join(self.log_dir, \"mask_rcnn_{}_*epoch*.h5\".format(\n            self.config.NAME.lower()))\n        self.checkpoint_path = self.checkpoint_path.replace(\n            \"*epoch*\", \"{epoch:04d}\")\n\n    def train(self, train_dataset, val_dataset, learning_rate, epochs, layers,\n              augmentation=None):\n        \"\"\"Train the model.\n        train_dataset, val_dataset: Training and validation Dataset objects.\n        learning_rate: The learning rate to train with\n        epochs: Number of training epochs. Note that previous training epochs\n                are considered to be done alreay, so this actually determines\n                the epochs to train in total rather than in this particaular\n                call.\n        layers: Allows selecting wich layers to train. It can be:\n            - A regular expression to match layer names to train\n            - One of these predefined values:\n              heaads: The RPN, classifier and mask heads of the network\n              all: All the layers\n              3+: Train Resnet stage 3 and up\n              4+: Train Resnet stage 4 and up\n              5+: Train Resnet stage 5 and up\n        augmentation: Optional. An imgaug (https://github.com/aleju/imgaug) augmentation.\n            For example, passing imgaug.augmenters.Fliplr(0.5) flips images\n            right/left 50% of the time.\n        \"\"\"\n        assert self.mode == \"training\", \"Create model in training mode.\"\n\n        # Pre-defined layer regular expressions\n        layer_regex = {\n            # all layers but the backbone\n            \"heads\": r\"(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n            # From a specific Resnet stage and up\n            \"3+\": r\"(res3.*)|(bn3.*)|(res4.*)|(bn4.*)|(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n            \"4+\": r\"(res4.*)|(bn4.*)|(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n            \"5+\": r\"(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n            # All layers\n            \"all\": \".*\",\n        }\n        if layers in layer_regex.keys():\n            layers = layer_regex[layers]\n\n        # Data generators\n        train_generator = data_generator(train_dataset, self.config, shuffle=True,\n                                         augmentation=augmentation,\n                                         batch_size=self.config.BATCH_SIZE)\n        val_generator = data_generator(val_dataset, self.config, shuffle=True,\n                                       batch_size=self.config.BATCH_SIZE)\n\n        # Callbacks\n        callbacks = [\n            keras.callbacks.TensorBoard(log_dir=self.log_dir,\n                                        histogram_freq=0, write_graph=True, write_images=False),\n            keras.callbacks.ModelCheckpoint(self.checkpoint_path,\n                                            verbose=0, save_weights_only=True),\n        ]\n\n        # Train\n        log(\"\\nStarting at epoch {}. LR={}\\n\".format(self.epoch, learning_rate))\n        log(\"Checkpoint Path: {}\".format(self.checkpoint_path))\n        self.set_trainable(layers)\n        self.compile(learning_rate, self.config.LEARNING_MOMENTUM)\n\n        # Work-around for Windows: Keras fails on Windows when using\n        # multiprocessing workers. See discussion here:\n        # https://github.com/matterport/Mask_RCNN/issues/13#issuecomment-353124009\n        if os.name is 'nt':\n            workers = 0\n        else:\n            workers = multiprocessing.cpu_count()\n\n        self.keras_model.fit_generator(\n            train_generator,\n            initial_epoch=self.epoch,\n            epochs=epochs,\n            steps_per_epoch=self.config.STEPS_PER_EPOCH,\n            callbacks=callbacks,\n            validation_data=val_generator,\n            validation_steps=self.config.VALIDATION_STEPS,\n            max_queue_size=100,\n            workers=workers,\n            use_multiprocessing=True,\n        )\n        self.epoch = max(self.epoch, epochs)\n\n    def mold_inputs(self, images):\n        \"\"\"Takes a list of images and modifies them to the format expected\n        as an input to the neural network.\n        images: List of image matricies [height,width,depth]. Images can have\n            different sizes.\n\n        Returns 3 Numpy matricies:\n        molded_images: [N, h, w, 3]. Images resized and normalized.\n        image_metas: [N, length of meta data]. Details about each image.\n        windows: [N, (y1, x1, y2, x2)]. The portion of the image that has the\n            original image (padding excluded).\n        \"\"\"\n        molded_images = []\n        image_metas = []\n        windows = []\n        for image in images:\n            # Resize image to fit the model expected size\n            # TODO: move resizing to mold_image()\n            molded_image, window, scale, padding = utils.resize_image(\n                image,\n                min_dim=self.config.IMAGE_MIN_DIM,\n                max_dim=self.config.IMAGE_MAX_DIM,\n                padding=self.config.IMAGE_PADDING)\n            molded_image = mold_image(molded_image, self.config)\n            # Build image_meta\n            image_meta = compose_image_meta(\n                0, image.shape, window,\n                np.zeros([self.config.NUM_CLASSES], dtype=np.int32))\n            # Append\n            molded_images.append(molded_image)\n            windows.append(window)\n            image_metas.append(image_meta)\n        # Pack into arrays\n        molded_images = np.stack(molded_images)\n        image_metas = np.stack(image_metas)\n        windows = np.stack(windows)\n        return molded_images, image_metas, windows\n\n    def unmold_detections(self, detections, mrcnn_mask, image_shape, window):\n        \"\"\"Reformats the detections of one image from the format of the neural\n        network output to a format suitable for use in the rest of the\n        application.\n\n        detections: [N, (y1, x1, y2, x2, class_id, score)]\n        mrcnn_mask: [N, height, width, num_classes]\n        image_shape: [height, width, depth] Original size of the image before resizing\n        window: [y1, x1, y2, x2] Box in the image where the real image is\n                excluding the padding.\n\n        Returns:\n        boxes: [N, (y1, x1, y2, x2)] Bounding boxes in pixels\n        class_ids: [N] Integer class IDs for each bounding box\n        scores: [N] Float probability scores of the class_id\n        masks: [height, width, num_instances] Instance masks\n        \"\"\"\n        # How many detections do we have?\n        # Detections array is padded with zeros. Find the first class_id == 0.\n        zero_ix = np.where(detections[:, 4] == 0)[0]\n        N = zero_ix[0] if zero_ix.shape[0] > 0 else detections.shape[0]\n\n        # Extract boxes, class_ids, scores, and class-specific masks\n        boxes = detections[:N, :4]\n        class_ids = detections[:N, 4].astype(np.int32)\n        scores = detections[:N, 5]\n        masks = mrcnn_mask[np.arange(N), :, :, class_ids]\n\n        # Compute scale and shift to translate coordinates to image domain.\n        h_scale = image_shape[0] / (window[2] - window[0])\n        w_scale = image_shape[1] / (window[3] - window[1])\n        scale = min(h_scale, w_scale)\n        shift = window[:2]  # y, x\n        scales = np.array([scale, scale, scale, scale])\n        shifts = np.array([shift[0], shift[1], shift[0], shift[1]])\n\n        # Translate bounding boxes to image domain\n        boxes = np.multiply(boxes - shifts, scales).astype(np.int32)\n\n        # Filter out detections with zero area. Often only happens in early\n        # stages of training when the network weights are still a bit random.\n        exclude_ix = np.where(\n            (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]) <= 0)[0]\n        if exclude_ix.shape[0] > 0:\n            boxes = np.delete(boxes, exclude_ix, axis=0)\n            class_ids = np.delete(class_ids, exclude_ix, axis=0)\n            scores = np.delete(scores, exclude_ix, axis=0)\n            masks = np.delete(masks, exclude_ix, axis=0)\n            N = class_ids.shape[0]\n\n        # Resize masks to original image size and set boundary threshold.\n        full_masks = []\n        for i in range(N):\n            # Convert neural network mask to full size mask\n            full_mask = utils.unmold_mask(masks[i], boxes[i], image_shape)\n            full_masks.append(full_mask)\n        full_masks = np.stack(full_masks, axis=-1)\\\n            if full_masks else np.empty((0,) + masks.shape[1:3])\n\n        return boxes, class_ids, scores, full_masks\n\n    def detect(self, images, verbose=0):\n        \"\"\"Runs the detection pipeline.\n\n        images: List of images, potentially of different sizes.\n\n        Returns a list of dicts, one dict per image. The dict contains:\n        rois: [N, (y1, x1, y2, x2)] detection bounding boxes\n        class_ids: [N] int class IDs\n        scores: [N] float probability scores for the class IDs\n        masks: [H, W, N] instance binary masks\n        \"\"\"\n        assert self.mode == \"inference\", \"Create model in inference mode.\"\n        assert len(\n            images) == self.config.BATCH_SIZE, \"len(images) must be equal to BATCH_SIZE\"\n\n        if verbose:\n            log(\"Processing {} images\".format(len(images)))\n            for image in images:\n                log(\"image\", image)\n        # Mold inputs to format expected by the neural network\n        molded_images, image_metas, windows = self.mold_inputs(images)\n        if verbose:\n            log(\"molded_images\", molded_images)\n            log(\"image_metas\", image_metas)\n        # Run object detection\n        detections, mrcnn_class, mrcnn_bbox, mrcnn_mask, \\\n            rois, rpn_class, rpn_bbox =\\\n            self.keras_model.predict([molded_images, image_metas], verbose=0)\n        # Process detections\n        results = []\n        for i, image in enumerate(images):\n            final_rois, final_class_ids, final_scores, final_masks =\\\n                self.unmold_detections(detections[i], mrcnn_mask[i],\n                                       image.shape, windows[i])\n            results.append({\n                \"rois\": final_rois,\n                \"class_ids\": final_class_ids,\n                \"scores\": final_scores,\n                \"masks\": final_masks,\n            })\n        return results\n\n    def ancestor(self, tensor, name, checked=None):\n        \"\"\"Finds the ancestor of a TF tensor in the computation graph.\n        tensor: TensorFlow symbolic tensor.\n        name: Name of ancestor tensor to find\n        checked: For internal use. A list of tensors that were already\n                 searched to avoid loops in traversing the graph.\n        \"\"\"\n        checked = checked if checked is not None else []\n        # Put a limit on how deep we go to avoid very long loops\n        if len(checked) > 500:\n            return None\n        # Convert name to a regex and allow matching a number prefix\n        # because Keras adds them automatically\n        if isinstance(name, str):\n            name = re.compile(name.replace(\"/\", r\"(\\_\\d+)*/\"))\n\n        parents = tensor.op.inputs\n        for p in parents:\n            if p in checked:\n                continue\n            if bool(re.fullmatch(name, p.name)):\n                return p\n            checked.append(p)\n            a = self.ancestor(p, name, checked)\n            if a is not None:\n                return a\n        return None\n\n    def find_trainable_layer(self, layer):\n        \"\"\"If a layer is encapsulated by another layer, this function\n        digs through the encapsulation and returns the layer that holds\n        the weights.\n        \"\"\"\n        if layer.__class__.__name__ == 'TimeDistributed':\n            return self.find_trainable_layer(layer.layer)\n        return layer\n\n    def get_trainable_layers(self):\n        \"\"\"Returns a list of layers that have weights.\"\"\"\n        layers = []\n        # Loop through all layers\n        for l in self.keras_model.layers:\n            # If layer is a wrapper, find inner trainable layer\n            l = self.find_trainable_layer(l)\n            # Include layer if it has weights\n            if l.get_weights():\n                layers.append(l)\n        return layers\n\n    def run_graph(self, images, outputs):\n        \"\"\"Runs a sub-set of the computation graph that computes the given\n        outputs.\n\n        outputs: List of tuples (name, tensor) to compute. The tensors are\n            symbolic TensorFlow tensors and the names are for easy tracking.\n\n        Returns an ordered dict of results. Keys are the names received in the\n        input and values are Numpy arrays.\n        \"\"\"\n        model = self.keras_model\n\n        # Organize desired outputs into an ordered dict\n        outputs = OrderedDict(outputs)\n        for o in outputs.values():\n            assert o is not None\n\n        # Build a Keras function to run parts of the computation graph\n        inputs = model.inputs\n        if model.uses_learning_phase and not isinstance(K.learning_phase(), int):\n            inputs += [K.learning_phase()]\n        kf = K.function(model.inputs, list(outputs.values()))\n\n        # Run inference\n        molded_images, image_metas, windows = self.mold_inputs(images)\n        # TODO: support training mode?\n        # if TEST_MODE == \"training\":\n        #     model_in = [molded_images, image_metas,\n        #                 target_rpn_match, target_rpn_bbox,\n        #                 gt_boxes, gt_masks]\n        #     if not config.USE_RPN_ROIS:\n        #         model_in.append(target_rois)\n        #     if model.uses_learning_phase and not isinstance(K.learning_phase(), int):\n        #         model_in.append(1.)\n        #     outputs_np = kf(model_in)\n        # else:\n\n        model_in = [molded_images, image_metas]\n        if model.uses_learning_phase and not isinstance(K.learning_phase(), int):\n            model_in.append(0.)\n        outputs_np = kf(model_in)\n\n        # Pack the generated Numpy arrays into a a dict and log the results.\n        outputs_np = OrderedDict([(k, v)\n                                  for k, v in zip(outputs.keys(), outputs_np)])\n        for k, v in outputs_np.items():\n            log(k, v)\n        return outputs_np\n\n\n\n```\n\n\n3. 工具\n\n一個是輸出log的function，另一個是去修改batch norm，畢竟這repo是2 or 4，太小了會造成反效果\n\n```python\ndef log(text, array=None):\n    \"\"\"Prints a text message. And, optionally, if a Numpy array is provided it\n    prints it's shape, min, and max values.\n    \"\"\"\n    if array is not None:\n        text = text.ljust(25)\n        text += (\"shape: {:20}  min: {:10.5f}  max: {:10.5f}\".format(\n            str(array.shape),\n            array.min() if array.size else \"\",\n            array.max() if array.size else \"\"))\n    print(text)\n\n\nclass BatchNorm(KL.BatchNormalization):\n    \"\"\"Batch Normalization class. Subclasses the Keras BN class and\n    hardcodes training=False so the BN layer doesn't update\n    during training.\n\n    Batch normalization has a negative effect on training if batches are small\n    so we disable it here.\n    \"\"\"\n\n    def call(self, inputs, training=None):\n        return super(self.__class__, self).call(inputs, training=False)\n```\n\n4.resnet graph\n\nResnet All\n\n```python\ndef resnet_graph(input_image, architecture, stage5=False):  \n    assert architecture in [\"resnet50\", \"resnet101\"]  \n    # Stage 1  \n    x = KL.ZeroPadding2D((3, 3))(input_image)  \n    x = KL.Conv2D(64, (7, 7), strides=(2, 2), name='conv1', use_bias=True)(x)  \n    x = BatchNorm(axis=3, name='bn_conv1')(x)  \n    x = KL.Activation('relu')(x)  \n    C1 = x = KL.MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\")(x)  \n    # Stage 2  \n    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))  \n    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')  \n    C2 = x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')  \n    # Stage 3  \n    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')  \n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')  \n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')  \n    C3 = x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')  \n    # Stage 4  \n    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')  \n    block_count = {\"resnet50\": 5, \"resnet101\": 22}[architecture]  \n    for i in range(block_count):  \n        x = identity_block(x, 3, [256, 256, 1024], stage=4, block=chr(98 + i))  \n    C4 = x  \n    # Stage 5  \n    if stage5:  \n        x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')  \n        x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')  \n        C5 = x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')  \n    else:  \n        C5 = None  \n    return [C1, C2, C3, C4, C5]  \n```\n\nConvolution\n\n```python\nef conv_block(input_tensor, kernel_size, filters, stage, block,\n               strides=(2, 2), use_bias=True):\n    \"\"\"conv_block is the block that has a conv layer at shortcut\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n        filters: list of integers, the nb_filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n    And the shortcut should have subsample=(2,2) as well\n    \"\"\"\n    nb_filter1, nb_filter2, nb_filter3 = filters\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n\n    x = KL.Conv2D(nb_filter1, (1, 1), strides=strides,\n                  name=conv_name_base + '2a', use_bias=use_bias)(input_tensor)\n    x = BatchNorm(axis=3, name=bn_name_base + '2a')(x)\n    x = KL.Activation('relu')(x)\n\n    x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same',\n                  name=conv_name_base + '2b', use_bias=use_bias)(x)\n    x = BatchNorm(axis=3, name=bn_name_base + '2b')(x)\n    x = KL.Activation('relu')(x)\n\n    x = KL.Conv2D(nb_filter3, (1, 1), name=conv_name_base +\n                  '2c', use_bias=use_bias)(x)\n    x = BatchNorm(axis=3, name=bn_name_base + '2c')(x)\n\n    shortcut = KL.Conv2D(nb_filter3, (1, 1), strides=strides,\n                         name=conv_name_base + '1', use_bias=use_bias)(input_tensor)\n    shortcut = BatchNorm(axis=3, name=bn_name_base + '1')(shortcut)\n\n    x = KL.Add()([x, shortcut])\n    x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x)\n    return x\n```\n\nIdentity Block\n\n```python\ndef identity_block(input_tensor, kernel_size, filters, stage, block,\n                   use_bias=True):\n    \"\"\"The identity_block is the block that has no conv layer at shortcut\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n        filters: list of integers, the nb_filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n    \"\"\"\n    nb_filter1, nb_filter2, nb_filter3 = filters\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n\n    x = KL.Conv2D(nb_filter1, (1, 1), name=conv_name_base + '2a',\n                  use_bias=use_bias)(input_tensor)\n    x = BatchNorm(axis=3, name=bn_name_base + '2a')(x)\n    x = KL.Activation('relu')(x)\n\n    x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same',\n                  name=conv_name_base + '2b', use_bias=use_bias)(x)\n    x = BatchNorm(axis=3, name=bn_name_base + '2b')(x)\n    x = KL.Activation('relu')(x)\n\n    x = KL.Conv2D(nb_filter3, (1, 1), name=conv_name_base + '2c',\n                  use_bias=use_bias)(x)\n    x = BatchNorm(axis=3, name=bn_name_base + '2c')(x)\n\n    x = KL.Add()([x, input_tensor])\n    x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x)\n    return x\n```\n\n","tags":["Deep Learning","R-CNN","Segmentation","Pose Estimation"],"categories":["Segmentation","Detection"]},{"title":"CapsuleNet","url":"/2018/02/06/CapsuleNet/","content":"\n\n\n\n\n\n\n\n"},{"title":"GAN - 手推","url":"/2018/02/04/untitled-1517723627245/","content":"# Outline \n1. GAN 機率分佈\n2. f-divergence\n3. fenchel conjugate\n4. GAN公式\n\n\n\n# GAN 機率分佈\n\n機率質量函數(PMF) 可數->離散\n機率密度函數(PDF) 不可數-> 不可數\n\n\n#### ISSUE 如果換成凹函式呢?\n-> 大於小於對調 但生成結果好像沒差(?\n\n---未完成 待續\n\n\n\n\n\n\\mathcal {F_{i}^{RoI}}_{(u^\\prime,v^\\prime)} = \\sum_{(u,v)}^{W \\times H} G(u,v;u^\\prime, v^\\prime|B_i) \\mathcal F_(u,v)\n\n"},{"title":"ICNET code Analysis","url":"/2018/01/31/ICNET code Analysis/","tags":["Deep Learning","Segmentation","Tensorflow","real-time"],"categories":["Code"]},{"title":"ICNET for Real-Time Semantic Segmentation on High-Resolution Images","url":"/2018/01/28/ICNET-for-Real-Time-Semantic-Segmentation-on-High-Resolution-Images/","content":"\n## Recap\nQuote:\n其實很討厭這作者的paper\n效果都很好, 但是每次都是用matlab, 而且PSPNet作者還說training code因為公司問題不能發布, 傻眼\n![](/media/15172367977860.jpg)\nhttps://www.zhihu.com/question/53356671\n\n\nPaper\nhttps://arxiv.org/pdf/1704.08545.pdf\nCode\nhttps://github.com/hszhao/ICNet\nhttps://github.com/aitorzip/Keras-ICNet\nhttps://github.com/hellochick/ICNet-tensorflow\n\nKey Difference\n之前的那些方法，如FCN、SegNet、UNet、RefineNet等，用高解析度圖片當input以後，強調Single scale或是Multi Scale在不同層之間的特徵融合，所有的Data需要在整個網絡中運行，因為高解析度的輸入而導致了昂貴的計算費用.而本文的方法，使用低解析度圖片作為主要輸入，採用高解析度圖片進行refine，保留細節的同時減少了開銷.\n\n## Abstract\n\n````\nWe propose an compressed-PSPNet-based image cascade network (ICNet) that incorporates multi-resolution branches under proper label guidance to address this challenge.\n````\nICNet是一個基於PSPNet的real-time semantic segmentation network，論文內對PSPNet做深入的分析，並且找出影響inference speed*的缺點。並且用搭配multi-resolution cascade combination。\n\n*註:inference speed是單指DeConv的階段。\n\n## Introduction\n\n![](/media/15172162210529.png)\n\n\n在論文發表的時刻(2017 March), CityScapes上所有的Model表現基本上分成兩種類型, 一種是擁有高精準度但速度不行, 另一種是速度快但精准度不行。此論文在PSPNet的基礎上來增進速度，並找一個速度跟精準度的平衡點。\n\n論文貢獻:\n- 可以在1024x2048的解析度下保持30 fps的計算速度(Tensorflow版本實測可行, 但要去掉preproccess部分)\n- 相對PSPNet來說, 可疑提高5倍速度並可以減少五倍RAM消耗\n- 低解析的速度+高解析的細節做cascade的整合\n\n\n## Speed Analysis\n從PSPNet做解析\n![](/media/15172351270369.png)\n藍色是1024x2048, 綠色是512x1024 (1/4大小)\n從上圖可知\n- 圖越大速度越慢\n- 網路寬度越大速度越慢\n- Kernel越多速度越慢, 以圖中例子來說stage4跟stage5在解析同樣的input時, inference speed差距十分驚人, 因為這部分的kernel number差距了一倍。\n\n## Intuitive Speedup\n\n#### 加速方法 1: 輸入向下採樣(Downsampling Input)\n在resolution只有原本的0.5跟0.25的狀況下雖然速度變快但精准度如上圖所示可以看出效果很差。\n![](/media/15172353341447.png)\n\n\n\n#### 加速方法 2 : 利用較小的feature map來做inference(Downsampling Feature)\nFCN Downsampling到32倍, Deep Lab到 8倍, 而下方是用作者之前的PSPNet50, 縮小到了1:8, 1:16, 1:32整理的Table, 但可以看到最快的速度也只有132ms, 不太能符合real-time的標準。\n![](/media/15172758974357.png)\n#### 加速方法 3 : 減少模型複雜度(Model Compression)\n採用了其他篇paper(Pruning filters for efficient convnets)，作法就是減少Filter數量, 但一樣差強人意\n ![](/media/15172822508697.jpg)\n\n#### FCN:Fully Convolutional Networks for Fully Convolutional Networks\n這裡額外多講一下FCN，算是CNN做semantic segmentation的始祖，本質上的區別大概就是...FCN是沒有全連結層的CNN，好處是可以接受任意大小輸入。\n![](/media/15172765679435.jpg)\n\nCNN要如何轉FCN? 以此篇paper為例，input是一個224x224x3的圖，經過一系列Conv跟Downsampling之後是7x7x512。\nAlexNet使用了兩個4096的全連接層，最後一個有1000個神經元的全連接層用於計算分類評分。我們可以將這3個全連接層轉化為Convolution層。\n\n任一全連結層轉化為Conv的方式以以下為例：\n\n###### 例如 K=4096 的全連接層，輸入是7x7x512，這個全連接層可以被等效地看做一個F=7,Padding=0,Stride=1,Filter Number=4096 的Conv層。換句話說，就是將Filter Size設置的和Input Data Size一致了。輸出將變成 1x1x4096，這個結果就和使用初始的那個全連接層一樣了。\n\n針對第一個連接區域是[7x7x512]的全連接層，令其Filter Size為F=7**（Filter Size為7x7）**，這樣輸出為[1x1x4096]。\n針對第二個全連接層，令其Filter Size為F=1**（Filter Size為1x1）**，這樣輸出為[1x1x4096]。\n對最後一個全連接層也做類似的，令其F=1**（Filter Size為1x1）**，最終輸出為[1x1x1000]\n\n##### Step 1:\n下圖是是原始CNN結構，CNN中輸入的圖像大小是統一固定resize成227x227大小的圖像，第一層pooling後為55x55，第二層pooling後為27x27，第五層pooling後的圖像大小為13*13。\n![](/media/15172417899356.jpg)\n##### Step 2:\nFCN輸入的圖像是假設是H*W，第一層pooling後變為原圖大小的1/4，第二層變為的1/8，第五層變為1/ 16，第八層變為1/32\n![](/media/15172417973947.jpg)\n##### Step 3:\nConvolution本質上就是DownSampling（下採樣）。經過多次Convolution和pooling以後，得到的圖像越來越小，解析度越來越低。其中圖像到H/32∗W/32 的時候圖片是最小的一層時，所產生圖叫做heatmap，heatmap就是我們最重要的高維特徵圖，得到高維特徵的heatmap之後就是最重要的一步也是最後的一步，就是對此heatmap進行UpSampling(Deconvolution)，把圖像進行放大到原圖像的大小。\n\n![](/media/15172418178955.jpg)\n##### Step 4:\n最後的輸出是1000張heatmap經過UpSampling變為原圖大小的圖片。\n![](/media/15172418074043.jpg)\n\n##### Upsampling\n![](/media/20161024115403020.gif)\n其實這篇paper內雖然叫做Deconvolution，但之前CS231n課程內的大神也有說到，叫做Transposed Convolution比較適合。\n舉個例子來說：\n\n4x4的圖片輸入，Filter Size為3x3, 沒有Padding / Stride, 則輸出為2x2。\n\n輸入矩陣可展開為16維向量，記作*x*\n輸出矩陣可展開為4維向量，記作*y*\nConvolution運算可表示為*y*=*Cx*\nC其實就是如下的稀疏陣，而Forwarding就改成了的矩陣運算\n![](/media/15172789125366.jpg)\nBackPropagation的話，假若已經從更深的網路得到了\n<center>![](/media/15172790446136.jpg)</center>\n那麼就可以導出以下公式:\n![](/media/15172790496567.jpg)\nDeconvolution其實就是Forwarding時乘CT，而BackPropagation時乘(CT)T，即C。總結來說，Deconvolution等於Convolution在神經網絡結構的正向和反向傳播中的計算，做相反的計算。\n\n##### Skip Architecture\n由於縮小32倍結果超糟糕，所以FCN在前面的Pooling Layer進行Upsampling，然後結合這些結果來優化輸出。\n![](/media/15172794424801.jpg)\n\n\n## Architecture \n總結一下前面速度分析的結果，一系列的優化方法：\n\n- Downsampling Input：降低輸入解析度能都大幅度的加速，但同時會讓預測非常模糊\n- Downsampling Feature：可以加速但同時會降低準確率\n- Model Compression：壓縮訓練好的模型，通過減輕模型達到加速效果，可惜實驗效果不佳\n\n針對以上的分析，發現，低解析度的圖片能夠有效降低運行時間，但是失去很多細節，而且邊界模糊；但是高解析度的計算時間難以忍受，ICNet總結了上述幾個問題，提出了一個綜合性的方法：使用低解析度加速捕捉語義，使用高解析度獲取細節，使用特徵融合(CFF)結合，同時使用guide label來監督，在限制的時間內獲得有效的結果。\n\n\n![](/media/15172827737568.png)\n\n\n#### Branch Analysis\n圖中用了原尺寸,1/2,1/4當input，低解析度分枝超過50層Convolution，來提取更多的語義信息(inference 18 ms)，中解析度分枝有17層Convolution，但是由於權重共享，只有inference 6ms，而高解析度分枝是3 Convolution，有inference 9ms.\n\n\n| 分枝   | 過程| 耗時|\n|:--------:|-------------|--------------|\n| 低解析 | 低解析是FCN-based PSPNet的架構，總和有超過50層的Convolution，在中解析度的1/16輸出的基礎上，再縮放到1/32.經過Convolution後，然後使用幾個dilated convolution擴展接受野但不縮小尺寸，最終以原圖的1/32大小輸出feature map。| 雖然層數較多，但是解析度低，速度快，且與分枝二共享一部分權重，耗時為18 ms|\n| 中解析 | 以原圖的1/2的解析度作為輸入，經過17層Convolution後以1/8縮放，得到原圖的1/16大小feature map，再將低解析度分枝的輸出feature map通過CFF(cascade feature fusion )單元相融合得到最終輸出。值得注意的是：低解析度和中解析度的捲積參數是共享的。 | 有17個Convolution層，與分枝一共享一部分權重，與分枝一一起一共耗時6ms |\n| 高解析 | 原圖輸入，經過三層的Convolution(Stride=2,Size=3x3)得到原圖的1/8大小的feature map，再將中解析度處理後的輸出通過CFF單元融合| 有3個卷積層，雖然解析度高，因為少，耗時為9ms|\n\n對於每個分枝的輸出特徵，首先會上採樣2倍做輸出，在訓練的時候，會以Ground truth的1/16、1/8/、1/4來指導各個分枝的訓練，這樣的輔助訓練使得梯度優化更為平滑，便於訓練收斂，隨著每個分枝學習能力的增強，預測沒有被任何一個分枝主導。利用這樣的漸變的特徵融合和級聯引導結構可以產生合理的預測結果。\n\nICNet使用低解析度完成語義分割，使用高解析度幫助細化結果。在結構上，產生的feature大大減少，同時仍然保持必要的細節。\n#### Cascade Label Guidance\n\n\n#### Branch Output\n不同分枝的預測效果如下:\n![](/media/15172838466580.png)\n\n可以看到第三個分枝輸出效果無疑是最好的。在測試時，只保留第三分枝的結果。\n\n## Cascade Feature Fusion \n\n![](/media/15172845289475.jpg)\n圖中的Loss是輔助Loss,F1是較低解析的分枝, F2是較高解析的分枝，\n\n\n## Loss Function\n\nL=λ1L1+λ2L2+λ3L3\nLoss是對應到每個downsampled score maps使用cross-entropy loss\n\n依據CFF的設置，下分枝的lossL3的佔比λ3設置為1的話,則中分枝的lossL2的佔比λ2設置為0.4，上分枝的lossL1的佔比λ1設置為0.16\n\n\n## Experiment\n| 項目       | 設置                                   |\n|:------------:|:-------------------:|\n| 平台 | Caffe，CUDA7.5 cudnnV5，TitanX一張|\n| 測量時間| Caffe Time 100次取平均|\n| Batch Size | 16   |\n| 學習速率| Poly, Learning Rate 0.01, Momentum 0.9 |\n| 迭代次數   | 30K    |\n| 權重衰減   | 0.0001 |\n| 數據增強   | Random flip, 0.5 to 2 random scale     |\n|資料集|Cityscapes|\n\n#### model Compression\n以PSPNet50為例，直接壓縮結果如下表Baseline：\n\n![](/media/15173712115585.png)\n\nmIoU降低了，但時間170ms達不到realtime。這表明只有模型壓縮是達不到有良好分割結果的實時性能。對比ICNet，有類似的分割結果，但速度提升了5倍多。\n\n#### Cascade Structure Experiment\n![](/media/15173713226717.png)\nsub4代表只有低解析度輸入的結果，sub24代表前兩個分枝，sub124全部分枝。注意到全部分枝的速度很快，並且性能接近PSPNet了，且能保持30fps。而且Ram消耗也明顯減少了。\n\n#### Visualization\n\n![](/media/15173715003744.jpg)\n\n\n#### Cityscape Comparison\n![](/media/15173715775469.jpg)\n\n\n\n","tags":["Deep Learning","Computer Vision","Segmentation","real-time"],"categories":["Segmentation"]},{"title":"機器學習中的相似性度量","url":"/2018/01/14/Distance/","content":"\n## Euclidean Distance(歐幾里和距離)\n\n二維與三維中就是兩點之間的距離。\n\n\n![](/media/15158672262601.jpg)\n\n上圖中的綠線為歐氏距離，又稱歐幾里和距離(Euclidean Distance)，其餘的藍色與黃色還有紅色皆為曼哈頓距離。\n\n\n\n## Manhattan distance(曼哈頓距離)\n\n平面上，坐標（x1, y1）的點P1與坐標（x2, y2）的點P2的曼哈頓距離為：\n![](/media/15158673215658.jpg)\n\nL1-距離\n\n## Mahalanobis distance(馬氏距離)\n[Covariance Matrix](https://zh.wikipedia.org/wiki/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5)\n\n```\nMahalanobis distance can be defined as a dissimilarity measure between two random vectors x and y of the same distribution with the covariance matrix S:\n```\n\n![](/media/15158695072163.jpg)\n![](/media/15158699156018.jpg)\n\n## Chebyshev distance(切比雪夫距離)\n\n\n## Minkowski distance (明可夫斯基距離)\n\nhttp://www.cnblogs.com/heaad/archive/2011/03/08/1977733.html\n\n"},{"title":"Deep Sort: Simple Online and Realtime Tracking with a Deep Association Metric","url":"/2018/01/14/Deep-Sort/","content":"\n\n此篇文章是基於SORT的改進\n具體來說就是一句話\n\n````\nWe adopt a conventional single hypothesis tracking methodology with recursive kalman filtering and frame-by-frame data association.\n````\n\n\n\n\n","tags":["Deep Learning","Tracking","Computer Vision","MOT"],"categories":["MOT"]},{"title":"Multiple Object Tracking Summary","url":"/2018/01/10/Multiple-Object-Tracking-Summary/","content":"\nhttp://perception.yale.edu/Brian/refGuides/MOT.html\n\n\n","tags":["Tracking","Computer Vision","MOT","Summary"],"categories":["MOT"]},{"title":"Deep Learning and Computer Vision Recommended Paper","url":"/2018/01/09/Deep-Learning-Recommended-Papers/","content":"\n## Image Classification\n#### Must Read : LeNet, AlexNet, VGG-16, GoogleNet, ResNet\n|Title|Authors  |Pub.  |Links|Figure        |  \n|:--:|:--:|:--:|:--:|:--------:|\n|LeNet-5, convolutional neural networks|Y. LeCun|??? 199X|[Web](http://yann.lecun.com/exdb/lenet/) | ![LeNet](/media/data/LeNet.png)|\n|ImageNet Classification with Deep Convolutional Neural Networks|Alex Krizhevsky,Ilya Sutskever,Geoffrey E. Hinton|NIPS 2014|[paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) |![AlexNet](/media/data/AlexNet.jpg)|\n|Very Deep Convolutional Networks for Large-Scale Image Recognition|Karen Simonyan, Andrew Zisserman|ICLR 2014|[paper](https://arxiv.org/pdf/1409.1556.pdf) |![VGG16](/media/data/VGG16.png)|\n|Going Deeper with Convolutions|Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed|CVPR 2015|[paper](http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html) |![GoogLeNet](/media/data/GoogLeNet.png)|\n|Deep Residual Learning for Image Recognition|Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun|CVPR 2016 *`best`*|[paper](https://arxiv.org/abs/1512.03385) [github](https://github.com/KaimingHe/deep-residual-networks)|![ResNet](/media/data/ResNet.png)|\n|Residual Attention Network for Image Classification|Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, Xiaoou Tang|CVPR 2017|[paper](https://arxiv.org/abs/1704.06904) [github](https://github.com/buptwangfei/residual-attention-network) |![Res-Attention-Network](/media/data/Res-Attention-Network.png)|\n|Aggregated Residual Transformations for Deep Neural Networks|Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He|CVPR 2017|[paper](https://arxiv.org/abs/1611.05431) [github](https://github.com/facebookresearch/ResNeXt)|![ResNeXt](/media/data/ResNeXt.png)|\n|Densely Connected Convolutional Networks|Gao Huang, Zhuang Liu, Kilian Q. Weinberger|CVPR 2017 *`best`*|[paper](https://arxiv.org/abs/1608.06993) [github](https://github.com/liuzhuang13/DenseNet) |![DenseNet](/media/data/DenseNet.png)|\n|Deep Pyramidal Residual Networks|Dongyoon Han, Jiwhan Kim, Junmo Kim|CVPR 2017|[paper](https://arxiv.org/pdf/1610.02915.pdf) [github](https://github.com/jhkim89/PyramidNet)  |![PyramidNet](/media/data/PyramidNet.png)|\n\n\n\n\n\n## Object Detection\n#### Must Read : R-CNN, Fast R-CNN, Faster R-CNN, YOLO, SSD\n\n|Title   |Authors  |Pub.  |Links|Figure   |\n|:-----:|:-----:|:-----:|:-----:|:---:|\n|Rich feature hierarchies for accurate object detection and semantic segmentation|Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik|CVPR 2014|[paper](http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf) [github](https://github.com/rbgirshick/rcnn) |![R-CNN](/media/data/R-CNN.png)|\n|Fast R-CNN|Ross Girshick|ICCV 2015|[paper](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf) [github](https://github.com/rbgirshick/fast-rcnn) |![Fast-R-CNN](/media/data/Fast-R-CNN.png)|\n|Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition|Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun|TPAMI 2015|[paper](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf) |![SPP Net](/media/data/SPP-Net.png)|\n|Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks|Shaoqing Ren, [Kaiming He](http://kaiminghe.com/), Ross Girshick, Jian Sun|NIPS 2015|[paper](http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf) [`matlab`](https://github.com/ShaoqingRen/faster_rcnn) [`python`](https://github.com/rbgirshick/py-faster-rcnn) [`pytorch`](https://github.com/longcw/faster_rcnn_pytorch) |![Faster-R-CNN](/media/data/Faster-R-CNN.png)|\n|You Only Look Once: Unified, Real-Time Object Detection|Joseph Redmon,Santosh Divvala,Ross Girshick, Ali Farhadi|CVPR 2016|[paper](http://arxiv.org/pdf/1506.02640.pdf)|![YOLO](/media/data/YOLO.jpg)|\n|SSD: Single Shot MultiBox Detector|Wei Liu1, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg|CVPR 2016|[paper](http://arxiv.org/pdf/1512.02325.pdf)|![SSD](/media/data/SSD.png)|\n|Convolutional Feature Masking for Joint Object and Stuff Segmentation|Jifeng Dai, [Kaiming He](http://kaiminghe.com/), Jian Sun|CVPR 2015|[paper](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Dai_Convolutional_Feature_Masking_2015_CVPR_paper.pdf)|![CFM](/media/data/CFM.png)|\n|Instance-aware Semantic Segmentation via Multi-task Network Cascades|Jifeng Dai, [Kaiming He](http://kaiminghe.com/), Jian Sun|CVPR 2016|[paper](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Dai_Instance-Aware_Semantic_Segmentation_CVPR_2016_paper.pdf) [github](https://github.com/daijifeng001/MNC) |![MNC](/media/data/MNC.png)|\n|R-FCN: Object Detection via Region-based Fully Convolutional Networks|Jifeng Dai, Yi Li, [Kaiming He](http://kaiminghe.com/), Jian Sun|NIPS 2016|[paper](https://arxiv.org/abs/1605.06409) [github](https://github.com/daijifeng001/R-FCN)|![Region-FCN](/media/data/Region-FCN.png)|\n|Feature Pyramid Networks for Object Detection|Tsung-Yi Lin, Piotr Dollár, Ross Girshick, [Kaiming He](http://kaiminghe.com/), Bharath Hariharan, and Serge Belongie|CVPR 2017|[paper](https://arxiv.org/pdf/1612.03144.pdf)|![FPN](/media/data/FPN.png)|\n|Mask R-CNN|Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick|ICCV 2017|[paper](https://arxiv.org/abs/1703.06870) |![Mask-R-CNN](/media/data/Mask-R-CNN.png)|\n|A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection|Xiaolong Wang, Abhinav Shrivastava, Abhinav Gupta|CVPR 2017|[paper](https://arxiv.org/abs/1704.03414)  [github](https://github.com/xiaolonw/adversarial-frcnn) |![A-Fast-R-CNN](/media/data/A-Fast-R-CNN.png)|\n|Multiple Instance Detection Network with Online Instance Classifier Refinement|Peng Tang, Xinggang Wang, Xiang Bai, Wenyu Liu|CVPR 2017|[paper](https://arxiv.org/abs/1704.00138)|![MIDN](/media/data/MIDN.png)|\n|R-FCN-3000 at 30fps: Decoupling Detection and Classification|Bharat Singh, Hengdou Li, Abhishek Sharma and Larry S. Davis|Tech Report|[paper](https://arxiv.org/abs/1712.01802)|![R-FCN-3000](/media/data/R-FCN-3000.png)|\n\n## Semantic Segmentation and Scene Parsing\n#### Must Read : FCN, Learning Deconvolution Network for Semantic Segmentation, U-Net\n\n|Title   |Authors  |Pub.  |Links|Figure   |\n|--------|:-----:|:-----:|:-----:|:-----:|:---:|\n|Fully Convolutional Networks for Semantic Segmentation|Jonathan Long, Evan Shelhamer, Trevor Darrell|CVPR 2015|[paper](https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf) |![FCN](/media/data/FCN.png)|\n|Learning to Segment Object Candidates|Pedro O. Pinheiro, Ronan Collobert, Piotr Dollar|NIPS 2015|[paper](http://papers.nips.cc/paper/5852-learning-to-segment-object-candidates.pdf)|![LSOC](/media/data/LSOC.png)|\n|Learning to Refine Object Segments|Pedro O. Pinheiro , Tsung-Yi Lin , Ronan Collobert, Piotr Doll ́ar|arXiv 1603.08695|[paper](https://arxiv.org/pdf/1603.08695.pdf)|![LROS](/media/data/LROS.png)|\n|Conditional Random Fields as Recurrent Neural Networks|Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, ZhiZhong Su, Dalong Du, Chang Huang, and Philip H. S. Torr|ICCV 2015|[paper](http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Zheng_Conditional_Random_Fields_ICCV_2015_paper.html)|![CRFRNN](/media/data/CRFRNN.png)|\n|Learning Deconvolution Network for Semantic Segmentation|Heonwoo Noh, Seunghoon Hong, Bohyung Han|ICCV 2015|[paper](http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Noh_Learning_Deconvolution_Network_ICCV_2015_paper.html)|![LDN](/media/data/LDN.png)|\n|U-Net: Convolutional Networks for Biomedical Image Segmentation|Olaf Ronneberger, Philipp Fischer, Thomas Brox|MICCAI 2015|[paper](https://arxiv.org/pdf/1505.04597.pdf)|![U-Net](/media/data/U-Net.png)|\n|Instance-sensitive Fully Convolutional Networks|Jifeng Dai, Kaiming He, Yi Li, Shaoqing Ren, Jian Sun|ECCV 2016|[paper](https://arxiv.org/abs/1603.08678)|![ISFCN](/media/data/ISFCN.png)|\n|Laplacian Pyramid Reconstruction and Refinement for Semantic Segmentation|Golnaz Ghiasi, Charless C. Fowlkes|ECCV 2016|[paper](https://link.springer.com/chapter/10.1007/978-3-319-46487-9_32)  [github](https://github.com/golnazghiasi/LRR)|![LPRR](/media/data/LPRR.png)|\n|Attention to Scale: Scale-aware Semantic Image Segmentation|Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu|CVPR 2016|[paper](http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Chen_Attention_to_Scale_CVPR_2016_paper.html) [`DeepLab`](http://liangchiehchen.com/projects/DeepLab.html) |![Attention-to-scale](/media/data/Attention-to-scale.png)|\n|RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation|Guosheng Lin, Anton Milan, Chunhua Shen, Ian Reid|CVPR 2017|[paper](https://arxiv.org/abs/1611.06612)  [github](https://github.com/guosheng/refinenet)|![RefineNet](/media/data/RefineNet.png)\n|\n|Pyramid Scene Parsing Network|Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia|CVPR 2017|[paper](https://arxiv.org/abs/1612.01105)  [github](https://github.com/hszhao/PSPNet)|![PSPNet](/media/data/PSPNet.png)|\n|ICNet for Real-Time Semantic Segmentation on High-Resolution Images|Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, Jiaya Jia|Tech Report|[paper](https://arxiv.org/pdf/1704.08545.pdf) |![ICNet](/media/data/ICNet.png)|\n|Dilated Residual Networks|Fisher Yu, Vladlen Koltun, Thomas Funkhouser|CVPR 2017|[paper](https://arxiv.org/abs/1705.09914) [github](https://github.com/fyu/drn)|![DRN](/media/data/DRN.png)|\n|Fully Convolutional Instance-aware Semantic Segmentation|Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, Yichen Wei|CVPR 2017|[paper](https://arxiv.org/abs/1611.07709) [github](https://github.com/msracver/FCIS)|![FCIS](/media/data/FCIS.png)|\n|Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes|Tobias Pohlen, Alexander Hermans, Markus Mathias, Bastian Leibe|CVPR 2017|[paper](https://arxiv.org/abs/1611.08323) [github](https://github.com/TobyPDE/FRRN)|![FRRN](/media/data/FRRN.png)|\n|Object Region Mining with Adversarial Erasing: A Simple Classification toSemantic Segmentation Approach|Yunchao Wei, Jiashi Feng, Xiaodan Liang, Ming-Ming Cheng, Yao Zhao, Shuicheng Yan|CVPR 2017|[paper](https://arxiv.org/abs/1703.08448)|![A-Erasing](/media/data/A-Erasing.png)|\n|Not All Pixels Are Equal: Difficulty-Aware Semantic Segmentation via Deep Layer Cascade|Xiaoxiao Li, Ziwei Liu, Ping Luo, Chen Change Loy, Xiaoou Tang|CVPR 2017|[paper](https://arxiv.org/pdf/1704.01344.pdf)|![Not-All-Pixels-Are-Equal](/media/data/Not-All-Pixels-Are-Equal.png)|\n|Semantic Segmentation with Reverse Attention|Qin Huang, Chunyang Xia, Wuchi Hao, Siyang Li, Ye Wang, Yuhang Song and C.-C. Jay Kuo|BMVC 2017|[paper](https://arxiv.org/abs/1707.06426) [`code`](https://drive.google.com/drive/folders/0By2w_AaM8Rzbllnc3JCQjhHYnM?usp=sharing)|![Rev-Attention](/media/data/Rev-Attention.png)|\n|Predicting Deeper into the Future of Semantic Segmentation|Pauline Luc, Natalia Neverova, Camille Couprie, Jakob Verbeek and Yann LeCun|ICCV 2017|[paper](https://arxiv.org/abs/1703.07684) [`project page`](https://thoth.inrialpes.fr/people/pluc/iccv2017)|![Deeper-into-Future](/media/data/Deeper-into-Future.png)|\n|Learning to Segment Every Thing|Ronghang Hu, Piotr Dollar, Kaiming He, Trevor Darrell, Ross Girshick|Tech Report|[paper](https://arxiv.org/abs/1711.10370)|![Seg-Everything](/media/data/Seg-Everything.png)|\n\n\n## Regularization\n- Dropout- A Simple Way to Prevent Neural Networks from Overfitting\n- Batch Normalization- Accelerating Deep Network Training by Reducing Internal Covariate Shift\n\n## RNN\n- Generating Sequences With Recurrent Neural Networks\n- Word embedding\n- Distributed Representations of Words and Phrases and their Compositionality\n\n## Image captioning\nShow and Tell: A Neural Image Caption Generator\nShow, Attend and Tell: Neural Image Caption Generation with Visual Attention\n\n\n","tags":["Deep Learning","Computer Vision","Paper List","Self-Study"],"categories":["Deep Learning"]},{"title":"Person re-ID Summary","url":"/2018/01/02/person_re-ID_summary/","content":"\n![](/media/15149820234945.jpg)\n\n\n## 目標\n\n\n\n## 難度\n1. 目標遮擋（Occlusion）導致部分特徵丟失\n2. 不同的 View，Illumination 導致同一目標的特徵差異\n3. 不同目標衣服顏色近似、特徵近似導致區分度下降\n\n## 解決方案\n\n### 1. Representation learning + ReID\n看做分類(Classification/Identification)問題或者驗證(Verification)問題：\n(1) 分類問題是指利用行人的ID或者屬性等作為訓練標籤來訓練模型；\n(2) 驗證問題是指輸入一對（兩張）行人圖片，讓網絡來學習這兩張圖片是否屬於同一個行人。\n\nClassification/Identification loss和verification loss\n\n![](/media/15150472221676.jpg)\n\n\n\n額外改進方向[2]是在加上許多行人的label，像是性別、頭髮以及服裝等等。\n![](/media/15150474384428.jpg)\n\n\n### 2. Metric learning + ReID\n常用於圖像檢索的方法，通過網絡學習出兩張圖片的相似度。\n(Contrastive loss)[5]、三元組損失(Triplet loss)、 四元組損失(Quadruplet loss)、難樣本採樣三元組損失(Triplet hard loss with batch hard mining, TriHard loss)、邊界挖掘損失(Margin sample mining loss, MSML\n\n\nContrastive loss 基本上就是Siamese CNN\n![](/media/15150477657739.jpg)\n\n訓練時是三個正樣本一個副樣本，test時未知\n![](/media/15150479915859.jpg)\n![](/media/15150480384222.jpg)\n\n### 3. Local Feature + ReID\n論文[3]用local feature而不用global feature，切割好以後送到LSTM去學\n![](/media/15150481401848.jpg)\n\n但論文[3]會有對齊問題，所以論文[4]用pose跟skeleton來做姿勢預測，再通過仿射變換對齊\n\n![](/media/15150485085441.jpg)\n\n論文[5]直接拿關節點切出ROI，14個人體關節點，得到7個ROI區域，(頭、上身、下身和四肢)\n![](/media/15150485782017.jpg)\n\n\n### 4. Video Sequence + ReID\n這方向不熟 貼兩張圖參考參考而已\n![](/media/15150487793131.jpg)\n![](/media/15150488009610.jpg)\n\n### 5. GAN + ReID\nReID數據集目前最大的也只有幾千個ID，跟萬張圖片而已，CNN based還容易overfitting\nGAN主要是用在遷移學習跟基於條件的生成\n\n第一篇就是ICCV2017的論文[5]以及後來同作者改進的論文[6]，是可以避免overfitting但生成效果就很慘\n![](/media/15150491105798.jpg)\n\n為了處理不同數據集，甚至是不同camera所造成bias的問題，論文[7]是利用cycleGAN based的設計，利用遷移學習來處理兩個數同數據集的問題，先切割分前景跟背景，在轉換過去。\nD有兩個loss(還是有兩個D不確定，paper內沒架構圖)一個是前景的絕對誤差loss，一個是正常的判別器loss。判別器loss是用來判斷生成的圖屬於哪個domain，前景的loss是為了保證行人前景儘可能逼真不變。mask用PSPnet來找的。\n\n![](/media/15150492644360.jpg)\n\n\nPose Normalization[8]\n![](/media/15150490021821.jpg)\n![](/media/15150490274695.jpg)\n![](/media/15150495642066.jpg)\n\n\n## 資料種類\n- Video-based\n- Image-based\n- Long-term activity\n- Individual action \n\n## 資料庫\n[Robust Systems Lab](http://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/projectpages/reiddataset.html)\n## 程式碼\n\n[简单行人重识别代码到88%准确率](https://zhuanlan.zhihu.com/p/32585203)\nhttps://github.com/layumi/Person_reID_baseline_pytorch\n\n- ### ICCV 2017\n    - [Cross-view Asymmetric Metric Learning for Unsupervised Re-id ](https://github.com/KovenYu/CAMEL)\n    - [Deeply-Learned Part-Aligned Representations for Person Re-Identification ](https://github.com/zlmzju/part_reid)\n    - [In Defense of the Triplet Loss for Person Re-Identification ](https://github.com/VisualComputingInstitute/triplet-reid)\n    - [Jointly Attentive Spatial-Temporal Pooling Networks for Video-based Person Re-Identification](https://github.com/shuangjiexu/Spatial-Temporal-Pooling-Networks-ReID)\n    - [SVDNet for Pedestrian Retrieval](http://github.com/syfafterzy/SVDNet-for-Pedestrian-Retrieval)\n    - [Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro](http://github.com/layumi/Person-reID_GAN)\n\n\n- ### CVPR 2017\n    - [Spindle Net: Person Re-Identification With Human Body Region Guided Feature Decomposition and Fusion](http://github.com/yokattame/SpindleNet)\n    - [Person Re-Identification in the Wild](http://github.com/liangzheng06/PRW-baseline) \n    - [Joint Detection and Identification Feature Learning for Person Search](http://github.com/ShuangLI59/person_search)\n    - [Quality Aware Network for Set to Set Recognition](http//github.com/sciencefans/Quality-Aware-Network)\n\n\n\n\n\n\n## Paper List\n \n    - Point to Set Similarity Based Deep Feature Learning for Person Re-Identification\n    - Fast Person Re-Identification via Cross-Camera Semantic Binary Transformation\n    - See the Forest for the Trees: Joint Spatial and Temporal Recurrent Neural Networks for Video-Based Person Re-Identification\n    - Learning Deep Context-Aware Features Over Body and Latent Parts for Person Re-Identification\n    - Consistent-Aware Deep Learning for Person Re-Identification in a Camera Network\n    - Re-Ranking Person Re-Identification With k-Reciprocal Encoding\n    - Multiple People Tracking by Lifted Multicut and Person Re-Identification\n\n\n[1] Mengyue Geng, Yaowei Wang, Tao Xiang, Yonghong Tian. Deep transfer learning for person reidentification[J]. arXiv preprint arXiv:1611.05244, 2016.\n\n[2] Yutian Lin, Liang Zheng, Zhedong Zheng, YuWu, Yi Yang. Improving person re-identification by attribute and identity learning[J]. arXiv preprint arXiv:1703.07220, 2017.\n\n[3] Rahul Rama Varior, Bing Shuai, Jiwen Lu, Dong Xu, Gang Wang. A siamese long short-term memory architecture for human re-identification[C]//European Conference on Computer Vision. Springer, 2016:135–153.\n\n[4]Liang Zheng, Yujia Huang, Huchuan Lu, Yi Yang. Pose invariant embedding for deep person reidentification[J]. arXiv preprint arXiv:1701.07732, 2017.\n\n[5]  Haiyu Zhao, Maoqing Tian, Shuyang Sun, Jing Shao, Junjie Yan, Shuai Yi, Xiaogang Wang, Xiaoou Tang. Spindle net: Person re-identification with human body region guided feature decomposition and fusion[C]. CVPR, 2017.\n\n[6] Zhong Z, Zheng L, Zheng Z, et al. Camera Style Adaptation for Person Re-identification[J]. arXiv preprint arXiv:1711.10295, 2017.\n\n[7] Wei L, Zhang S, Gao W, et al. Person Transfer GAN to Bridge Domain Gap for Person Re-Identification[J]. arXiv preprint arXiv:1711.08565, 2017.\n\n[8] Qian X, Fu Y, Wang W, et al. Pose-Normalized Image Generation for Person Re-identification[J]. arXiv preprint arXiv:1712.02225, 2017.\n\n","tags":["Tracking","image retrieval","survey"],"categories":["Re-ID"]},{"title":"PYTHON中如何使用*ARGS和**KWARGS","url":"/2017/12/24/PYTHON中如何使用-ARGS和-KWARGS/","content":"\n範例與翻譯理解自[連結](http://www.saltycrane.com/blog/2008/01/how-to-use-args-and-kwargs-in-python/)與[連結](http://appsgaga.com/python%E5%AD%B8%E7%BF%92%E7%AD%86%E8%A8%98-init%E6%96%B9%E6%B3%95%E4%B8%AD%EF%BC%8Cargs-kwargs%E9%80%99%E5%85%A9%E5%80%8B%E5%8F%83%E6%95%B8%E6%98%AF%E4%BB%80%E9%BA%BC%EF%BC%9F/)\n\n\n## \\*args跟 \\*\\*kwargs是類似的東西，是可有可無的參數。 一顆星的\\*args是tuple，可以接受很多的值。兩顆星的**kwargs一樣是可以接受很多值，但是是接受dictionary。\n\n###一顆星用法\n\n範例\n\n````\ndef test_var_args(farg, *args):\n    print \"formal arg:\", farg\n    for arg in args:\n        print \"another arg:\", arg\n        \ntest_var_args(1, \"two\", 3)\n````\n\n结果\n\n````python\nformal arg: 1\nanother arg: two\nanother arg: 3\n````\n###兩顆星用法\n\n範例\n\n````python\ndef test_var_kwargs(farg, **kwargs):\n    print \"formal arg:\", farg\n    for key in kwargs:\n        print \"another keyword arg: %s: %s\" % (key, kwargs[key])\n\ntest_var_kwargs(farg=1, myarg2=\"two\", myarg3=3)\n````\n\n结果\n\n````python\nformal arg: 1\nanother keyword arg: myarg2: two\nanother keyword arg: myarg3: 3\n````\n\n![](/media/15141999823321.png)\n\n### 順序問題\n如果function定義時如上圖先放了tuple才是dictionary，呼叫時參數先放dictionary再放tuple會跳error。\n\n\n\n","tags":["code","python"],"categories":["Code"]},{"title":"Python 筆記","url":"/2017/12/13/python-note/","content":"Different in Py2 and Py3\n\nPickle module\n\n````python\nimport sys\nif sys.version_info[0]<3:\n    import cPickle as pickle\nelse\n    import _pickle as pickle    \n````\n\n","tags":["code"],"categories":["Python"]},{"title":"zi2zi: Master Chinese Calligraphy with Conditional Adversarial Networks","url":"/2017/12/01/zi2zi/","content":"\n![](/media/15128482902404.png)\nGenerated samples. Related code can be found [here](https://github.com/kaonashi-tyc/zi2zi)\n\n## 目標\n\n字體風格轉換\n![](/media/15128485151045.png)\n\n## 動機\n\n\n直接用CNN進行風格轉換會有下列問題\n1. 生成常常是模糊的\n2. 多數生成結果是失敗的\n3. 只能做一對一生成\n\n![](/media/15128485338820.png)\n\n結論：用GAN試試看\n\n \n\n## 用GAN秒殺一切！\n\n這篇借鑒了三篇paper內容\n\n[Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004)\n[Conditional Image Synthesis With Auxiliary Classifier GANs](https://arxiv.org/abs/1610.09585)\n[Unsupervised Cross-Domain Image Generation](https://arxiv.org/abs/1611.02200)\n\n主要是由pix2pix這篇修改而來的\n![](/media/15128487302209.png)\n\n其中Encoder跟Decoder還有Discriminator是直接用pix2pix的, 尤其是裡面的Unet模型\n\n","tags":["Generation","Generative Model","GAN","Style Transfer"],"categories":["GAN"]},{"title":"Faster R-CNN","url":"/2017/11/11/Faster R-CNN/","content":"#Outline\n- R-CNN, Fast R-CNN, Faster R-CNN 關係\n- 流程圖\n- BackBone\n- RPN\n    - Anchor\n    - ROI Pooling\n    - Bounding box regression\n- 訓練\n- 測試\n- \n\n\n## R-CNN, Fast R-CNN, Faster R-CNN 關係\n| <center>網路架構</center>                                                     | <center>使用方法</center>                                                                                                                                                       | <center>流程</center>                                                                                                                                                                                                                    | <center>缺點</center>                                                                                                                                             | <center>重點貢獻</center>                                                                                                               |\n|--------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------|\n| R-CNN            | 1.使用Selective Search 提取 Region Propose <br>2. 利用在ImageNet 訓練的CNN網路提取特徵 <br>3.Bounding Box regression做邊框計算 <br>4.由K個二分類SVM做分類(Ｋ= 任務分類數量) | 1.先利用SS提取2K個候選區(圖片) <br>2.每個候選區經過CNN取得特徵 <br>3.特徵交由每個SVM計算分數以及Bbox Regressor調整邊框範圍                                                                                                      | 1.訓練步驟繁瑣（微調網絡+訓練SVM +訓練bbox）<br>2.訓練，測試均速度慢 <br>3.訓練佔空間(每張圖片SS提取了2K個候選區)                                     | 1.從DPM HSC的34.3％直接提升到了66％（mAP）<br>2.引入 CNN來做檢測問題|\n| Fast R-CNN   | 1.使用Selective Search 提取 Region Propose <br>2. 利用在ImageNet 訓練的CNN網路提取特徵 <br>3.Bounding Box regression配合多任務損失函數做邊框計算 <br>4.Softmax做分類       | 1.先利用SS提取2K個候選區(邊框資訊) <br>2.整張圖用CNN取得特徵圖 <br>3.將每個圖提取的邊框資訊(2K個候選區)對應到特徵圖上取得2K個特徵<br>4.透過ROI Pooling將不同大小的特徵統一成同樣大小<br>5.每個特徵交經過處理後由Softmax計算分數以及Bbox Regressor調整邊框範圍                          | 1.依舊用SS提取RP（耗時2-3s，特徵提取耗時0.32s）<br>2.無法滿足實時應用，沒有真正實現端到端訓練測試<br>3.利用了GPU，但是RP是在CPU上運算| 1.由66.9％提升至70％<br>2.每張圖像耗時約為3s。                                                                             |\n| Faster R-CNN | 1.使用Region Propose Network 提取 Region Propose <br>2. 利用在ImageNet 訓練的CNN網路提取特徵 <br>3. BBox Regressor 配合多任務損失函數做邊框計算<br>4.Softmax做分類  | 1.原圖先經過網路得到特徵圖 <br>2.特徵圖經過RPN得到Proposed Region以及相應對分數 <br>3. 採用NMS篩出Top-N個框(N=300) <br>4.透過ROI Pooling將不同大小的特徵統一成同樣大小<br>5.N個篩選出的框對應回原特徵圖取得N特徵 <br>6.每個特徵交經過處理後由Softmax計算分數以及Bbox Regressor調整邊框範圍 | 1.還是無法達到Real-Time detection <br>2.計算量還是比較大，因為經過RPN得到多個Proposed Region，再對每個RP做Classification的話，還是有大量的重複計算。 | 1.提出了RPN，換掉了速度瓶頸的關鍵(SS)，提高了檢測精度和速度<br>2.真正實現end-to-end的目標檢測框架<br>3.RPN生成bbox僅需約10ms。 |\n\n\n\n\n\n## 流程圖\n![](/media/15231974376489.jpg)\n![](/media/15232012344585.jpg)\n\n(1）輸入測試圖像;\n(2）將整張圖片輸入CNN，進行特徵提取;\n(3）用RPN生成建議窗口(建議)，每張圖片生成300個建議窗口;\n(4）把建議窗口映射到CNN的最後一層卷積feature map上;\n(5）通過RoI pooling層使每個RoI生成固定尺寸的特徵圖;\n(6）利用Softmax Loss（探測分類概率）和Smooth L1 Loss（探測邊框回歸）對分類概率和邊框回歸（Bounding box regression）聯合訓練。\n## Backbone\n原論文沒有特別提網路架構 一般來說是用ZF Net後來都是用VGG架構，甚至是後來用RESNET或是FPN\n這裡就VGG16來討論，此結構中需要注意的是\n**所有的Conv層都是：kernel_size = 3，pad= 1**\n**所有的Pooling都是：kernel_size = 2，stride= 2**\nFaster RCNN 中Conv層對所有的捲積都做了擴邊處理（pad = 1，即填充一圈0），導致原圖變為（M+2）x（N+2）大小，再做3x3卷積後輸出MxN \n\n```    \n輸出圖片大小計算：[（M + 2） -  3 +1] x [（N + 2） -  3 +1 ]\n```\n因此Conv層中的轉換次數層不改變輸入和輸出矩陣大小\n\n![](/media/15231978666930.jpg)\n\n說明：卷積核mxm，輸入圖片WxH（擴邊之後的尺寸），則輸出圖片的尺寸是（W-m + 1）x（H-m + 1）\n\n## RPN\n![](/media/15232012107286.jpg)\n\n### Anchor\n\n\n### ROI POOLING\nROI POOLING 概念如下圖所示\n![](/media/15137535435975.png)\n\n![](/media/15137536007979.gif)\n\n\n","tags":["Deep Learning","R-CNN","CNN","Object Detection"],"categories":["Detection"]},{"title":"Conditional GAN - 條件式生成對抗網路","url":"/2017/11/08/Conditional-GAN/","content":"\n\n","tags":["Deep Learning","Generation","Generative Model","GAN"],"categories":["GAN"]},{"title":"GAN - Generative Adversarial Network","url":"/2017/11/07/GAN-Generative-Adversarial-Network/","content":"\n\n## Generative Adversarial Networks (GANs)\n### Lists  \n\n*Name* | *Paper Link* | *Value Function*\n:---: | :---: | :--- |\n**GAN** | [Arxiv](https://arxiv.org/abs/1406.2661) | <img src = '/media/equations/GAN.png' >\n**LSGAN**| [Arxiv](https://arxiv.org/abs/1611.04076) | <img src = '/media/equations/LSGAN.png' >\n**WGAN**| [Arxiv](https://arxiv.org/abs/1701.07875) | <img src = '/media/equations/WGAN.png' >\n**WGAN-GP**| [Arxiv](https://arxiv.org/abs/1704.00028) | <img src = '/media/equations/WGAN_GP.png' >\n**DRAGAN**| [Arxiv](https://arxiv.org/abs/1705.07215) | <img src = '/media/equations/DRAGAN.png' >\n**CGAN**| [Arxiv](https://arxiv.org/abs/1411.1784) | <img src = '/media/equations/CGAN.png' >\n**infoGAN**| [Arxiv](https://arxiv.org/abs/1606.03657) | <img src = '/media/equations/infoGAN.png' >\n**ACGAN**| [Arxiv](https://arxiv.org/abs/1610.09585) | <img src = '/media/equations/ACGAN.png' >\n**EBGAN**| [Arxiv](https://arxiv.org/abs/1609.03126) | <img src = '/media/equations/EBGAN.png' >\n**BEGAN**| [Arxiv](https://arxiv.org/abs/1702.08431) | <img src = '/media/equations/BEGAN.png' >  \n\n#### Variants of GAN structure\n<img src = '/media/etc/GAN_structure.png' >\n\n### Results for mnist\nNetwork architecture of generator and discriminator is the exaclty sames as in [infoGAN paper](https://arxiv.org/abs/1606.03657).  \nFor fair comparison of core ideas in all gan variants, all implementations for network architecture are kept same except EBGAN and BEGAN. Small modification is made for EBGAN/BEGAN, since those adopt auto-encoder strucutre for discriminator. But I tried to keep the capacity of discirminator.\n\n\n#### Random generation\nAll results are randomly sampled.\n\n*Name* | *Epoch 2* | *Epoch 10* | *Epoch 25*\n:---: | :---: | :---: | :---: |\nGAN | <img src = '/media/mnist_results/random_generation/GAN_epoch001_test_all_classes.png' > | <img src = '/media/mnist_results/random_generation/GAN_epoch009_test_all_classes.png' > | <img src = '/media/mnist_results/random_generation/GAN_epoch024_test_all_classes.png' >\nLSGAN | <img src = '/media/mnist_results/random_generation/LSGAN_epoch001_test_all_classes.png' > | <img src = '/media/mnist_results/random_generation/LSGAN_epoch009_test_all_classes.png' > | <img src = '/media/mnist_results/random_generation/LSGAN_epoch024_test_all_classes.png' >\nWGAN | <img src = '/media/mnist_results/random_generation/WGAN_epoch001_test_all_classes.png' > | <img src = '/media/mnist_results/random_generation/WGAN_epoch009_test_all_classes.png' > | <img src = '/media/mnist_results/random_generation/WGAN_epoch024_test_all_classes.png' >\nWGAN-GP | <img src = '/media/mnist_results/random_generation/WGAN-GP_epoch001_test_all_classes.png' > | <img src = '/media/mnist_results/random_generation/WGAN-GP_epoch009_test_all_classes.png' > | <img src = '/media/mnist_results/random_generation/WGAN-GP_epoch024_test_all_classes.png' >\nDRAGAN | <img src = '/media/mnist_results/random_generation/DRAGAN_epoch001_test_all_classes.png' > | <img src = '/media/mnist_results/random_generation/DRAGAN_epoch009_test_all_classes.png' > | <img src = '/media/mnist_results/random_generation/DRAGAN_epoch024_test_all_classes.png' >\nEBGAN | <img src = '/media/mnist_results/random_generation/EBGAN_epoch001_test_all_classes.png' > | <img src = '/media/mnist_results/random_generation/EBGAN_epoch009_test_all_classes.png' > | <img src = '/media/mnist_results/random_generation/EBGAN_epoch024_test_all_classes.png' >\nBEGAN | <img src = '/media/mnist_results/random_generation/BEGAN_epoch001_test_all_classes.png' > | <img src = '/media/mnist_results/random_generation/BEGAN_epoch009_test_all_classes.png' > | <img src = '/media/mnist_results/random_generation/BEGAN_epoch024_test_all_classes.png' >\n\n#### Conditional generation\nEach row has the same noise vector and each column has the same label condition.\n\n*Name* | *Epoch 1* | *Epoch 10* | *Epoch 25*\n:---: | :---: | :---: | :---: |\nCGAN | <img src = '/media/mnist_results/conditional_generation/CGAN_epoch000_test_all_classes_style_by_style.png' > | <img src = '/media/mnist_results/conditional_generation/CGAN_epoch009_test_all_classes_style_by_style.png' > | <img src = '/media/mnist_results/conditional_generation/CGAN_epoch024_test_all_classes_style_by_style.png' >\nACGAN | <img src = '/media/mnist_results/conditional_generation/ACGAN_epoch000_test_all_classes_style_by_style.png' > | <img src = '/media/mnist_results/conditional_generation/ACGAN_epoch009_test_all_classes_style_by_style.png' > | <img src = '/media/mnist_results/conditional_generation/ACGAN_epoch024_test_all_classes_style_by_style.png' >\ninfoGAN | <img src = '/media/mnist_results/conditional_generation/infoGAN_epoch000_test_all_classes_style_by_style.png' > | <img src = '/media/mnist_results/conditional_generation/infoGAN_epoch009_test_all_classes_style_by_style.png' > | <img src = '/media/mnist_results/conditional_generation/infoGAN_epoch024_test_all_classes_style_by_style.png' >\n\n#### InfoGAN : Manipulating two continous codes\n<table align='center'>\n<td><img src = '/media/mnist_results/infogan/infoGAN_epoch024_test_class_c1c2_2.png' ></td>\n<td><img src = '/media/mnist_results/infogan/infoGAN_epoch024_test_class_c1c2_5.png' ></td>\n<td><img src = '/media/mnist_results/infogan/infoGAN_epoch024_test_class_c1c2_7.png' ></td>\n<td><img src = '/media/mnist_results/infogan/infoGAN_epoch024_test_class_c1c2_9.png' ></td>\n</table>\n\n### Results for fashion-mnist\nComments on network architecture in mnist are also applied to here.  \n[Fashion-mnist](https://github.com/zalandoresearch/fashion-mnist) is a recently proposed dataset consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. (T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot)\n\n\n#### Random generation\nAll results are randomly sampled.\n\n*Name* | *Epoch 1* | *Epoch 20* | *Epoch 40*\n:---: | :---: | :---: | :---: |\nGAN | <img src = '/media/fashion_mnist_results/random_generation/GAN_epoch000_test_all_classes.png' > | <img src = '/media/fashion_mnist_results/random_generation/GAN_epoch019_test_all_classes.png' > | <img src = '/media/fashion_mnist_results/random_generation/GAN_epoch039_test_all_classes.png' >\nLSGAN | <img src = '/media/fashion_mnist_results/random_generation/LSGAN_epoch000_test_all_classes.png' > | <img src = '/media/fashion_mnist_results/random_generation/LSGAN_epoch019_test_all_classes.png' > | <img src = '/media/fashion_mnist_results/random_generation/LSGAN_epoch039_test_all_classes.png' >\nWGAN | <img src = '/media/fashion_mnist_results/random_generation/WGAN_epoch000_test_all_classes.png' > | <img src = '/media/fashion_mnist_results/random_generation/WGAN_epoch019_test_all_classes.png' > | <img src = '/media/fashion_mnist_results/random_generation/WGAN_epoch039_test_all_classes.png' >\nWGAN-GP | <img src = '/media/fashion_mnist_results/random_generation/WGAN-GP_epoch000_test_all_classes.png' > | <img src = '/media/fashion_mnist_results/random_generation/WGAN-GP_epoch019_test_all_classes.png' > | <img src = '/media/fashion_mnist_results/random_generation/WGAN-GP_epoch039_test_all_classes.png' >\nDRAGAN | <img src = '/media/fashion_mnist_results/random_generation/DRAGAN_epoch000_test_all_classes.png' > | <img src = '/media/fashion_mnist_results/random_generation/DRAGAN_epoch019_test_all_classes.png' > | <img src = '/media/fashion_mnist_results/random_generation/DRAGAN_epoch039_test_all_classes.png' >\nEBGAN | <img src = '/media/fashion_mnist_results/random_generation/EBGAN_epoch000_test_all_classes.png' > | <img src = '/media/fashion_mnist_results/random_generation/EBGAN_epoch019_test_all_classes.png' > | <img src = '/media/fashion_mnist_results/random_generation/EBGAN_epoch039_test_all_classes.png' >\nBEGAN | <img src = '/media/fashion_mnist_results/random_generation/BEGAN_epoch000_test_all_classes.png' > | <img src = '/media/fashion_mnist_results/random_generation/BEGAN_epoch019_test_all_classes.png' > | <img src = '/media/fashion_mnist_results/random_generation/BEGAN_epoch039_test_all_classes.png' >\n\n#### Conditional generation\nEach row has the same noise vector and each column has the same label condition.\n\n*Name* | *Epoch 1* | *Epoch 20* | *Epoch 40*\n:---: | :---: | :---: | :---: |\nCGAN | <img src = '/media/fashion_mnist_results/conditional_generation/CGAN_epoch000_test_all_classes_style_by_style.png' > | <img src = '/media/fashion_mnist_results/conditional_generation/CGAN_epoch019_test_all_classes_style_by_style.png' > | <img src = '/media/fashion_mnist_results/conditional_generation/CGAN_epoch039_test_all_classes_style_by_style.png' >\nACGAN | <img src = '/media/fashion_mnist_results/conditional_generation/ACGAN_epoch000_test_all_classes_style_by_style.png' > | <img src = '/media/fashion_mnist_results/conditional_generation/ACGAN_epoch019_test_all_classes_style_by_style.png' > | <img src = '/media/fashion_mnist_results/conditional_generation/ACGAN_epoch039_test_all_classes_style_by_style.png' >\ninfoGAN | <img src = '/media/fashion_mnist_results/conditional_generation/infoGAN_epoch000_test_all_classes_style_by_style.png' > | <img src = '/media/fashion_mnist_results/conditional_generation/infoGAN_epoch019_test_all_classes_style_by_style.png' > | <img src = '/media/fashion_mnist_results/conditional_generation/infoGAN_epoch039_test_all_classes_style_by_style.png' >\n\nWithout hyper-parameter tuning from mnist-version, ACGAN/infoGAN does not work well as compared with CGAN.  \nACGAN tends to fall into mode-collapse.  \ninfoGAN tends to ignore noise-vector. It results in that various style within the same class can not be represented.\n\n#### InfoGAN : Manipulating two continous codes\n<table align='center'>\n<td><img src = '/media/fashion_mnist_results/infogan/infoGAN_epoch039_test_class_c1c2_1.png' ></td>\n<td><img src = '/media/fashion_mnist_results/infogan/infoGAN_epoch039_test_class_c1c2_4.png' ></td>\n<td><img src = '/media/fashion_mnist_results/infogan/infoGAN_epoch039_test_class_c1c2_5.png' ></td>\n<td><img src = '/media/fashion_mnist_results/infogan/infoGAN_epoch039_test_class_c1c2_8.png' ></td>\n</table>\n\n### Some results for celebA\n(to be added)\n\n## Variational Auto-Encoders (VAEs)\n\n### Lists\n\n*Name* | *Paper Link* | *Loss Function*\n:---: | :---: | :---\n**VAE**| [Arxiv](https://arxiv.org/abs/1312.6114) | <img src = '/media/equations/CVAE.png' > \n**CVAE**| [Arxiv](https://arxiv.org/abs/1406.5298) | <img src = '/media/equations/CVAE.png' >\n**DVAE**| [Arxiv](https://arxiv.org/abs/1511.06406) | (to be added)\n**AAE**| [Arxiv](https://arxiv.org/abs/1511.05644) | (to be added) \n\n#### Variants of VAE structure\n<img src = '/media/etc/VAE_structure.png' >\n\n### Results for mnist\nNetwork architecture of decoder(generator) and encoder(discriminator) is the exaclty sames as in [infoGAN paper](https://arxiv.org/abs/1606.0365). The number of output nodes in encoder is different. (2x z_dim for VAE, 1 for GAN)\n\n\n#### Random generation\nAll results are randomly sampled.\n\n*Name* | *Epoch 1* | *Epoch 10* | *Epoch 25*\n:---: | :---: | :---: | :---: |\nVAE | <img src = '/media/mnist_results/random_generation/VAE_epoch000_test_all_classes.png' > | <img src = '/media/mnist_results/random_generation/VAE_epoch009_test_all_classes.png' > | <img src = '/media/mnist_results/random_generation/VAE_epoch024_test_all_classes.png' >\nGAN | <img src = '/media/mnist_results/random_generation/GAN_epoch000_test_all_classes.png' > | <img src = '/media/mnist_results/random_generation/GAN_epoch009_test_all_classes.png' > | <img src = '/media/mnist_results/random_generation/GAN_epoch024_test_all_classes.png' >\n\nResults of GAN is also given to compare images generated from VAE and GAN.\nThe main difference (VAE generates smooth and blurry images, otherwise GAN generates sharp and artifact images) is cleary observed from the results.\n\n#### Conditional generation\nEach row has the same noise vector and each column has the same label condition.\n\n*Name* | *Epoch 1* | *Epoch 10* | *Epoch 25*\n:---: | :---: | :---: | :---: |\nCVAE | <img src = '/media/mnist_results/conditional_generation/CVAE_epoch000_test_all_classes_style_by_style.png' > | <img src = '/media/mnist_results/conditional_generation/CVAE_epoch009_test_all_classes_style_by_style.png' > | <img src = '/media/mnist_results/conditional_generation/CVAE_epoch024_test_all_classes_style_by_style.png' >\nCGAN | <img src = '/media/mnist_results/conditional_generation/CGAN_epoch000_test_all_classes_style_by_style.png' > | <img src = '/media/mnist_results/conditional_generation/CGAN_epoch009_test_all_classes_style_by_style.png' > | <img src = '/media/mnist_results/conditional_generation/CGAN_epoch024_test_all_classes_style_by_style.png' >\n\nResults of CGAN is also given to compare images generated from CVAE and CGAN.\n\n#### Learned manifold\n\nThe following results can be reproduced with command:  \n```\npython main.py --dataset mnist --gan_type VAE --epoch 25 --batch_size 64 --dim_z 2\n```\nPlease notice that dimension of noise-vector z is 2.\n\n*Name* | *Epoch 1* | *Epoch 10* | *Epoch 25*\n:---: | :---: | :---: | :---: |\nVAE | <img src = '/media/mnist_results/learned_manifold/VAE_epoch000_learned_manifold.png' > | <img src = '/media/mnist_results/learned_manifold/VAE_epoch009_learned_manifold.png' > | <img src = '/media/mnist_results/learned_manifold/VAE_epoch024_learned_manifold.png' >\n\n### Results for fashion-mnist\nComments on network architecture in mnist are also applied to here. \n\nThe following results can be reproduced with command:  \n```\npython main.py --dataset fashion-mnist --gan_type <TYPE> --epoch 40 --batch_size 64\n```\n\n#### Random generation\nAll results are randomly sampled.\n\n*Name* | *Epoch 1* | *Epoch 20* | *Epoch 40*\n:---: | :---: | :---: | :---: |\nVAE | <img src = '/media/fashion_mnist_results/random_generation/VAE_epoch000_test_all_classes.png' > | <img src = '/media/fashion_mnist_results/random_generation/VAE_epoch019_test_all_classes.png' > | <img src = '/media/fashion_mnist_results/random_generation/VAE_epoch039_test_all_classes.png' >\nGAN | <img src = '/media/fashion_mnist_results/random_generation/GAN_epoch000_test_all_classes.png' > | <img src = '/media/fashion_mnist_results/random_generation/GAN_epoch019_test_all_classes.png' > | <img src = '/media/fashion_mnist_results/random_generation/GAN_epoch039_test_all_classes.png' >\n\nResults of GAN is also given to compare images generated from VAE and GAN.\n\n#### Conditional generation\nEach row has the same noise vector and each column has the same label condition.\n\n*Name* | *Epoch 1* | *Epoch 20* | *Epoch 40*\n:---: | :---: | :---: | :---: |\nCVAE | <img src = '/media/fashion_mnist_results/conditional_generation/CVAE_epoch000_test_all_classes_style_by_style.png' > | <img src = '/media/fashion_mnist_results/conditional_generation/CVAE_epoch019_test_all_classes_style_by_style.png' > | <img src = '/media/fashion_mnist_results/conditional_generation/CVAE_epoch039_test_all_classes_style_by_style.png' >\nCGAN | <img src = '/media/fashion_mnist_results/conditional_generation/CGAN_epoch000_test_all_classes_style_by_style.png' > | <img src = '/media/fashion_mnist_results/conditional_generation/CGAN_epoch019_test_all_classes_style_by_style.png' > | <img src = '/media/fashion_mnist_results/conditional_generation/CGAN_epoch039_test_all_classes_style_by_style.png' >\n\nResults of CGAN is also given to compare images generated from CVAE and CGAN.\n\n#### Learned manifold\n\nThe following results can be reproduced with command:  \n```\npython main.py --dataset fashion-mnist --gan_type VAE --epoch 25 --batch_size 64 --dim_z 2\n```\nPlease notice that dimension of noise-vector z is 2.\n\n*Name* | *Epoch 1* | *Epoch 10* | *Epoch 25*\n:---: | :---: | :---: | :---: |\nVAE | <img src = '/media/fashion_mnist_results/learned_manifold/VAE_epoch000_learned_manifold.png' > | <img src = '/media/fashion_mnist_results/learned_manifold/VAE_epoch009_learned_manifold.png' > | <img src = '/media/fashion_mnist_results/learned_manifold/VAE_epoch024_learned_manifold.png' >\n\n\n","tags":["Deep Learning","Generation","Generative Model","GAN"],"categories":["GAN"]},{"title":"Self-Study Courses List","url":"/2017/11/07/Self-Study-Courses-List/","content":"\n## 學習路程\n- Python\n    政大 or MIT python都好 後者比較舊但作業比較多\n- ML\n    先看莫凡 看完再看NTU基礎的 剩下就看要看進階的還是standford的\n\n## Basic Concept of Machine Learning\n- Morvan莫凡\n\t- [有趣的機器學習](https://www.youtube.com/playlist?list=PLXO45tsB95cIFm8Y8vMkNNPPXAtYXwKin)\n\n## Machine Learning \n- [Udacity ud120](https://cn.udacity.com/course/intro-to-machine-learning--ud120)\n\n\n## Machine Learning + Deep Learning Series\n- ### National Taiwan University - 李宏毅\n\t- BASIC\n      - [Machine Learning (2016,Fall)](http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html)\n      - [Machine Learning (2017,Spring)](http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML17.html)\n      - [Machine Learning (2017,Spring)](http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML17_2.html)\n    \n    - ADV\n      - [Machine Learning and having it deep and structured (2017,Spring](http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS17.html)\n      - [Machine Learning and having it deep and structured (2017,Fall)](speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS17.html)\n      -\n - ### National Taiwan University - 林軒田\n    - [機器學習基石](https://www.youtube.com/watch?v=nQvpFSMPhr0&list=PLXVfgk9fNX2I7tB6oIINGBmW50rrmFTqf)\n    - [機器學習技法](https://www.youtube.com/watch?v=A-GxGCCAIrg&list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2)\n\n - ### Coursera\n    - [Deep Learning Specialization\n](https://www.coursera.org/specializations/deep-learning)\n\n- ### blog\n    - [Blog](http://www.cosmosshadow.com/ml)\n    - [資料科學・機器・人](https://brohrer.mcknote.com/zh-Hant/)\n- ### GitHub-sjchoi86\n    https://github.com/sjchoi86/dl-workshop\n    https://github.com/sjchoi86/dl_tutorials\n    https://github.com/sjchoi86/Deep-Learning-101\n    https://github.com/sjchoi86/dl_tutorials_10weeks\n    https://github.com/sjchoi86/dl_tutorials_3rd\n    \n- ### Deep Learning Specialize\n    - [網易雲中文版](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)  \n- ### Colorado \n    - [Neural Networks and Deep Learning 2017 Fall](https://www.cs.colorado.edu/~mozer/Teaching/syllabi/DeepLearningFall2017/) \n- ### Book\n    - [花書](https://github.com/zsdonghao/deep-learning-book) \n- ### Google Brain Hugo Larochelle\n    - [Mooc.ai](http://www.mooc.ai/course/300#modal)\n## Visual\n- ### Stanford CS213\n  - [2016 Syllabus](http://cs231n.stanford.edu/2016/syllabus) \n  - [2016 Winter](https://www.youtube.com/playlist?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC)\n  - [2016 winter 中文版](http://study.163.com/course/introduction/1003223001.htm)\n\n  - [2017 Syllabus](http://cs231n.stanford.edu/2017/syllabus)\n  - [2017 Spring](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)\n  - [2017 winter 中文版](http://www.mooc.ai/course/268)\n  \n## Python\n- ### NCCU \n    - [Data science with Python](http://moocs.nccu.edu.tw/course/121/intro)\n- ### MIT\n    - [MIT Python](http://learn.edx.org/mit-python/) \n- ### Blog\n    - [saltycrane](https://www.saltycrane.com/blog/tag/python/)   \n    - [30天python雜談](https://ithelp.ithome.com.tw/users/20107274/ironman/1578)\n- ### Effective Python\n    - [59 Specific Ways to Write Better Python簡中翻譯](https://guoruibiao.gitbooks.io/effective-python/content/)\n    -[Python慣用語&Effective Python](http://seanlin.logdown.com/archives)\n    \n## Deep Learning Framework Tutorial\n- ### Tensorflow\n    - [Tensorflow 官方](tensorflow.org)\n    - [Tensorflow 中文](http://usyiyi.cn/documents/tensorflow_13/tutorials/index.html) \n    - [Tensorflow 中文2](wiki.jikexueyuan.com/project/tensorflow-zh)\n    - [Stanford CS20SI - TensorFlow](https://www.youtube.com/playlist?list=PLQ0sVbIj3URf94DQtGPJV629ctn2c1zN-)\n    - [Udacity ud730](https://www.udacity.com/course/deep-learning--ud730)\n    - [cognitiveclass.ai](https://www.youtube.com/playlist?list=PL-XeOa5hMEYxNzHM7YLRjIwE1k3VQpqEh)\n    - [learning Tensorflow Web](http://learningtensorflow.com/)\n    - [Stanford CS 20](https://web.stanford.edu/class/cs20si/index.html)\n    - [Data Camp](https://www.datacamp.com/community/tutorials/cnn-tensorflow-python)\n- ### Pytorch\n    - []()\n    \n- ### Keras\n    - [Data Camp](https://www.datacamp.com/community/tutorials/deep-learning-python)\n    - [緯創資通](https://github.com/erhwenkuo/deep-learning-with-keras-notebooks) \n\n## Tech Paper\n- [dl_tutorials](https://github.com/sjchoi86/dl_tutorials)   \n- [dl_tutorials_10weeks](https://github.com/sjchoi86/dl_tutorials_10weeks)\n   \n## Deep Learning in practice\n- [Fast.ai](http://www.fast.ai/) \n    - [Fast.ai 2018 Course](http://course.fast.ai/) \n    - [Fast.ai 2017 Course](course17.fast.ai)\n\n- ### Tensorflow\n    - [Tensorflow 101](https://github.com/sjchoi86/Tensorflow-101)\n\n- ### Keras\n    - [緯創]( https://github.com/erhwenkuo/deep-learning-with-keras-notebooks)\n    \n- ### MXNET\n    -[Gluon](https://zh.gluon.ai/)\n    -[Gluon-Github](https://github.com/mli/gluon-tutorials-zh/)\n\n- ### Udacity Self-drive\n    - [self-driving-car](https://github.com/ndrplz/self-driving-car) \n## Concept\n-CapsuleNet\n- [李宏毅](https://www.youtube.com/watch?v=UhGWH3hb3Hk)\n\n## Git\n- [Git](https://try.github.io/)\n- \n## Online IDE\n- [repl.it](https://repl.it/)\n\n## Online practice\n- [codewars](http://www.codewars.com/)\n\n## Two Minutes Paper\n- [Two Minute Papers]()\n雷鋒網也有兩分鐘系列\n\n## Leiphone\n- [Video Collection](https://www.leiphone.com/news/201710/ipWGpR7iTGkvNhHL.html)\n\n## Scikit-Learn\n-[Data Camp](https://www.datacamp.com/community/tutorials/scikit-learn-python)\n## 尚未整理\nhttp://open.163.com/special/opencourse/daishu.html\n\nhttp://www.kaierlong.me/blog/post/kaierlong/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%80%E8%AF%BE%E6%B1%87%E6%80%BB\n\nCognitive Class\nhttps://cognitiveclass.ai/\nCognitive Class - IBM Big Data Learn\nhttps://cognitiveclass.ai/learn/big-data/\nCognitive Class - IBM Hadoop Foundations Learn\nhttps://cognitiveclass.ai/learn/hadoop/\nCognitive Class - IBM Data Science Foundations Learn\nhttps://cognitiveclass.ai/learn/data-science/\nCognitive Class - IBM Data Science for Business Learn\nhttps://cognitiveclass.ai/learn/data-science-business/\nCognitive Class - IBM Deep Learning Learn\nhttps://cognitiveclass.ai/learn/deep-learning/\n\n\nhttp://dlib.net/ml_guide.svg\n\nhttp://www.deepideas.net/deep-learning-from-scratch-theory-and-implementation/\n\nhttps://www.ctolib.com/\n\nhttp://blog.csdn.net/u011974639/article/details/73196349\n\nhttps://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#\n\n\n\nhttps://my.oschina.net/hardbone/blog/798552\nhttps://jhui.github.io/\n\n\nhttps://developers.google.cn/machine-learning/crash-course/ml-intro\n\n","tags":["Deep Learning","Self-Study","Courses"]},{"title":"GAN - Github List","url":"/2017/11/07/GAN-Github-List/","content":"\npaperList\nhttps://github.com/zhangqianhui/AdversarialNetsPapers\nhttps://github.com/hindupuravinash/the-gan-zoo\nhttps://github.com/nightrome/really-awesome-gan\nhttps://github.com/nashory/gans-awesome-applications\nhttps://github.com/GKalliatakis/Delving-deep-into-GANs\n\n====IMPLEMENTATION====\nhttps://github.com/YadiraF/GAN\nhttps://github.com/jonbruner/generative-adversarial-networks\nhttps://github.com/tjwei/GANotebooks\nhttps://github.com/AaronYALai/Generative_Adversarial_Networks_PyTorch\n\n最完整的GAN實作\nhttps://github.com/znxlwm/pytorch-generative-model-collections\nhttps://github.com/hwalsuklee/tensorflow-generative-model-collections\nhttps://github.com/eriklindernoren/Keras-GAN\n\n用貓玩GAN\nhttps://github.com/AlexiaJM/Deep-learning-with-cats\n\ntensorflow跟GAN都有\nhttps://github.com/wiseodd/generative-models\n\nhttps://github.com/jtoy/awesome-tensorflow\n\n","tags":["Generation","Generative Model","GAN"],"categories":["Github"]},{"title":"R-CNN:Region proposals+CNN","url":"/2017/11/06/R-CNN/","content":"# RCNN- 將CNN引入目標檢測的開山之作\n\n![](/media/15099664021450.png)\n\n[CS231n lecture8](http://cs231n.stanford.edu/slides/2016/winter1516_lecture8.pdf)\n\nRCNN (論文：Rich feature hierarchies for accurate object detection and semantic segmentation) 是將CNN方法引入目標檢測領域， 大大提高了目標檢測效果，可以說改變了目標檢測領域的主要研究思路， 緊隨其後的系列文章： （ RCNN ）, Fast RCNN , Faster RCNN 代表該領域當前最高水準。\n\n【論文主要特點】（相對傳統方法的改進）\n\n速度： 經典的目標檢測算法使用滑動窗法依次判斷所有可能的區域。 本文則(採用Selective Search方法)預先提取一系列較可能是物體的候選區域，之後僅在這些候選區域上(採用CNN)提取特徵，進行判斷。\n訓練集： 經典的目標檢測算法在區域中提取人工設定的特徵。 本文則採用深度網絡進行特徵提取。 使用兩個數據庫： 一個較大的識​​別庫（ImageNet ILSVC 2012）：標定每張圖片中物體的類別。 一千萬圖像，1000類。 一個較小的檢測庫（PASCAL VOC 2007）：標定每張圖片中，物體的類別和位置，一萬圖像，20類。 本文使用識別庫進行預訓練得到CNN（有監督預訓練），而後用檢測庫調優參數，最後在檢測庫上評測。\n看到這裡也許你已經對很多名詞很困惑，下面會解釋。 先來看看它的基本流程：\n\n# 基本流程\n![](/media/15099664799300.jpg)\n![](/media/15099707654238.jpg)\n\nRCNN算法分為4個步驟 \n\n候選區域生成： 一張圖像生成1K~2K個候選區域（採用Selective Search 方法） \n特徵提取： 對每個候選區域，使用深度卷積網絡提取特徵（CNN） \n類別判斷： 特徵送入每一類的SVM 分類器，判別是否屬於該類 \n位置精修： 使用回歸器精細修正候選框位置 \n\n\n【基礎知識===================================】\n\n### Selective Search \n主要思想:\n使用一種過分割手段，將圖像分割成小區域(1k~2k 個)\n查看現有小區域，按照合併規則合併可能性最高的相鄰兩個區域。 重複直到整張圖像合併成一個區域位置\n輸出所有曾經存在過的區域，所謂候選區域\n其中合併規則如下： 優先合併以下四種區域：\n\n顏色（顏色直方圖）相近的 \n紋理（梯度直方圖）相近的 \n合併後總面積小的： 保證合併操作的尺度較為均勻，避免一個大區域陸續“吃掉”其他小區域（例：設有區域abcdefgh。較好的合併方式是：ab-cd-ef-gh - > abcd-efgh -> abcdefgh。 不好的合併方法是：ab-cdefgh ->abcd-efgh ->abcdef-gh -> abcdefgh） \n合併後，總面積在其BBOX中所佔比例大的： 保證合併後形狀規則。\n![](/media/15099665196976.jpg)\n\n上述四條規則只涉及區域的顏色直方圖、梯度直方圖、面積和位置。 合併後的區域特徵可以直接由子區域特徵計算而來，速度較快。\n\n\n有監督預訓練與無監督預訓練: \n\n(1)無監督預訓練(Unsupervised pre-training)\n\n預訓練階段的樣本不需要人工標註數據，所以就叫做無監督預訓練。\n\n(2)有監督預訓練(Supervised pre-training)\n\n所謂的有監督預訓練也可以把它稱之為遷移學習。 比如你已經有一大堆標註好的人臉年齡分類的圖片數據，訓練了一個CNN，用於人臉的年齡識別。 然後當你遇到新的項目任務時：人臉性別識別，那麼這個時候你可以利用已經訓練好的年齡識別CNN模型，去掉最後一層，然後其它的網絡層參數就直接複製過來，繼續進行訓練，讓它輸出性別。 這就是所謂的遷移學習，說的簡單一點就是把一個任務訓練好的參數，拿到另外一個任務，作為神經網絡的初始參數值,這樣相比於你直接採用隨機初始化的方法，精度可以有很大的提高。 \n\n對於目標檢測問題： 圖片分類標註好的訓練數據非常多，但是物體檢測的標註數據卻很少，如何用少量的標註數據，訓練高質量的模型，這就是文獻最大的特點，這篇論文采用了遷移學習的思想：先用了ILSVRC2012這個訓練數據庫（這是一個圖片分類訓練數據庫），先進行網絡圖片分類訓練。 這個數據庫有大量的標註數據，共包含了1000種類別物體，因此預訓練階段CNN模型的輸出是1000個神經元（當然也直接可以採用Alexnet訓練好的模型參數）。\n\n### 重疊度（IOU）: \n\n物體檢測需要定位出物體的bounding box，就像下面的圖片一樣，我們不僅要定位出車輛的bounding box 我們還要識別出bounding box 裡面的物體就是車輛。 \n![](/media/15099665384173.jpg)\n\n對於bounding box的定位精度，有一個很重要的概念： 因為我們算法不可能百分百跟人工標註的數據完全匹配，因此就存在一個定位精度評價公式：IOU。 它定義了兩個bounding box的重疊度，如下圖所示 \n\n![](/media/15099665471055.jpg)\n\n就是矩形框A、B的重疊面積佔A、B並集的面積比例。\n\n\n### 非極大值抑制（ NMS ）：\n\nRCNN會從一張圖片中找出n個可能是物體的矩形框，然後為每個矩形框為做類別分類概率：\n![](/media/15099665602965.jpg)\n\n\n\n就像上面的圖片一樣，定位一個車輛，最後算法就找出了一堆的方框，我們需要判別哪些矩形框是沒用的。 非極大值抑制的方法是：先假設有6個矩形框，根據分類器的類別分類概率做排序，假設從小到大屬於車輛的概率分別為A、B、C、D、E、F。\n\n(1)從最大概率矩形框F開始，分別判斷A~E與F的重疊度IOU是否大於某個設定的閾值;\n\n(2)假設B、D與F的重疊度超過閾值，那麼就扔掉B、D；並標記第一個矩形框F，是我們保留下來的。\n\n(3)從剩下的矩形框A、C、E中，選擇概率最大的E，然後判斷E與A、C的重疊度，重疊度大於一定的閾值，那麼就扔掉；並標記E是我們保留下來的第二個矩形框。\n\n就這樣一直重複，找到所有被保留下來的矩形框。\n\n非極大值抑制（NMS）顧名思義就是抑制不是極大值的元素，搜索局部的極大值。 這個局部代表的是一個鄰域，鄰域有兩個參數可變，一是鄰域的維數，二是鄰域的大小。 這裡不討論通用的NMS算法，而是用於在目標檢測中用於提取分數最高的窗口的。 例如在行人檢測中，滑動窗口經提取特徵，經分類器分類識別後，每個窗口都會得到一個分數。 但是滑動窗口會導致很多窗口與其他窗口存在包含或者大部分交叉的情況。 這時就需要用到NMS來選取那些鄰域里分數最高（是行人的概率最大），並且抑制那些分數低的窗口。\n\n###VOC物體檢測任務:\n\n相當於一個競賽，裡麵包含了20個物體類別： PASCAL VOC2011 Example Images 還有一個背景，總共就相當於21個類別，因此一會設計fine-tuning CNN的時候，我們softmax分類輸出層為21個神經元。\n\n【各個階段詳解===================================】 \n\n總體思路再回顧：\n\n首先對每一個輸入的圖片產生近2000個不分種類的候選區域（region proposals），然後使用CNNs從每個候選框中提取一個固定長度的特徵向量（4096維度），接著對每個取出的特徵向量使用特定種類的線性SVM進行分類。 也就是總個過程分為三個程序：a、找出候選框；b、利用CNN提取特徵向量；c、利用SVM進行特徵向量分類。\n\n![](/media/15099666382907.jpg)\n\n\n#### 候選框搜索階段：\n\n當我們輸入一張圖片時，我們要搜索出所有可能是物體的區域，這裡採用的就是前面提到的Selective Search方法，通過這個算法我們搜索出2000個候選框。 然後從上面的總流程圖中可以看到，搜出的候選框是矩形的，而且是大小各不相同。 然而CNN對輸入圖片的大小是有固定的，如果把搜索到的矩形選框不做處理，就扔進CNN中，肯定不行。 因此對於每個輸入的候選框都需要縮放到固定的大小。 下面我們講解要怎麼進行縮放處理，為了簡單起見我們假設下一階段CNN所需要的輸入圖片大小是個正方形圖片227*227。 因為我們經過selective search 得到的是矩形框，paper試驗了兩種不同的處理方法：\n\n(1)各向異性縮放\n\n這種方法很簡單，就是不管圖片的長寬比例，管它是否扭曲，進行縮放就是了，全部縮放到CNN輸入的大小227*227，如下圖(D)所示；\n![](/media/15099666636066.jpg)\n\n\n(2)各向同性縮放\n\n因為圖片扭曲後，估計會對後續CNN的訓練精度有影響，於是作者也測試了“各向同性縮放”方案。 有兩種辦法\n\nA、先擴充後裁剪： 直接在原始圖片中，把bounding box的邊界進行擴展延伸成正方形，然後再進行裁剪；如果已經延伸到了原始圖片的外邊界，那麼就用bounding box中的顏色均值填充；如上圖(B)所示;\n\nB、先裁剪後擴充：先把bounding box圖片裁剪出來，然後用固定的背景顏色填充成正方形圖片(背景顏色也是採用bounding box的像素顏色均值),如上圖(C)所示;\n\n對於上面的異性、同性縮放，文獻還有個padding處理，上面的示意圖中第1、3行就是結合了padding=0,第2、4行結果圖採用padding=16的結果。 經過最後的試驗，作者發現採用各向異性縮放、padding=16的精度最高。\n\n（備註：候選框的搜索策略作者也考慮過使用一個滑動窗口的方法，然而由於更深的網絡，更大的輸入圖片和滑動步長，使得使用滑動窗口來定位的方法充滿了挑戰。） \n\nCNN特徵提取階段： \n\n1、算法實現\n\na、網絡結構設計階段\n![](/media/15099666918238.jpg)\n\n![](/media/15099666873410.jpg)\n\n網絡架構兩個可選方案：第一選擇經典的Alexnet；第二選擇VGG16。 經過測試Alexnet精度為58.5%，VGG16精度為66%。 VGG這個模型的特點是選擇比較小的捲積核、選擇較小的跨步，這個網絡的精度高，不過計算量是Alexnet的7倍。 後面為了簡單起見，我們就直接選用Alexnet，並進行講解；Alexnet特徵提取部分包含了5個卷積層、2個全連接層，在Alexnet中p5層神經元個數為9216、 f6、f7的神經元個數都是4096，通過這個網絡訓練完畢後，最後提取特徵每個輸入候選框圖片都能得到一個4096維的特徵向量。\n\n\nb、網絡有監督預訓練階段（圖片數據庫：ImageNet ILSVC ）\n![](/media/15099667042004.jpg)\n\n參數初始化部分：物體檢測的一個難點在於，物體標籤訓練數據少，如果要直接採用隨機初始化CNN參數的方法，那麼目前的訓練數據量是遠遠不夠的。 這種情況下，最好的是採用某些方法，把參數初始化了，然後在進行有監督的參數微調，這里文獻採用的是有監督的預訓練。 所以paper在設計網絡結構的時候，是直接用Alexnet的網絡，然後連參數也是直接採用它的參數，作為初始的參數值，然後再fine-tuning訓練。 網絡優化求解時採用隨機梯度下降法，學習率大小為0.001；\n\n\nc、fine-tuning階段（圖片數據庫： PASCAL VOC）\n\n我們接著採用selective search 搜索出來的候選框（PASCAL VOC 數據庫中的圖片） 繼續對上面預訓練的CNN模型進行fine-tuning訓練。 假設要檢測的物體類別有N類，那麼我們就需要把上面預訓練階段的CNN模型的最後一層給替換掉，替換成N+1個輸出的神經元(加1，表示還有一個背景) (20 + 1bg = 21)，然後這一層直接採用參數隨機初始化的方法，其它網絡層的參數不變；接著就可以開始繼續SGD訓練了。 開始的時候，SGD學習率選擇0.001，在每次訓練的時候，我們batch size大小選擇128，其中32個事正樣本、96個事負樣本。\n![](/media/15099667194897.jpg)\n\n\n\n關於正負樣本問題：\n\n一張照片我們得到了2000個候選框。 然而人工標註的數據一張圖片中就只標註了正確的bounding box，我們搜索出來的2000個矩形框也不可能會出現一個與人工標註完全匹配的候選框。 因此在CNN階段我們需要用IOU為2000個bounding box打標籤。 如果用selective search挑選出來的候選框與物體的人工標註矩形框（PASCAL VOC的圖片都有人工標註）的重疊區域IoU大於0.5，那麼我們就把這個候選框標註成物體類別（正樣本），否則我們就把它當做背景類別（負樣本）。\n\n\n\n（備註： 如果不針對特定任務進行fine-tuning，而是把CNN當做特徵提取器，卷積層所學到的特徵其實就是基礎的共享特徵提取層，就類似於SIFT算法一樣，可以用於提取各種圖片的特徵，而f6、f7所學習到的特徵是用於針對特定任務的特徵。打個比方：對於人臉性別識別來說，一個CNN模型前面的捲積層所學習到的特徵就類似於學習人臉共性特徵，然後全連接層所學習的特徵就是針對性別分類的特徵了）\n\n\n\n2.疑惑點 ： CNN訓練的時候，本來就是對bounding box的物體進行識別分類訓練，在訓練的時候最後一層softmax就是分類層。 那麼為什麼作者閒著沒事幹要先用CNN做特徵提取（提取fc7層數據），然後再把提取的特徵用於訓練svm分類器？ \n\n這個是因為svm訓練和cnn訓練過程的正負樣本定義方式各有不同，導致最後採用CNN softmax輸出比採用svm精度還低。 事情是這樣的，cnn在訓練的時候，對訓練數據做了比較寬鬆的標註，比如一個bounding box可能只包含物體的一部分，那麼我也把它標註為正樣本，用於訓練cnn；採用這個方法的主要原因在於因為CNN容易過擬合，所以需要大量的訓練數據，所以在CNN訓練階段我們是對Bounding box的位置限制條件限制的比較鬆(IOU只要大於0.5都被標註為正樣本了)；然而svm訓練的時候，因為svm適用於少樣本訓練，所以對於訓練樣本數據的IOU要求比較嚴格，我們只有當bounding box把整個物體都包含進去了，我們才把它標註為物體類別，然後訓練svm ，具體請看下文。\n\nSVM訓練、測試階段\n\n訓練階段 ：\n\n這是一個二分類問題，我麼假設我們要檢測車輛。 我們知道只有當bounding box把整量車都包含在內，那才叫正樣本；如果bounding box 沒有包含到車輛，那麼我們就可以把它當做負樣本。 但問題是當我們的檢測窗口只有部分包含物體，那該怎麼定義正負樣本呢？ 作者測試了IOU閾值各種方案數值0,0.1,0.2,0.3,0.4,0.5。 最後通過訓練發現，如果選擇IOU閾值為0.3 效果最好 （選擇為0精度下降了4個百分點，選擇0.5精度下降了5個百分點）,即當重疊度小於0.3的時候，我們就把它標註為負樣本。 一旦CNN f7層特徵被提取出來，那麼我們將為每個物體類訓練一個svm分類器。 當我們用CNN提取2000個候選框，可以得到2000x4096這樣的特徵向量矩陣，然後我們只需要把這樣的一個矩陣與svm權值矩陣4096xN點乘(N為分類類別數目，因為我們訓練的N個svm，每個svm包含了4096個權值w)，就可以得到結果了。 \n\n![](/media/15099667603349.jpg)\n\n得到的特徵輸入到SVM進行分類看看這個feature vector所對應的region proposal是需要的物體還是無關的實物(background) 。 排序，canny邊界檢測之後就得到了我們需要的bounding-box。 \n\n再回顧總結一下：整個系統分為三個部分：1.產生不依賴與特定類別的region proposals，這些region proposals定義了一個整個檢測器可以獲得的候選目標2.一個大的捲積神經網絡，對每個region產生一個固定長度的特徵向量3.一系列特定類別的線性SVM分類器。\n\n\n位置精修： 目標檢測問題的衡量標準是重疊面積：許多看似準確的檢測結果，往往因為候選框不夠準確，重疊面積很小。 故需要一個位置精修步驟。 回歸器：對每一類目標，使用一個線性脊回歸器進行精修。 正則項λ=10000。 輸入為深度網絡pool5層的4096維特徵，輸出為xy方向的縮放和平移。 訓練樣本：判定為本類的候選框中和真值重疊面積大於0.6的候選框。 \n\n測試階段 ：\n\n使用selective search的方法在測試圖片上提取2000個region propasals ，將每個region proposals歸一化到227x227，然後再CNN中正向傳播，將最後一層得到的特徵提取出來。 然後對於每一個類別，使用為這一類訓練的SVM分類器對提取的特徵向量進行打分，得到測試圖片中對於所有region proposals的對於這一類的分數，再使用貪心的非極大值抑制（ NMS）去除相交的多餘的框。 再對這些框進行canny邊緣檢測，就可以得到bounding-box(then B-BoxRegression)。\n\n（非極大值抑制（NMS）先計算出每一個bounding box的面積，然後根據score進行排序，把score最大的bounding box作為選定的框，計算其餘bounding box與當前最大score與box的IoU，去除IoU大於設定的閾值的bounding box。然後重複上面的過程，直至候選bounding box為空，然後再將score小於一定閾值的選定框刪除得到這一類的結果（然後繼續進行下一個分類） 。作者提到花費在region propasals和提取特徵的時間是13s/張-GPU和53s/張-CPU，可以看出時間還是很長的，不能夠達到及時性。 \n\n完。\n\n本文主要整理自以下文章：\n\nRCNN學習筆記(0):rcnn簡介 \nRCNN學習筆記(1):Rich feature hierarchies for accurate object detection and semantic segmentation \nRCNN學習筆記(2):Rich feature hierarchies for accurate object detection and semantic segmentation \n《Rich feature hierarchies for Accurate Object Detection and Segmentation》\n《Spatial 《Pyramid Pooling in Deep Convolutional Networks for Visual Recognition》\n\n","tags":["Deep Learning","R-CNN","CNN","Object Detection"],"categories":["Detection"]},{"title":"Object Detection with Convolution Neural Network Series","url":"/2017/11/06/Object-Detection-with-Convolution-Neural-Network/","content":"# Object Detection with Convolution Neural Network Series\n\nOutline\n- 檢測任務?\n- R-CNN\n- Fast R-CNN\n- Faster R-CNN\n- SSD\n- YOLO\n\n## 檢測任務\n![](/media/15100388874028.png)\n\n## R-CNN\n<center>![](/media/15099707654238.jpg)</center>\n\n步驟 \n1. 候選區域生成： 一張圖像生成1K~2K個候選區域（採用Selective Search 方法） \n2. 特徵提取： 對每個候選區域，使用深度卷積網絡提取特徵（CNN） \n3. 類別判斷： 特徵送入每一類的SVM 分類器，判別是否屬於該類 \n4. 位置精修： 使用回歸器精細修正候選框位置 \n5. 用Non-Maximum Selection 合併後選框\n\n\n## Fast R-CNN\n<center>![](/media/15100371122830.jpg)</center>\n步驟 \n1. 候選區域生成： 一張圖像生成1K~2K個候選區域（採用Selective Search 方法） \n2. 特徵提取： 該張圖片，使用深度卷積網絡提取特徵（CNN）\n3. ROI Pooling： \n4. 類別判斷： 特徵送入每一類的SVM 分類器，判別是否屬於該類 \n5. 位置精修： 使用回歸器精細修正候選框位置 \n6. 用Non-Maximum Selection 合併後選框\n\n## Faster R-CNN\n<center>![](/media/15100371249624.jpg)</center>\n\n## SSD\n<center>![](/media/15100374296902.jpg)\n</center>\n\n## YOLO\n![](/media/15144441218022.jpg)\n\n\n\n\n\n\nReference:\n1. http://zh.gluon.ai/\n\n\n","tags":["Deep Learning","R-CNN","CNN","Object Detection"],"categories":["Detection"]},{"title":"CPP 筆記","url":"/2017/10/19/CPP-note/","content":"\n##C++中cin、cin.get()、cin.getline()、getline()、gets()等函数的用法\n学C++的时候，这几个输入函数弄的有点迷糊；这里做个小结，为了自己复习，也希望对后来者能有所帮助，如果有差错的地方还请各位多多指教\n\n1、cin \n2、cin.get() \n3、cin.getline() \n4、getline() \n5、gets() \n6、getchar()\n\n附:cin.ignore();cin.get()//跳过一个字符,例如不想要的回车,空格等字符\n\n1、cin>>         \n\n用法1：最基本，也是最常用的用法，输入一个数字：\n\n````cpp\n#include <iostream> \nusing namespace std; \nmain () \n{ \nint a,b; \ncin>>a>>b; \ncout<<a+b<<endl; \n}\n````\n输入：2[回车]3[回车] \n输出：5\n\n注意:>> 是会过滤掉不可见字符（如 空格 回车，TAB 等） \ncin>>noskipws>>input[j];//不想略过空白字符，那就使用 noskipws 流控制\n\n用法2：接受一个字符串，遇“空格”、“TAB”、“回车”都结束\n\n````cpp\n#include <iostream> \nusing namespace std; \nmain () \n{ \nchar a[20]; \ncin>>a; \ncout<<a<<endl; \n}\n````\n输入：jkljkljkl \n输出：jkljkljkl\n\n输入：jkljkl jkljkl       //遇空格结束 \n输出：jkljkl\n\n2、cin.get()\n\n用法1： cin.get(字符变量名)可以用来接收字符\n\n````cpp\n#include <iostream> \nusing namespace std; \nmain () \n{ \nchar ch; \nch=cin.get();               //或者cin.get(ch); \ncout<<ch<<endl; \n}\n````\n输入：jljkljkl \n输出：j\n\n用法2：cin.get(字符数组名,接收字符数目)用来接收一行字符串,可以接收空格\n\n````cpp\n#include <iostream> \nusing namespace std; \nmain () \n{ \nchar a[20]; \ncin.get(a,20); \ncout<<a<<endl; \n}\n````\n输入：jkl jkl jkl \n输出：jkl jkl jkl\n\n输入：abcdeabcdeabcdeabcdeabcde （输入25个字符） \n输出：abcdeabcdeabcdeabcd              （接收19个字符+1个'\\0'）\n\n用法3：cin.get(无参数)没有参数主要是用于舍弃输入流中的不需要的字符,或者舍弃回车,弥补cin.get(字符数组名,接收字符数目)的不足.\n\n这个我还不知道怎么用，知道的前辈请赐教；\n\n3、cin.getline()   // 接受一个字符串，可以接收空格并输出\n\n````cpp\n#include <iostream> \nusing namespace std; \nmain () \n{ \nchar m[20]; \ncin.getline(m,5); \ncout<<m<<endl; \n}\n````\n\n输入：jkljkljkl \n输出：jklj\n\n接受5个字符到m中，其中最后一个为'\\0'，所以只看到4个字符输出；\n\n如果把5改成20： \n输入：jkljkljkl \n输出：jkljkljkl\n\n输入：jklf fjlsjf fjsdklf \n输出：jklf fjlsjf fjsdklf\n\n//延伸： \n//cin.getline()实际上有三个参数，cin.getline(接受字符串的看哦那间m,接受个数5,结束字符) \n//当第三个参数省略时，系统默认为'\\0' \n//如果将例子中cin.getline()改为cin.getline(m,5,'a');当输入jlkjkljkl时输出jklj，输入jkaljkljkl时，输出jk\n\n当用在多维数组中的时候，也可以用cin.getline(m[i],20)之类的用法：\n\n````cpp\n#include<iostream> \n#include<string> \nusing namespace std;\n\nmain () \n{ \nchar m[3][20]; \nfor(int i=0;i<3;i++) \n{ \ncout<<\"\\n请输入第\"<<i+1<<\"个字符串：\"<<endl; \ncin.getline(m[i],20); \n}\n\ncout<<endl; \nfor(int j=0;j<3;j++) \ncout<<\"输出m[\"<<j<<\"]的值:\"<<m[j]<<endl;\n\n}\n````\n请输入第1个字符串： \nkskr1\n\n请输入第2个字符串： \nkskr2\n\n请输入第3个字符串： \nkskr3\n\n输出m[0]的值:kskr1 \n输出m[1]的值:kskr2 \n输出m[2]的值:kskr3\n\n4、getline()     // 接受一个字符串，可以接收空格并输出，需包含“#include<string>”\n\n````cpp\n#include<iostream> \n#include<string> \nusing namespace std; \nmain () \n{ \nstring str; \ngetline(cin,str); \ncout<<str<<endl; \n}\n````\n输入：jkljkljkl \n输出：jkljkljkl\n\n输入：jkl jfksldfj jklsjfl \n输出：jkl jfksldfj jklsjfl\n\n和cin.getline()类似，但是cin.getline()属于istream流，而getline()属于string流，是不一样的两个函数\n\n\n\n\n在寫程式的時候遇到這個問題\n因為atoi函式只吃char*\n上網GOOGLE一下找到了解決之法\n\n````cpp\nstring x;\nint temp=atoi(x.c_str());\nc_str()可以轉換string成為char*\n````\n\n````cpp\n#include <iostream>\nusing namespace std ;\n \nint main()\n{\n    string s ;\n    int a;\n    while (getline(cin,s))\n    {\n        int n=s.length();\n        for(int i=0;i<n;i++) \n        {\n            a=s.at(i);\n            cout << a << \" \";\n         }\n         cout << endl;\n    }\n    return 0 ;\n}\n````\n\n\n","tags":["code","CPP","C++"],"categories":["Code"]},{"title":"Hello World","url":"/2017/03/28/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n"}]