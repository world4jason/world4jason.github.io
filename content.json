[{"title":"Mask RCNNCode Reading - Detection Layer","date":"2018-06-03T21:57:33.000Z","path":"2018/06/04/Mask-RCNN-Code-Reading-DetectionLayer/","text":"這邊其實主要是因為程式碼撰寫方便 所以才拆成Detection Target Layer與Detection Layer 前者是在訓練時用到的,後者是在測試時用的, Detection Layer refine_detections_graph根據rois和probs(每個ROI都有總類別個數的probs)和deltas進行檢測的優化，得到固定數量的優化目標。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798def refine_detections_graph(rois, probs, deltas, window, config): \"\"\"Refine classified proposals and filter overlaps and return final detections. # 輸入為N個rois、N個具有num_classes的probs，scores由probs得出 Inputs: rois: [N, (y1, x1, y2, x2)] in normalized coordinates probs: [N, num_classes]. Class probabilities. deltas: [N, num_classes, (dy, dx, log(dh), log(dw))]. Class-specific bounding box deltas. window: (y1, x1, y2, x2) in image coordinates. The part of the image that contains the image excluding the padding. Returns detections shaped: [N, (y1, x1, y2, x2, class_id, score)] where coordinates are normalized. \"\"\" # Class IDs per ROI class_ids = tf.argmax(probs, axis=1, output_type=tf.int32) # Class probability of the top class of each ROI indices = tf.stack([tf.range(probs.shape[0]), class_ids], axis=1) class_scores = tf.gather_nd(probs, indices) # Class-specific bounding box deltas deltas_specific = tf.gather_nd(deltas, indices) # Apply bounding box deltas # Shape: [boxes, (y1, x1, y2, x2)] in normalized coordinates refined_rois = apply_box_deltas_graph( rois, deltas_specific * config.BBOX_STD_DEV) # Clip boxes to image window refined_rois = clip_boxes_graph(refined_rois, window) # TODO: Filter out boxes with zero area # Filter out background boxes keep = tf.where(class_ids &gt; 0)[:, 0] # Filter out low confidence boxes if config.DETECTION_MIN_CONFIDENCE: conf_keep = tf.where(class_scores &gt;= config.DETECTION_MIN_CONFIDENCE)[:, 0] keep = tf.sets.set_intersection(tf.expand_dims(keep, 0), tf.expand_dims(conf_keep, 0)) keep = tf.sparse_tensor_to_dense(keep)[0] # 留下既滿足是前景又滿足scores大於MIN_CONFIDENCE的 # Apply per-class NMS # 1. Prepare variables pre_nms_class_ids = tf.gather(class_ids, keep) pre_nms_scores = tf.gather(class_scores, keep) pre_nms_rois = tf.gather(refined_rois, keep) unique_pre_nms_class_ids = tf.unique(pre_nms_class_ids)[0] def nms_keep_map(class_id): \"\"\"Apply Non-Maximum Suppression on ROIs of the given class.\"\"\" # Indices of ROIs of the given class ixs = tf.where(tf.equal(pre_nms_class_ids, class_id))[:, 0] # Apply NMS class_keep = tf.image.non_max_suppression( tf.gather(pre_nms_rois, ixs), tf.gather(pre_nms_scores, ixs), max_output_size=config.DETECTION_MAX_INSTANCES, iou_threshold=config.DETECTION_NMS_THRESHOLD) # Map indicies class_keep = tf.gather(keep, tf.gather(ixs, class_keep)) # Pad with -1 so returned tensors have the same shape gap = config.DETECTION_MAX_INSTANCES - tf.shape(class_keep)[0] class_keep = tf.pad(class_keep, [(0, gap)], mode='CONSTANT', constant_values=-1) # Set shape so map_fn() can infer result shape class_keep.set_shape([config.DETECTION_MAX_INSTANCES]) return class_keep # 2. Map over class IDs # 在nms_keep_map內分類別的進行NMS。 nms_keep = tf.map_fn(nms_keep_map, unique_pre_nms_class_ids, dtype=tf.int64) # 3. Merge results into one list, and remove -1 padding nms_keep = tf.reshape(nms_keep, [-1]) nms_keep = tf.gather(nms_keep, tf.where(nms_keep &gt; -1)[:, 0]) # 4. Compute intersection between keep and nms_keep keep = tf.sets.set_intersection(tf.expand_dims(keep, 0), tf.expand_dims(nms_keep, 0)) keep = tf.sparse_tensor_to_dense(keep)[0] # Keep top detections roi_count = config.DETECTION_MAX_INSTANCES class_scores_keep = tf.gather(class_scores, keep) num_keep = tf.minimum(tf.shape(class_scores_keep)[0], roi_count) top_ids = tf.nn.top_k(class_scores_keep, k=num_keep, sorted=True)[1] keep = tf.gather(keep, top_ids) # Arrange output as [N, (y1, x1, y2, x2, class_id, score)] # Coordinates are normalized. detections = tf.concat([ tf.gather(refined_rois, keep), tf.to_float(tf.gather(class_ids, keep))[..., tf.newaxis], tf.gather(class_scores, keep)[..., tf.newaxis] ], axis=1) # Pad with zeros if detections &lt; DETECTION_MAX_INSTANCES gap = config.DETECTION_MAX_INSTANCES - tf.shape(detections)[0] detections = tf.pad(detections, [(0, gap), (0, 0)], \"CONSTANT\") return detections 123456789101112131415161718192021222324252627282930313233343536373839404142class DetectionLayer(KE.Layer): \"\"\"Takes classified proposal boxes and their bounding box deltas and returns the final detection boxes. Returns: [batch, num_detections, (y1, x1, y2, x2, class_id, class_score)] where coordinates are normalized. \"\"\" def __init__(self, config=None, **kwargs): super(DetectionLayer, self).__init__(**kwargs) self.config = config def call(self, inputs): rois = inputs[0] mrcnn_class = inputs[1] mrcnn_bbox = inputs[2] image_meta = inputs[3] # Get windows of images in normalized coordinates. Windows are the area # in the image that excludes the padding. # Use the shape of the first image in the batch to normalize the window # because we know that all images get resized to the same size. m = parse_image_meta_graph(image_meta) image_shape = m['image_shape'][0] window = norm_boxes_graph(m['window'], image_shape[:2]) # Run detection refinement graph on each item in the batch detections_batch = utils.batch_slice( [rois, mrcnn_class, mrcnn_bbox, window], lambda x, y, w, z: refine_detections_graph(x, y, w, z, self.config), self.config.IMAGES_PER_GPU) # Reshape output # [batch, num_detections, (y1, x1, y2, x2, class_score)] in # normalized coordinates return tf.reshape( detections_batch, [self.config.BATCH_SIZE, self.config.DETECTION_MAX_INSTANCES, 6]) def compute_output_shape(self, input_shape): return (None, self.config.DETECTION_MAX_INSTANCES, 6)","tags":[]},{"title":"Batch Normalization and Group Normalization","date":"2018-06-03T20:30:09.000Z","path":"2018/06/04/Batch-Normalization-and-Group-Normalization/","text":"#Batch NormalizationBatch Normalization 在深度學習上算是不可或缺的一部分，基本上所有的框架中都會用到它，我記得比較清楚的是，在YOLOV2中作者採用了Batch Normalization 從而提高了4個百分點的Map吧。 ##為何要提出Batch Normalization？在每次給network輸入數據時，都需要進行預處理，比如歸一化之類的，為什麼需要歸一化呢？神經網絡學習過程本質就是為了學習數據分佈，一旦訓練數據與測試數據的分佈不同，那麼網絡的泛化能力也大大降低；另外一方面，一旦每批訓練數據的分佈各不相同(batch 梯度下降)，那麼網絡就要在每次迭代都去學習適應不同的分佈，這樣將會大大降低網絡的訓練速度，這也正是為什麼我們需要對數據都要做一個歸一化預處理的原因。 而且在訓練的過程中，經過一層層的網絡運算，中間層的學習到的數據分佈也是發生著挺大的變化，這就要求我們必須使用一個很小的學習率和對參數很好的初始化，但是這麼做會讓訓練過程變得慢而且複雜m在論文中，這種現象被稱為Internal Covariate Shift。為瞭解決這個問題，作者提出了Batch Normalization。 ##Batch Normalization原理為了降低Internal Covariate Shift帶來的影響，其實只要進行歸一化就可以的。比如，我們把network每一層的輸出都整為方差為1，均值為0的正態分佈，這樣看起來是可以解決問題，但是想想，network好不容易學習到的數據特徵，被你這樣一弄又回到瞭解放前了，相當於沒有學習了。所以這樣是不行的，大神想到了一個大招：變換重構，引入了兩個可以學習的參數γ、β，當然，這也是算法的靈魂所在： 具體的算法流程如下： Batch Normalization 是對一個batch來進行normalization的，例如我們的輸入的一個batch為：β=x_(1…m)，輸出為：y_i=BN(x)。具體的完整流程如下： 1.求出該batch數據x的均值 2.求出該batch數據的方差 3.對輸入數據x做歸一化處理，得到： 4.最後加入可訓練的兩個參數：縮放變量γ和平移變量β，計算歸一化後的值： 加入了這兩個參數之後，網絡就可以更加容易的學習到更多的東西了。先想想極端的情況，當縮放變量γ和平移變量β分別等於batch數據的方差和均值時，最後得到的yi就和原來的xi一模一樣了，相當於batch normalization沒有起作用了。這樣就保證了每一次數據經過歸一化後還保留的有學習來的特徵，同時又能完成歸一化這個操作，加速訓練。 引入參數的更新過程，也就是微積分的Chain Rule： Example 12345678910111213141516171819202122def Batchnorm_simple_for_train(x, gamma,beta, bn_param):\"\"\" param:x : 輸入數據，設shape(B,L) param:gama : 縮放因子 γ param:beta : 平移因子 β param:bn_param : batchnorm所需要的一些參數 eps : 接近0的數，防止分母出現0 momentum : 動量參數，一般為0.9，0.99， 0.999 running_mean ：滑動平均的方式計算新的均值，訓練時計算，為測試數據做準備 running_var : 滑動平均的方式計算新的方差，訓練時計算，為測試數據做準備 \"\"\" running_mean = bn_param['running_mean'] #shape = [B] running_var = bn_param['running_var'] #shape = [B] results = 0. # 建立一個新的變量 x_mean=x.mean(axis=0) # 計算x的均值 x_var=x.var(axis=0) # 計算方差 x_normalized=(x-x_mean)/np.sqrt(x_var+eps) # 歸一化 results = gamma * x_normalized + beta # 縮放平移 running_mean = momentum * running_mean + (1 - momentum) * x_mean running_var = momentum * running_var + (1 - momentum) * x_var #記錄新的值 bn_param['running_mean'] = running_mean bn_param['running_var'] = running_var return results , bn_param 這份code首先計算均值和方差，然後歸一化，然後縮放和平移就結束了！但是這是在訓練中完成的任務，每次訓練給一個批量，然後計算批量的均值方差，但是在測試的時候可不是這樣，測試的時候每次只輸入一張圖片，這怎麼計算批量的均值和方差，於是，就有了代碼中下面兩行，在訓練的時候實現計算好mean var測試的時候直接拿來用就可以了，不用計算均值和方差。 12running_mean = momentum * running_mean + (1- momentum) * x_mean running_var = momentum * running_var + (1 -momentum) * x_var 所以，測試的時候是這樣的： 12345678910111213141516def Batchnorm_simple_for_test(x, gamma,beta, bn_param):\"\"\" param:x : 輸入數據，設shape(B,L) param:gama : 縮放因子 γ param:beta : 平移因子 β param:bn_param : batchnorm所需要的一些參數 eps : 接近0的數，防止分母出現0 momentum : 動量參數，一般為0.9，0.99， 0.999 running_mean ：滑動平均的方式計算新的均值，訓練時計算，為測試數據做準備 running_var : 滑動平均的方式計算新的方差，訓練時計算，為測試數據做準備 \"\"\" running_mean = bn_param['running_mean'] #shape = [B] running_var = bn_param['running_var'] #shape = [B] results = 0. # 建立一個新的變量 x_normalized=(x-running_mean )/np.sqrt(running_var +eps) # 歸一化 results = gamma * x_normalized + beta # 縮放平移 return results , bn_param 整個過程還是很順的，很好理解的。這部分的內容摘抄自微信公眾號：機器學習算法工程師。一個很好的公眾號，推薦一波。 Batch Normalization 的TensorFlow 源碼解讀，來自知乎： 123456789101112131415161718192021def batch_norm_layer(x, train_phase,scope_bn): with tf.variable_scope(scope_bn): # 新建兩個變量，平移、縮放因子 beta = tf.Variable(tf.constant(0.0, shape=[x.shape[-1]]), name='beta',trainable=True) gamma = tf.Variable(tf.constant(1.0, shape=[x.shape[-1]]), name='gamma',trainable=True) # 計算此次批量的均值和方差 axises = np.arange(len(x.shape) - 1) batch_mean, batch_var = tf.nn.moments(x, axises, name='moments') # 滑動平均做衰減 ema = tf.train.ExponentialMovingAverage(decay=0.5) def mean_var_with_update(): ema_apply_op = ema.apply([batch_mean, batch_var]) with tf.control_dependencies([ema_apply_op]): return tf.identity(batch_mean),tf.identity(batch_var) # train_phase 訓練還是測試的flag # 訓練階段計算runing_mean和runing_var，使用mean_var_with_update（）函數 # 測試的時候直接把之前計算的拿去用 ema.average(batch_mean) mean, var = tf.cond(train_phase, mean_var_with_update, lambda:(ema.average(batch_mean), ema.average(batch_var))) normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3) return normed 至於此行代碼tf.nn.batch_normalization()就是簡單的計算batchnorm過程，這個函數所實現的功能就如此公式： 1234567def batch_normalization(x, mean, variance, offset,scale, variance_epsilon, name=None): with ops.name_scope(name, \"batchnorm\", [x, mean, variance,scale, offset]): inv = math_ops.rsqrt(variance + variance_epsilon) if scale is not None: inv *= scale return x * inv + (offset - mean * inv if offset is not Noneelse -mean * inv) ##Batch Normalization的帶來的優勢： 沒有它之前，需要小心的調整學習率和權重初始化，但是有了BN可以放心的使用大學習率，但是使用了BN，就不用小心的調參了，較大的學習率極大的提高了學習速度， Batchnorm本身上也是一種正則的方式，可以代替其他正則方式如dropout等 另外，個人認為，batchnorm降低了數據之間的絕對差異，有一個去相關的性質，更多的考慮相對差異性，因此在分類任務上具有更好的效果 #Group Normalizationgroup normalization是2018年3月份何愷明大神的又一力作，優化了batch normalization在比較小的batch size 情況下表現不太好的劣勢。批量維度進行歸一化會帶來一些問題——批量統計估算不準確導致批量變小時，BN 的誤差會迅速增加。在訓練大型網絡和將特徵轉移到計算機視覺任務中（包括檢測、分割和視頻），內存消耗限制了只能使用小批量的BN。尤其是在我的破電腦裡面，batch的大小一般都是使用的1，相當於不存在BN。 下圖是論文中給出BN和GN的對比： 可以看出在bath size比較小的情況下，BN的性能十分地差，而GN的性能基本上沒有太大改變。 ##Group Normalization 原理：先給出他目前出現比較多的幾種normalization的示意圖： BatchNorm：batch方向做歸一化，算NHW的均值 LayerNorm：channel方向做歸一化，算CHW的均值 InstanceNorm：一個channel內做歸一化，算H*W的均值 GroupNorm：將channel方向分group，然後每個group內做歸一化，算(C//G)HW的均值 從示意圖中看，也可以看出其實沒有太大的變化，所以代碼中也沒有需要太大的變動，只需要稍微修改一下就好了。 GN程式碼範例： 123456789def GroupNorm(x,G=16,eps=1e-5): N,H,W,C=x.shape x=tf.reshape(x,[tf.cast(N,tf.int32),tf.cast(H,tf.int32),tf.cast(W,tf.int32),tf.cast(G,tf.int32),tf.cast(C//G,tf.int32)]) mean,var=tf.nn.moments(x,[1,2,4],keep_dims=True) x=(x-mean)/tf.sqrt(var+eps) x=tf.reshape(x,[tf.cast(N,tf.int32),tf.cast(H,tf.int32),tf.cast(W,tf.int32),tf.cast(C,tf.int32)]) gamma = tf.Variable(tf.ones(shape=[1,1,1,tf.cast(C,tf.int32)]), name=\"gamma\") beta = tf.Variable(tf.zeros(shape=[1,1,1,tf.cast(C,tf.int32)]), name=\"beta\") return x*gamma+beta Group Normalization in Keras其實也是在keras中的BatchNormalization層上進行一定的修改就得到了GroupNormalization層。正常和batchnormalization一樣的調用即可。但注意需要保持channel數是group的整數倍。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187from keras.engine import Layer, InputSpecfrom keras import initializersfrom keras import regularizersfrom keras import constraintsfrom keras import backend as Kfrom keras.utils.generic_utils import get_custom_objectsclass GroupNormalization(Layer): \"\"\"Group normalization layer Group Normalization divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes # Arguments groups: Integer, the number of groups for Group Normalization. axis: Integer, the axis that should be normalized (typically the features axis). For instance, after a `Conv2D` layer with `data_format=\"channels_first\"`, set `axis=1` in `BatchNormalization`. epsilon: Small float added to variance to avoid dividing by zero. center: If True, add offset of `beta` to normalized tensor. If False, `beta` is ignored. scale: If True, multiply by `gamma`. If False, `gamma` is not used. When the next layer is linear (also e.g. `nn.relu`), this can be disabled since the scaling will be done by the next layer. beta_initializer: Initializer for the beta weight. gamma_initializer: Initializer for the gamma weight. beta_regularizer: Optional regularizer for the beta weight. gamma_regularizer: Optional regularizer for the gamma weight. beta_constraint: Optional constraint for the beta weight. gamma_constraint: Optional constraint for the gamma weight. # Input shape Arbitrary. Use the keyword argument `input_shape` (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. # Output shape Same shape as input. # References - [Group Normalization](https://arxiv.org/abs/1803.08494) \"\"\" def __init__(self, groups=32, axis=-1, epsilon=1e-5, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None, **kwargs): super(GroupNormalization, self).__init__(**kwargs) self.supports_masking = True self.groups = groups self.axis = axis self.epsilon = epsilon self.center = center self.scale = scale self.beta_initializer = initializers.get(beta_initializer) self.gamma_initializer = initializers.get(gamma_initializer) self.beta_regularizer = regularizers.get(beta_regularizer) self.gamma_regularizer = regularizers.get(gamma_regularizer) self.beta_constraint = constraints.get(beta_constraint) self.gamma_constraint = constraints.get(gamma_constraint) def build(self, input_shape): dim = input_shape[self.axis] if dim is None: raise ValueError('Axis ' + str(self.axis) + ' of ' 'input tensor should have a defined dimension ' 'but the layer received an input with shape ' + str(input_shape) + '.') if dim &lt; self.groups: raise ValueError('Number of groups (' + str(self.groups) + ') cannot be ' 'more than the number of channels (' + str(dim) + ').') if dim % self.groups != 0: raise ValueError('Number of groups (' + str(self.groups) + ') must be a ' 'multiple of the number of channels (' + str(dim) + ').') self.input_spec = InputSpec(ndim=len(input_shape), axes=&#123;self.axis: dim&#125;) shape = (dim,) if self.scale: self.gamma = self.add_weight(shape=shape, name='gamma', initializer=self.gamma_initializer, regularizer=self.gamma_regularizer, constraint=self.gamma_constraint) else: self.gamma = None if self.center: self.beta = self.add_weight(shape=shape, name='beta', initializer=self.beta_initializer, regularizer=self.beta_regularizer, constraint=self.beta_constraint) else: self.beta = None self.built = True def call(self, inputs, **kwargs): input_shape = K.int_shape(inputs) # Prepare broadcasting shape. ndim = len(input_shape) reduction_axes = list(range(len(input_shape))) del reduction_axes[self.axis] broadcast_shape = [1] * len(input_shape) broadcast_shape[self.axis] = input_shape[self.axis] reshape_group_shape = list(input_shape) reshape_group_shape[self.axis] = input_shape[self.axis] // self.groups group_shape = [-1, self.groups] group_shape.extend(reshape_group_shape[1:]) group_reduction_axes = list(range(len(group_shape))) # Determines whether broadcasting is needed. needs_broadcasting = (sorted(reduction_axes) != list(range(ndim))[:-1]) inputs = K.reshape(inputs, group_shape) mean = K.mean(inputs, axis=group_reduction_axes[2:], keepdims=True) variance = K.var(inputs, axis=group_reduction_axes[2:], keepdims=True) inputs = (inputs - mean) / (K.sqrt(variance + self.epsilon)) original_shape = [-1] + list(input_shape[1:]) inputs = K.reshape(inputs, original_shape) if needs_broadcasting: outputs = inputs # In this case we must explicitly broadcast all parameters. if self.scale: broadcast_gamma = K.reshape(self.gamma, broadcast_shape) outputs = outputs * broadcast_gamma if self.center: broadcast_beta = K.reshape(self.beta, broadcast_shape) outputs = outputs + broadcast_beta else: outputs = inputs if self.scale: outputs = outputs * self.gamma if self.center: outputs = outputs + self.beta return outputs def get_config(self): config = &#123; 'groups': self.groups, 'axis': self.axis, 'epsilon': self.epsilon, 'center': self.center, 'scale': self.scale, 'beta_initializer': initializers.serialize(self.beta_initializer), 'gamma_initializer': initializers.serialize(self.gamma_initializer), 'beta_regularizer': regularizers.serialize(self.beta_regularizer), 'gamma_regularizer': regularizers.serialize(self.gamma_regularizer), 'beta_constraint': constraints.serialize(self.beta_constraint), 'gamma_constraint': constraints.serialize(self.gamma_constraint) &#125; base_config = super(GroupNormalization, self).get_config() return dict(list(base_config.items()) + list(config.items())) def compute_output_shape(self, input_shape): return input_shape","tags":[]},{"title":"Mask RCNNCode Reading for ROI Align Layer","date":"2018-06-03T00:57:33.000Z","path":"2018/06/03/Mask-RCNN-Code-Reading-ROI_Align/","text":"ROIAlign Layer這class在兩個地方會被呼叫到 1234567def build_fpn_mask_graph(rois, feature_maps, image_meta, pool_size, num_classes, train_bn=True): # ROI Pooling # Shape: [batch, num_boxes, pool_height, pool_width, channels] x = PyramidROIAlign([pool_size, pool_size], name=\"roi_align_mask\")([rois, image_meta] + feature_maps) 1234567def fpn_classifier_graph(rois, feature_maps, image_meta, pool_size, num_classes, train_bn=True): # ROI Pooling # Shape: [batch, num_boxes, pool_height, pool_width, channels] x = PyramidROIAlign([pool_size, pool_size], name=\"roi_align_classifier\")([rois, image_meta] + feature_maps) ROI獲得的可能會是從P2~P5, 而他們基本上anchors也不太依樣 功能型Layer, log2_graph存粹是因為tf 沒有這項功能所以只好自己寫一個. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117def log2_graph(x): \"\"\"Implementatin of Log2. TF doesn't have a native implemenation.\"\"\" return tf.log(x) / tf.log(2.0)class PyramidROIAlign(KE.Layer): \"\"\"Implements ROI Pooling on multiple levels of the feature pyramid. Params: - pool_shape: [height, width] of the output pooled regions. Usually [7, 7] Inputs: - boxes: [batch, num_boxes, (y1, x1, y2, x2)] in normalized coordinates. Possibly padded with zeros if not enough boxes to fill the array. - image_meta: [batch, (meta data)] Image details. See compose_image_meta() - Feature maps: List of feature maps from different levels of the pyramid. Each is [batch, height, width, channels] Output: Pooled regions in the shape: [batch, num_boxes, height, width, channels]. The width and height are those specific in the pool_shape in the layer constructor. \"\"\" def __init__(self, pool_shape, **kwargs): super(PyramidROIAlign, self).__init__(**kwargs) self.pool_shape = tuple(pool_shape) def call(self, inputs): # Crop boxes [batch, num_boxes, (y1, x1, y2, x2)] in normalized coords # inputs[0 is ROIs] boxes = inputs[0] # Image meta # Holds details about the image. See compose_image_meta() image_meta = inputs[1] # Feature Maps. List of feature maps from different level of the # feature pyramid. Each is [batch, height, width, channels] feature_maps = inputs[2:] # Assign each ROI to a level in the pyramid based on the ROI area. y1, x1, y2, x2 = tf.split(boxes, 4, axis=2) h = y2 - y1 w = x2 - x1 # Use shape of first image. Images in a batch must have the same size. image_shape = parse_image_meta_graph(image_meta)['image_shape'][0] # Equation 1 in the Feature Pyramid Networks paper. Account for # the fact that our coordinates are normalized here. # e.g. a 224x224 ROI (in pixels) maps to P4 image_area = tf.cast(image_shape[0] * image_shape[1], tf.float32) roi_level = log2_graph(tf.sqrt(h * w) / (224.0 / tf.sqrt(image_area))) roi_level = tf.minimum(5, tf.maximum( 2, 4 + tf.cast(tf.round(roi_level), tf.int32))) roi_level = tf.squeeze(roi_level, 2) # Loop through levels and apply ROI pooling to each. P2 to P5. pooled = [] box_to_level = [] for i, level in enumerate(range(2, 6)): ##應該是一個二維的array，存儲這哪一層的哪些box的indicies ix = tf.where(tf.equal(roi_level, level)) level_boxes = tf.gather_nd(boxes, ix) # Box indicies for crop_and_resize. box_indices = tf.cast(ix[:, 0], tf.int32) # Keep track of which box is mapped to which level ##應該是一個二維的array，存儲這哪一層的哪些box的indicies box_to_level.append(ix) # Stop gradient propogation to ROI proposals level_boxes = tf.stop_gradient(level_boxes) box_indices = tf.stop_gradient(box_indices) # Crop and Resize # From Mask R-CNN paper: \"We sample four regular locations, so # that we can evaluate either max or average pooling. In fact, # interpolating only a single value at each bin center (without # pooling) is nearly as effective.\" # # Here we use the simplified approach of a single value per bin, # which is how it's done in tf.crop_and_resize() # Result: [batch * num_boxes, pool_height, pool_width, channels] # 因為插值一個點和四個點的性能影響不大故插一個點 pooled.append(tf.image.crop_and_resize( feature_maps[i], level_boxes, box_indices, self.pool_shape, method=\"bilinear\")) # Pack pooled features into one tensor pooled = tf.concat(pooled, axis=0) # Pack box_to_level mapping into one array and add another # column representing the order of pooled boxes box_to_level = tf.concat(box_to_level, axis=0) box_range = tf.expand_dims(tf.range(tf.shape(box_to_level)[0]), 1) box_to_level = tf.concat([tf.cast(box_to_level, tf.int32), box_range], axis=1) # Rearrange pooled features to match the order of the original boxes # Sort box_to_level by batch then box index # TF doesn't have a way to sort by two columns, so merge them and sort. sorting_tensor = box_to_level[:, 0] * 100000 + box_to_level[:, 1] ix = tf.nn.top_k(sorting_tensor, k=tf.shape( box_to_level)[0]).indices[::-1] ix = tf.gather(box_to_level[:, 2], ix) pooled = tf.gather(pooled, ix) # Re-add the batch dimension pooled = tf.expand_dims(pooled, 0) return pooled def compute_output_shape(self, input_shape): return input_shape[0][:2] + self.pool_shape + (input_shape[2][-1], )","tags":[]},{"title":"Mask RCNNCode Reading - Detection Target Layer","date":"2018-06-02T22:57:33.000Z","path":"2018/06/03/Mask-RCNN-Code-Reading-DetectionTargetLayer/","text":"Detection Target Layer與Detection Layer 前者是在訓練時用到的後者是在測試時用的 其實對於inference來說,Proposal_Layer已經拿到要的結果了, DetectionTargetLayer最主要是訓練用途需要 所以會有ground truth (gt_box, gt_class_id, gt_mask) overlaps_graph這邊顧名思義是計算IOU用的, 123456789101112131415161718192021222324252627def overlaps_graph(boxes1, boxes2): \"\"\"Computes IoU overlaps between two sets of boxes. boxes1, boxes2: [N, (y1, x1, y2, x2)]. \"\"\" # 1. Tile boxes2 and repeat boxes1. This allows us to compare # every boxes1 against every boxes2 without loops. # TF doesn't have an equivalent to np.repeat() so simulate it # using tf.tile() and tf.reshape. b1 = tf.reshape(tf.tile(tf.expand_dims(boxes1, 1), [1, 1, tf.shape(boxes2)[0]]), [-1, 4]) b2 = tf.tile(boxes2, [tf.shape(boxes1)[0], 1]) # 2. Compute intersections b1_y1, b1_x1, b1_y2, b1_x2 = tf.split(b1, 4, axis=1) b2_y1, b2_x1, b2_y2, b2_x2 = tf.split(b2, 4, axis=1) y1 = tf.maximum(b1_y1, b2_y1) x1 = tf.maximum(b1_x1, b2_x1) y2 = tf.minimum(b1_y2, b2_y2) x2 = tf.minimum(b1_x2, b2_x2) intersection = tf.maximum(x2 - x1, 0) * tf.maximum(y2 - y1, 0) # 3. Compute unions b1_area = (b1_y2 - b1_y1) * (b1_x2 - b1_x1) b2_area = (b2_y2 - b2_y1) * (b2_x2 - b2_x1) union = b1_area + b2_area - intersection # 4. Compute IoU and reshape to [boxes1, boxes2] iou = intersection / union overlaps = tf.reshape(iou, [tf.shape(boxes1)[0], tf.shape(boxes2)[0]]) return overlaps Detection Target Layer12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758class DetectionTargetLayer(KE.Layer): \"\"\"Subsamples proposals and generates target box refinement, class_ids, and masks for each. Inputs: proposals: [batch, N, (y1, x1, y2, x2)] in normalized coordinates. Might be zero padded if there are not enough proposals. gt_class_ids: [batch, MAX_GT_INSTANCES] Integer class IDs. gt_boxes: [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)] in normalized coordinates. gt_masks: [batch, height, width, MAX_GT_INSTANCES] of boolean type Returns: Target ROIs and corresponding class IDs, bounding box shifts, and masks. rois: [batch, TRAIN_ROIS_PER_IMAGE, (y1, x1, y2, x2)] in normalized coordinates target_class_ids: [batch, TRAIN_ROIS_PER_IMAGE]. Integer class IDs. target_deltas: [batch, TRAIN_ROIS_PER_IMAGE, NUM_CLASSES, (dy, dx, log(dh), log(dw), class_id)] Class-specific bbox refinements. target_mask: [batch, TRAIN_ROIS_PER_IMAGE, height, width) Masks cropped to bbox boundaries and resized to neural network output size. Note: Returned arrays might be zero padded if not enough target ROIs. \"\"\" def __init__(self, config, **kwargs): super(DetectionTargetLayer, self).__init__(**kwargs) self.config = config def call(self, inputs): proposals = inputs[0] gt_class_ids = inputs[1] gt_boxes = inputs[2] gt_masks = inputs[3] # Slice the batch and run a graph for each slice # TODO: Rename target_bbox to target_deltas for clarity names = [\"rois\", \"target_class_ids\", \"target_bbox\", \"target_mask\"] outputs = utils.batch_slice( [proposals, gt_class_ids, gt_boxes, gt_masks], lambda w, x, y, z: detection_targets_graph( w, x, y, z, self.config), self.config.IMAGES_PER_GPU, names=names) return outputs def compute_output_shape(self, input_shape): return [ (None, self.config.TRAIN_ROIS_PER_IMAGE, 4), # rois (None, 1), # class_ids (None, self.config.TRAIN_ROIS_PER_IMAGE, 4), # deltas (None, self.config.TRAIN_ROIS_PER_IMAGE, self.config.MASK_SHAPE[0], self.config.MASK_SHAPE[1]) # masks ] def compute_mask(self, inputs, mask=None): return [None, None, None, None] detection_targets_graphdetection_targets_graph函數負責處理輸入的[proposals, gt_class_ids, gt_boxes, gt_masks] Code有點長，首先計算proposals和gt_boxes的覆蓋度矩陣proposals*gt_boxes，然後獲得每個proposals一個與gt_boxes最大覆蓋度值roi_iou_max，如果roi_iou_max&gt;0.5，則認為該proposals是positive_roi，最後再把對應gt_box分配給對應的positive_roi並進行框體微調獲得對應偏移。最終返回的rois包含positive_roi和negative_roi。 根據proposal和gt_box的overlap來確定正樣本和負樣本，並按照sample_ratio和train_anchor_per_image的大小進行sample，最終得出rois, class_id,delta,masks，其中進行了padding 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137def detection_targets_graph(proposals, gt_class_ids, gt_boxes, gt_masks, config): \"\"\"Generates detection targets for one image. Subsamples proposals and generates target class IDs, bounding box deltas, and masks for each. Inputs: proposals: [N, (y1, x1, y2, x2)] in normalized coordinates. Might be zero padded if there are not enough proposals. gt_class_ids: [MAX_GT_INSTANCES] int class IDs gt_boxes: [MAX_GT_INSTANCES, (y1, x1, y2, x2)] in normalized coordinates. gt_masks: [height, width, MAX_GT_INSTANCES] of boolean type. Returns: Target ROIs and corresponding class IDs, bounding box shifts, and masks. rois: [TRAIN_ROIS_PER_IMAGE, (y1, x1, y2, x2)] in normalized coordinates class_ids: [TRAIN_ROIS_PER_IMAGE]. Integer class IDs. Zero padded. deltas: [TRAIN_ROIS_PER_IMAGE, NUM_CLASSES, (dy, dx, log(dh), log(dw))] Class-specific bbox refinements. masks: [TRAIN_ROIS_PER_IMAGE, height, width). Masks cropped to bbox boundaries and resized to neural network output size. Note: Returned arrays might be zero padded if not enough target ROIs. \"\"\" # Assertions asserts = [ tf.Assert(tf.greater(tf.shape(proposals)[0], 0), [proposals],name=\"roi_assertion\"),] with tf.control_dependencies(asserts): proposals = tf.identity(proposals) # Remove zero padding # 去除0的padding proposals, _ = trim_zeros_graph(proposals, name=\"trim_proposals\") gt_boxes, non_zeros = trim_zeros_graph(gt_boxes, name=\"trim_gt_boxes\") gt_class_ids = tf.boolean_mask(gt_class_ids, non_zeros, name=\"trim_gt_class_ids\") gt_masks = tf.gather(gt_masks, tf.where(non_zeros)[:, 0], axis=2, name=\"trim_gt_masks\") # 有點看不懂 反正是處理coco的問題的樣子 # Handle COCO crowds # A crowd box in COCO is a bounding box around several instances. Exclude # them from training. A crowd box is given a negative class ID. crowd_ix = tf.where(gt_class_ids &lt; 0)[:, 0] non_crowd_ix = tf.where(gt_class_ids &gt; 0)[:, 0] crowd_boxes = tf.gather(gt_boxes, crowd_ix) crowd_masks = tf.gather(gt_masks, crowd_ix, axis=2) gt_class_ids = tf.gather(gt_class_ids, non_crowd_ix) gt_boxes = tf.gather(gt_boxes, non_crowd_ix) gt_masks = tf.gather(gt_masks, non_crowd_ix, axis=2) # Compute overlaps matrix [proposals, gt_boxes] overlaps = overlaps_graph(proposals, gt_boxes) # Compute overlaps with crowd boxes [anchors, crowds] crowd_overlaps = overlaps_graph(proposals, crowd_boxes) crowd_iou_max = tf.reduce_max(crowd_overlaps, axis=1) no_crowd_bool = (crowd_iou_max &lt; 0.001) # Determine postive and negative ROIs # 獲得proposals和gt_box最大overlaps的值[n_proposals,1] roi_iou_max = tf.reduce_max(overlaps, axis=1) # 1. Positive ROIs are those with &gt;= 0.5 IoU with a GT box # roi覆蓋度值&gt;0.5則認為其是positive_roi bool[n_proposals,1] positive_roi_bool = (roi_iou_max &gt;= 0.5) # 拿positive_roi的index [filter_n_proposals,1] positive_indices = tf.where(positive_roi_bool)[:, 0] # 2. Negative ROIs are those with &lt; 0.5 with every GT box. Skip crowds. negative_indices = tf.where(tf.logical_and(roi_iou_max &lt; 0.5, no_crowd_bool))[:, 0] # 對positive_roi和negative_roi進行了subsample # Subsample ROIs. Aim for 33% positive # Positive ROIs positive_count = int(config.TRAIN_ROIS_PER_IMAGE * config.ROI_POSITIVE_RATIO) positive_indices = tf.random_shuffle(positive_indices)[:positive_count] positive_count = tf.shape(positive_indices)[0] # Negative ROIs. Add enough to maintain positive:negative ratio. r = 1.0 / config.ROI_POSITIVE_RATIO negative_count = tf.cast(r * tf.cast(positive_count, tf.float32), tf.int32) - positive_count negative_indices = tf.random_shuffle(negative_indices)[:negative_count] # Gather selected ROIs positive_rois = tf.gather(proposals, positive_indices) negative_rois = tf.gather(proposals, negative_indices) # Assign positive ROIs to GT boxes. # 把sample後的positive_roi分配給gt_box positive_overlaps = tf.gather(overlaps, positive_indices) # 每個positive_rois對應gt_box overlaps最大值的下標[filter_n_proposals,1] roi_gt_box_assignment = tf.argmax(positive_overlaps, axis=1) roi_gt_boxes = tf.gather(gt_boxes, roi_gt_box_assignment) roi_gt_class_ids = tf.gather(gt_class_ids, roi_gt_box_assignment) # Compute bbox refinement for positive ROIs deltas = utils.box_refinement_graph(positive_rois, roi_gt_boxes) deltas /= config.BBOX_STD_DEV # Assign positive ROIs to GT masks # Permute masks to [N, height, width, 1] transposed_masks = tf.expand_dims(tf.transpose(gt_masks, [2, 0, 1]), -1) # Pick the right mask for each ROI roi_masks = tf.gather(transposed_masks, roi_gt_box_assignment) # Compute mask targets boxes = positive_rois if config.USE_MINI_MASK: # Transform ROI corrdinates from normalized image space # to normalized mini-mask space. y1, x1, y2, x2 = tf.split(positive_rois, 4, axis=1) gt_y1, gt_x1, gt_y2, gt_x2 = tf.split(roi_gt_boxes, 4, axis=1) gt_h = gt_y2 - gt_y1 gt_w = gt_x2 - gt_x1 y1 = (y1 - gt_y1) / gt_h x1 = (x1 - gt_x1) / gt_w y2 = (y2 - gt_y1) / gt_h x2 = (x2 - gt_x1) / gt_w boxes = tf.concat([y1, x1, y2, x2], 1) box_ids = tf.range(0, tf.shape(roi_masks)[0]) masks = tf.image.crop_and_resize(tf.cast(roi_masks, tf.float32), boxes, box_ids, config.MASK_SHAPE) # Remove the extra dimension from masks. masks = tf.squeeze(masks, axis=3) # Threshold mask pixels at 0.5 to have GT masks be 0 or 1 to use with # binary cross entropy loss. masks = tf.round(masks) # Append negative ROIs and pad bbox deltas and masks that # are not used for negative ROIs with zeros. rois = tf.concat([positive_rois, negative_rois], axis=0) N = tf.shape(negative_rois)[0] P = tf.maximum(config.TRAIN_ROIS_PER_IMAGE - tf.shape(rois)[0], 0) rois = tf.pad(rois, [(0, P), (0, 0)]) roi_gt_boxes = tf.pad(roi_gt_boxes, [(0, N + P), (0, 0)]) roi_gt_class_ids = tf.pad(roi_gt_class_ids, [(0, N + P)]) deltas = tf.pad(deltas, [(0, N + P), (0, 0)]) masks = tf.pad(masks, [[0, N + P], (0, 0), (0, 0)]) return rois, roi_gt_class_ids, deltas, masks","tags":[]},{"title":"Mask RCNNCode Reading for Proposal Layer","date":"2018-06-02T15:57:33.000Z","path":"2018/06/02/Mask-RCNN-Code-Reading-Propsal-Layer/","text":"通過下面這段程式碼拿到anchors以後 12345678910# Anchorsif mode == \"training\": anchors = self.get_anchors(config.IMAGE_SHAPE) # Duplicate across the batch dimension because Keras requires it # TODO: can this be optimized to avoid duplicating the anchors? anchors = np.broadcast_to(anchors, (config.BATCH_SIZE,) + anchors.shape) # A hack to get around Keras's bad support for constants anchors = KL.Lambda(lambda x: tf.Variable(anchors), name=\"anchors\")(input_image)else: anchors = input_anchors 這裡總算要進行proposal layer計算了將所有anchors還有相應對的分數跟偏移量當作輸入而前面的東西都是class init的時候的設定 123456789101112131415161718192021222324252627282930313233343536rpn_class_logits, rpn_class, rpn_bbox = outputs# Generate proposals# Proposals are [batch, N, (y1, x1, y2, x2)] in normalized coordinates# and zero padded.# POST_NMS_ROIS_INFERENCE = 1000# POST_NMS_ROIS_TRAINING = 2000 proposal_count = config.POST_NMS_ROIS_TRAINING if mode == \"training\"\\ else config.POST_NMS_ROIS_INFERENCE rpn_rois = ProposalLayer( proposal_count=proposal_count, nms_threshold=config.RPN_NMS_THRESHOLD, name=\"ROI\", config=config)([rpn_class, rpn_bbox, anchors] )if mode == \"training\": # Class ID mask to mark class IDs supported by the dataset the image # came from. active_class_ids = KL.Lambda( lambda x: parse_image_meta_graph(x)[\"active_class_ids\"] )(input_image_meta) if not config.USE_RPN_ROIS: # Ignore predicted ROIs and use ROIs provided as an input. input_rois = KL.Input(shape=[config.POST_NMS_ROIS_TRAINING, 4], name=\"input_roi\", dtype=np.int32) # Normalize coordinates target_rois = KL.Lambda(lambda x: norm_boxes_graph( x, K.shape(input_image)[1:3]))(input_rois) else: target_rois = rpn_rois 其中parse_image_meta_graph 12345678return &#123; \"image_id\": image_id, \"original_image_shape\": original_image_shape, \"image_shape\": image_shape, \"window\": window, \"scale\": scale, \"active_class_ids\": active_class_ids, &#125; Proposal Layerrpn_class：所有像素點BG/FG的機率值。rpn_bbox：所有像素點對應anchor上的4個偏移值[dy, dx, log(dh), log(dw)]。anchors: 剛剛通過預先生成的有序anchor列表，注意這裡有序表示feature_map上像素點生成的anchor以及該像素點生成的rpn_class和rpn_bbox是對應的（看paper看起來是這樣,但這樣理解對嗎, 有點不確定)scores和deltas都是RPN中得到的 最終ProposalLayer會return一個經過bbox regression以及NMS過濾後anchor boxes set(稱為roi或proposal)，至此已經完成了RPN啦~init內super的用法可參考 你不知道的 super 要注意的事情是, 其實對於inference來說 這裡已經有RPN的結果了, 但對於training來說還少了一些東西, 就是跟grondtruth的比較還有等等, 所以才有後續的DetectionTargetLayer 另外Python中，如果在創建class的時候寫了call()， 那麼該class實例化出實例後， 實例名()就是調用call()。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687class ProposalLayer(KE.Layer): \"\"\"Receives anchor scores and selects a subset to pass as proposals to the second stage. Filtering is done based on anchor scores and non-max suppression to remove overlaps. It also applies bounding box refinement deltas to anchors. Inputs: rpn_probs: [batch, anchors, (bg prob, fg prob)] rpn_bbox: [batch, anchors, (dy, dx, log(dh), log(dw))] anchors: [batch, (y1, x1, y2, x2)] anchors in normalized coordinates Returns: Proposals in normalized coordinates [batch, rois, (y1, x1, y2, x2)] \"\"\" def __init__(self, proposal_count, nms_threshold, config=None, **kwargs): super(ProposalLayer, self).__init__(**kwargs) self.config = config self.proposal_count = proposal_count self.nms_threshold = nms_threshold def call(self, inputs): ###實現了將傳入的anchors，及其scores、deltas進行topK的推薦和nms的推薦，最終輸出 ###數量為proposal_counts的proposals。其中的scores和deltas都是RPN網絡中得到的 # Box Scores. Use the foreground class confidence. [Batch, num_rois, 1] scores = inputs[0][:, :, 1] # Box deltas [batch, num_rois, 4] deltas = inputs[1] deltas = deltas * np.reshape(self.config.RPN_BBOX_STD_DEV, [1, 1, 4]) # Anchors anchors = inputs[2] # Improve performance by trimming to top anchors by score # and doing the rest on the smaller subset. pre_nms_limit = tf.minimum(6000, tf.shape(anchors)[1]) ix = tf.nn.top_k(scores, pre_nms_limit, sorted=True, name=\"top_anchors\").indices scores = utils.batch_slice([scores, ix], lambda x, y: tf.gather(x, y), self.config.IMAGES_PER_GPU) deltas = utils.batch_slice([deltas, ix], lambda x, y: tf.gather(x, y), self.config.IMAGES_PER_GPU) pre_nms_anchors = utils.batch_slice([anchors, ix], lambda a, x: tf.gather(a, x), self.config.IMAGES_PER_GPU, names=[\"pre_nms_anchors\"]) # Apply deltas to anchors to get refined anchors. # [batch, N, (y1, x1, y2, x2)] ##利用deltas在anchors上，得到精煉後的boxs boxes = utils.batch_slice([pre_nms_anchors, deltas], lambda x, y: apply_box_deltas_graph(x, y), self.config.IMAGES_PER_GPU, names=[\"refined_anchors\"]) # normalized coordinates就是對應原圖的百分比坐標 # 下面的作用：防止修正後的anchor坐標超出了邊界即0&lt;=x,y&lt;=1 # Clip to image boundaries. Since we're in normalized coordinates, # clip to 0..1 range. [batch, N, (y1, x1, y2, x2)] window = np.array([0, 0, 1, 1], dtype=np.float32) boxes = utils.batch_slice(boxes, lambda x: clip_boxes_graph(x, window), self.config.IMAGES_PER_GPU, names=[\"refined_anchors_clipped\"]) # Filter out small boxes # According to Xinlei Chen's paper, this reduces detection accuracy # for small objects, so we're skipping it. # Non-max suppression def nms(boxes, scores): indices = tf.image.non_max_suppression( boxes, scores, self.proposal_count, self.nms_threshold, name=\"rpn_non_max_suppression\") proposals = tf.gather(boxes, indices) # Pad if needed padding = tf.maximum(self.proposal_count - tf.shape(proposals)[0], 0) ##利用deltas在anchors上，得到精化的boxs proposals = tf.pad(proposals, [(0, padding), (0, 0)]) return proposals proposals = utils.batch_slice([boxes, scores], nms, self.config.IMAGES_PER_GPU) return proposals def compute_output_shape(self, input_shape): return (None, self.proposal_count, 4)","tags":[]},{"title":"Mask RCNNCode Reading for Anchor boxes generate","date":"2018-06-02T12:57:33.000Z","path":"2018/06/02/Mask-RCNN-Code-Reading-Anchor-boxes-Generate/","text":"接續上面的RPN output 123456789101112131415161718rpn_class_logits, rpn_class, rpn_bbox = outputs# Generate proposals# Proposals are [batch, N, (y1, x1, y2, x2)] in normalized coordinates# and zero padded.# POST_NMS_ROIS_INFERENCE = 1000# POST_NMS_ROIS_TRAINING = 2000 proposal_count = config.POST_NMS_ROIS_TRAINING if mode == \"training\"\\ else config.POST_NMS_ROIS_INFERENCE rpn_rois = ProposalLayer( proposal_count=proposal_count, nms_threshold=config.RPN_NMS_THRESHOLD, name=\"ROI\", config=config)([rpn_class, rpn_bbox, anchors] ) Anchor boxes generate而 anchors 的來源呢在主程式碼中有這段 12345678910# Anchorsif mode == \"training\": anchors = self.get_anchors(config.IMAGE_SHAPE) # Duplicate across the batch dimension because Keras requires it # TODO: can this be optimized to avoid duplicating the anchors? anchors = np.broadcast_to(anchors, (config.BATCH_SIZE,) + anchors.shape) # A hack to get around Keras's bad support for constants anchors = KL.Lambda(lambda x: tf.Variable(anchors), name=\"anchors\")(input_image)else: anchors = input_anchors def get_anchorscompute_backbone_shapes: 123Computes the width and height of each stage of the backbone network.Returns: [N, (height, width)]. Where N is the number of stages utils.generate_pyramid_anchors這邊看起來相對於single scale的backbone, 這裡多了backbone_shapes這項變數, 是因為需要知道該層feature_map大小才能回推anchor boxes 123456789101112131415161718192021222324RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)RPN_ANCHOR_RATIOS = [0.5, 1 ,2]def get_anchors(self, image_shape): \"\"\"Returns anchor pyramid for the given image size.\"\"\" backbone_shapes = compute_backbone_shapes(self.config, image_shape) # Cache anchors and reuse if image shape is the same if not hasattr(self, \"_anchor_cache\"): self._anchor_cache = &#123;&#125; if not tuple(image_shape) in self._anchor_cache: # Generate Anchors a = utils.generate_pyramid_anchors( self.config.RPN_ANCHOR_SCALES, self.config.RPN_ANCHOR_RATIOS, backbone_shapes, self.config.BACKBONE_STRIDES, self.config.RPN_ANCHOR_STRIDE) # Keep a copy of the latest anchors in pixel coordinates because # it's used in inspect_model notebooks. # TODO: Remove this after the notebook are refactored to not use it self.anchors = a # Normalize coordinates self._anchor_cache[tuple(image_shape)] = utils.norm_boxes(a, image_shape[:2]) return self._anchor_cache[tuple(image_shape)] utils.generate_pyramid_anchors這邊很明顯就是包裝一層, 讓各個stage (P2~P6)的各自去計算以後append後方操作…舉個例子, 順便了解一下IO, 其實只是會了整形而已 123456789101112131415&gt;&gt;&gt; a=np.arange(5)&gt;&gt;&gt; b=np.array([11,22,33])&gt;&gt;&gt; aarray([0, 1, 2, 3, 4])&gt;&gt;&gt; barray([11, 22, 33])&gt;&gt;&gt; c.append(a)&gt;&gt;&gt; c[array([0, 1, 2, 3, 4])]&gt;&gt;&gt; c.append(b)&gt;&gt;&gt; c[array([0, 1, 2, 3, 4]), array([11, 22, 33])]&gt;&gt;&gt; np.concatenate(c,axis=0)array([ 0, 1, 2, 3, 4, 11, 22, 33]) 1234567891011121314151617181920212223def generate_pyramid_anchors(scales, ratios, feature_shapes, feature_strides,anchor_stride): \"\"\"Generate anchors at different levels of a feature pyramid. Each scale is associated with a level of the pyramid, but each ratio is used in all levels of the pyramid. Returns: anchors: [N, (y1, x1, y2, x2)]. All generated anchors in one array. Sorted with the same order of the given scales. So, anchors of scale[0] come first, then anchors of scale[1], and so on. \"\"\" # Anchors # [anchor_count, (y1, x1, y2, x2)] anchors = [] for i in range(len(scales)): anchors.append( generate_anchors( scales[i], # self.config.RPN_ANCHOR_SCALES ratios, # self.config.RPN_ANCHOR_RATIOS feature_shapes[i], # backbone_shapes feature_strides[i], # self.config.BACKBONE_STRIDES [4, 8, 16, 32, 64] anchor_stride # self.config.RPN_ANCHOR_STRIDE )) return np.concatenate(anchors, axis=0) utils.generate_anchorsCode內其實有參數說明了scales就是anchor boxes邊長了ratio就是anchor boxes的比例shapes是原圖feature_stride 其實就是原圖與該feature層(例如p3)的縮放倍率, 如果是resnet或是VGG這種就一律都是縮放16倍了anchor_stride 這通常是1吧XD 就是以縮放比切割原圖後可以得到(w/feature_stride)x(h/feature_stride)個grid, 那在此多少區間弄一個anchor? 12345678910111213141516171819202122232425262728293031323334353637def generate_anchors(scales, ratios, shape, feature_stride, anchor_stride): \"\"\" scales: 1D array of anchor sizes in pixels. Example: [32, 64, 128] ratios: 1D array of anchor ratios of width/height. Example: [0.5, 1, 2] shape: [height, width] spatial shape of the feature map over which to generate anchors. feature_stride: Stride of the feature map relative to the image in pixels. anchor_stride: Stride of anchors on the feature map. For example, if the value is 2 then generate anchors for every other feature map pixel. \"\"\" # Get all combinations of scales and ratios scales, ratios = np.meshgrid(np.array(scales), np.array(ratios)) scales = scales.flatten() ratios = ratios.flatten() # Enumerate heights and widths from scales and ratios heights = scales / np.sqrt(ratios) widths = scales * np.sqrt(ratios) # Enumerate shifts in feature space shifts_y = np.arange(0, shape[0], anchor_stride) * feature_stride shifts_x = np.arange(0, shape[1], anchor_stride) * feature_stride shifts_x, shifts_y = np.meshgrid(shifts_x, shifts_y) # Enumerate combinations of shifts, widths, and heights box_widths, box_centers_x = np.meshgrid(widths, shifts_x) box_heights, box_centers_y = np.meshgrid(heights, shifts_y) # Reshape to get a list of (y, x) and a list of (h, w) box_centers = np.stack( [box_centers_y, box_centers_x], axis=2).reshape([-1, 2]) box_sizes = np.stack([box_heights, box_widths], axis=2).reshape([-1, 2]) # Convert to corner coordinates (y1, x1, y2, x2) boxes = np.concatenate([box_centers - 0.5 * box_sizes, box_centers + 0.5 * box_sizes], axis=1) return boxes 下面用一組參數示範一下, 到最後widths, heights 就可以得到anchor boxes的所有邊長了np.meshgrid的部分, 要注意傳入參數得需要是一維的, 接著再用flatten()把它變成1xN的arraynp.stack我覺得挺難理解, 可以看這邊 numpy.stack最通俗的理解 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180# Get all combinations of scales and ratios&gt;&gt;&gt; scales = [32, 64, 128, 256, 512]&gt;&gt;&gt; ratios = [0.5,1,2]&gt;&gt;&gt; np.meshgrid(np.array(scales), np.array(ratios))[ array([ [ 32, 64, 128, 256, 512], [ 32, 64, 128, 256, 512], [ 32, 64, 128, 256, 512] ]), array([ [0.5, 0.5, 0.5, 0.5, 0.5], [1. , 1. , 1. , 1. , 1. ], [2. , 2. , 2. , 2. , 2. ] ])]### 反過來傳結果有點不一樣&gt;&gt;&gt; np.meshgrid(np.array(ratios), np.array(scales))[array([[0.5, 1. , 2. ], [0.5, 1. , 2. ], [0.5, 1. , 2. ], [0.5, 1. , 2. ], [0.5, 1. , 2. ]]), array([[ 32, 32, 32], [ 64, 64, 64], [128, 128, 128], [256, 256, 256], [512, 512, 512]])]&gt;&gt;&gt; scales = scales.flatten()&gt;&gt;&gt; ratios = ratios.flatten()# Enumerate heights and widths from scales and ratios &gt;&gt;&gt; heights = scales / np.sqrt(ratios)&gt;&gt;&gt; widths = scales * np.sqrt(ratios)&gt;&gt;&gt; heightsarray([ 45.254834 , 90.50966799, 181.01933598, 362.03867197, 724.07734394, 32. , 64. , 128. , 256. , 512. , 22.627417 , 45.254834 , 90.50966799, 181.01933598, 362.03867197])&gt;&gt;&gt; widthsarray([ 22.627417 , 45.254834 , 90.50966799, 181.01933598, 362.03867197, 32. , 64. , 128. , 256. , 512. , 45.254834 , 90.50966799, 181.01933598, 362.03867197, 724.07734394])&gt;&gt;&gt; heights.shape, widths.shape((15,), (15,))# Enumerate shifts in feature space&gt;&gt;&gt; anchor_stride=1&gt;&gt;&gt; shape=[1024,2048]&gt;&gt;&gt; feature_strides = [4, 8, 16, 32, 64]&gt;&gt;&gt; feature_stride = feature_strides[2]&gt;&gt;&gt; shifts_x = np.arange(0, shape[1], anchor_stride) * feature_stride&gt;&gt;&gt; shifts_y = np.arange(0, shape[0], anchor_stride) * feature_stride&gt;&gt;&gt; shifts_xarray([ 0, 16, 32, ..., 32720, 32736, 32752])&gt;&gt;&gt; shifts_yarray([ 0, 16, 32, ..., 16336, 16352, 16368])&gt;&gt;&gt; len(shifts_x), len(shifts_y)2048,1024&gt;&gt;&gt; shifts_x, shifts_y = np.meshgrid(shifts_x, shifts_y)&gt;&gt;&gt; shifts_yarray([[ 0, 0, 0, ..., 0, 0, 0], [ 16, 16, 16, ..., 16, 16, 16], [ 32, 32, 32, ..., 32, 32, 32], ..., [16336, 16336, 16336, ..., 16336, 16336, 16336], [16352, 16352, 16352, ..., 16352, 16352, 16352], [16368, 16368, 16368, ..., 16368, 16368, 16368]])&gt;&gt;&gt; shifts_xarray([[ 0, 16, 32, ..., 32720, 32736, 32752], [ 0, 16, 32, ..., 32720, 32736, 32752], [ 0, 16, 32, ..., 32720, 32736, 32752], ..., [ 0, 16, 32, ..., 32720, 32736, 32752], [ 0, 16, 32, ..., 32720, 32736, 32752], [ 0, 16, 32, ..., 32720, 32736, 32752]])&gt;&gt;&gt; shifts_x.shape, shifts_y.shape((1024, 2048), (1024, 2048)) # Enumerate combinations of shifts, widths, and heights&gt;&gt;&gt; box_widths, box_centers_x = np.meshgrid(widths, shifts_x)&gt;&gt;&gt; box_heights, box_centers_y = np.meshgrid(heights, shifts_y)&gt;&gt;&gt; box_heightsarray([[ 45.254834 , 90.50966799, 181.01933598, ..., 90.50966799, 181.01933598, 362.03867197], [ 45.254834 , 90.50966799, 181.01933598, ..., 90.50966799, 181.01933598, 362.03867197], [ 45.254834 , 90.50966799, 181.01933598, ..., 90.50966799, 181.01933598, 362.03867197], ..., [ 45.254834 , 90.50966799, 181.01933598, ..., 90.50966799, 181.01933598, 362.03867197], [ 45.254834 , 90.50966799, 181.01933598, ..., 90.50966799, 181.01933598, 362.03867197], [ 45.254834 , 90.50966799, 181.01933598, ..., 90.50966799, 181.01933598, 362.03867197]])&gt;&gt;&gt; box_centers_yarray([[ 0, 0, 0, ..., 0, 0, 0], [ 0, 0, 0, ..., 0, 0, 0], [ 0, 0, 0, ..., 0, 0, 0], ..., [16368, 16368, 16368, ..., 16368, 16368, 16368], [16368, 16368, 16368, ..., 16368, 16368, 16368], [16368, 16368, 16368, ..., 16368, 16368, 16368]])&gt;&gt;&gt; box_centers_xarray([[ 0, 0, 0, ..., 0, 0, 0], [ 16, 16, 16, ..., 16, 16, 16], [ 32, 32, 32, ..., 32, 32, 32], ..., [32720, 32720, 32720, ..., 32720, 32720, 32720], [32736, 32736, 32736, ..., 32736, 32736, 32736], [32752, 32752, 32752, ..., 32752, 32752, 32752]])&gt;&gt;&gt; box_widthsarray([[ 22.627417 , 45.254834 , 90.50966799, ..., 181.01933598, 362.03867197, 724.07734394], [ 22.627417 , 45.254834 , 90.50966799, ..., 181.01933598, 362.03867197, 724.07734394], [ 22.627417 , 45.254834 , 90.50966799, ..., 181.01933598, 362.03867197, 724.07734394], ..., [ 22.627417 , 45.254834 , 90.50966799, ..., 181.01933598, 362.03867197, 724.07734394], [ 22.627417 , 45.254834 , 90.50966799, ..., 181.01933598, 362.03867197, 724.07734394], [ 22.627417 , 45.254834 , 90.50966799, ..., 181.01933598, 362.03867197, 724.07734394]])3867197,&gt;&gt;&gt; box_widths.shape, box_centers_x.shape, box_heights.shape , box_centers_y.shape((2097152, 15), (2097152, 15), (2097152, 15), (2097152, 15))# Reshape to get a list of (y, x) and a list of (h, w)&gt;&gt;&gt; box_centersarray([[ 0, 0], [ 0, 0], [ 0, 0], ..., [16368, 32752], [16368, 32752], [16368, 32752]])&gt;&gt;&gt; box_sizesarray([[ 45.254834 , 22.627417 ], [ 90.50966799, 45.254834 ], [181.01933598, 90.50966799], ..., [ 90.50966799, 181.01933598], [181.01933598, 362.03867197], [362.03867197, 724.07734394]])&gt;&gt;&gt; len(box_centers),len(box_sizes)(31457280, 31457280)&gt;&gt;&gt; box_centers.shape, box_sizes.shape((31457280, 2), (31457280, 2))# Convert to corner coordinates (y1, x1, y2, x2)&gt;&gt;&gt; boxes = np.concatenate([box_centers - 0.5 * box_sizes,... box_centers + 0.5 * box_sizes], axis=1)&gt;&gt;&gt; boxesarray([[-2.26274170e+01, -1.13137085e+01, 2.26274170e+01, 1.13137085e+01], [-4.52548340e+01, -2.26274170e+01, 4.52548340e+01, 2.26274170e+01], [-9.05096680e+01, -4.52548340e+01, 9.05096680e+01, 4.52548340e+01], ..., [ 1.63227452e+04, 3.26614903e+04, 1.64132548e+04, 3.28425097e+04], [ 1.62774903e+04, 3.25709807e+04, 1.64585097e+04, 3.29330193e+04], [ 1.61869807e+04, 3.23899613e+04, 1.65490193e+04, 3.31140387e+04]]) &gt;&gt;&gt; boxes.shape(31457280, 4) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&gt;&gt;&gt; np.broadcast_to(boxes, (4,) + boxes.shape)array([[[-2.26274170e+01, -1.13137085e+01, 2.26274170e+01, 1.13137085e+01], [-4.52548340e+01, -2.26274170e+01, 4.52548340e+01, 2.26274170e+01], [-9.05096680e+01, -4.52548340e+01, 9.05096680e+01, 4.52548340e+01], ..., [ 1.63227452e+04, 3.26614903e+04, 1.64132548e+04, 3.28425097e+04], [ 1.62774903e+04, 3.25709807e+04, 1.64585097e+04, 3.29330193e+04], [ 1.61869807e+04, 3.23899613e+04, 1.65490193e+04, 3.31140387e+04]], [[-2.26274170e+01, -1.13137085e+01, 2.26274170e+01, 1.13137085e+01], [-4.52548340e+01, -2.26274170e+01, 4.52548340e+01, 2.26274170e+01], [-9.05096680e+01, -4.52548340e+01, 9.05096680e+01, 4.52548340e+01], ..., [ 1.63227452e+04, 3.26614903e+04, 1.64132548e+04, 3.28425097e+04], [ 1.62774903e+04, 3.25709807e+04, 1.64585097e+04, 3.29330193e+04], [ 1.61869807e+04, 3.23899613e+04, 1.65490193e+04, 3.31140387e+04]], [[-2.26274170e+01, -1.13137085e+01, 2.26274170e+01, 1.13137085e+01], [-4.52548340e+01, -2.26274170e+01, 4.52548340e+01, 2.26274170e+01], [-9.05096680e+01, -4.52548340e+01, 9.05096680e+01, 4.52548340e+01], ..., [ 1.63227452e+04, 3.26614903e+04, 1.64132548e+04, 3.28425097e+04], [ 1.62774903e+04, 3.25709807e+04, 1.64585097e+04, 3.29330193e+04], [ 1.61869807e+04, 3.23899613e+04, 1.65490193e+04, 3.31140387e+04]], [[-2.26274170e+01, -1.13137085e+01, 2.26274170e+01, 1.13137085e+01], [-4.52548340e+01, -2.26274170e+01, 4.52548340e+01, 2.26274170e+01], [-9.05096680e+01, -4.52548340e+01, 9.05096680e+01, 4.52548340e+01], ..., [ 1.63227452e+04, 3.26614903e+04, 1.64132548e+04, 3.28425097e+04], [ 1.62774903e+04, 3.25709807e+04, 1.64585097e+04, 3.29330193e+04], [ 1.61869807e+04, 3.23899613e+04, 1.65490193e+04, 3.31140387e+04]]])&gt;&gt;&gt; np.broadcast_to(boxes, (4,) + boxes.shape).shape(4, 31457280, 4)","tags":[]},{"title":"Mask RCNN Code Reading for RPN","date":"2018-05-30T01:16:43.000Z","path":"2018/05/30/Mask-RCNN-Code-Reading-RPN/","text":"Mask_RCNN V2.1版本 檔案：Model.py Class MaskRCNN():這部分是在程式碼主體所使用到RPN的部分, 其中傳入參數預設為此, for loop那邊可以看到有幾個從fpn的output就會有幾個rpn RPN_ANCHOR_RATIOS = [0.5, 1, 2] RPN_ANCHOR_STRIDE = 1 123456789101112131415161718# Note that P6 is used in RPN, but not in the classifier heads. rpn_feature_maps = [P2, P3, P4, P5, P6] mrcnn_feature_maps = [P2, P3, P4, P5] # Anchors if mode == \"training\": anchors = self.get_anchors(config.IMAGE_SHAPE) # Duplicate across the batch dimension because Keras requires it # TODO: can this be optimized to avoid duplicating the anchors? anchors = np.broadcast_to(anchors, (config.BATCH_SIZE,) + anchors.shape) # A hack to get around Keras's bad support for constants anchors = KL.Lambda(lambda x: tf.Variable(anchors), name=\"anchors\")(input_image) else: anchors = input_anchors # RPN Model rpn = build_rpn_model(config.RPN_ANCHOR_STRIDE, len(config.RPN_ANCHOR_RATIOS), 256) 其中layer_output會長這樣, 就是一堆input是rpn_feature_maps = [P2, P3, P4, P5, P6] 輸出是[“rpn_class_logits”, “rpn_class”, “rpn_bbox”]的東西 1[[&lt;tf.Tensor 'rpn_model/lambda_2/Reshape:0' shape=(?, ?, 2) dtype=float32&gt;, &lt;tf.Tensor 'rpn_model/rpn_class_xxx/truediv:0' shape=(?, ?, 2) dtype=float32&gt;, &lt;tf.Tensor 'rpn_model/lambda_3/Reshape:0' shape=(?, ?, 4) dtype=float32&gt;], [&lt;tf.Tensor 'rpn_model_1/lambda_2/Reshape:0' shape=(?, ?, 2) dtype=float32&gt;, &lt;tf.Tensor 'rpn_model_1/rpn_class_xxx/truediv:0' shape=(?, ?, 2) dtype=float32&gt;, &lt;tf.Tensor 'rpn_model_1/lambda_3/Reshape:0' shape=(?, ?, 4) dtype=float32&gt;], [&lt;tf.Tensor 'rpn_model_2/lambda_2/Reshape:0' shape=(?, ?, 2) dtype=float32&gt;, &lt;tf.Tensor 'rpn_model_2/rpn_class_xxx/truediv:0' shape=(?, ?, 2) dtype=float32&gt;, &lt;tf.Tensor 'rpn_model_2/lambda_3/Reshape:0' shape=(?, ?, 4) dtype=float32&gt;], [&lt;tf.Tensor 'rpn_model_3/lambda_2/Reshape:0' shape=(?, ?, 2) dtype=float32&gt;, &lt;tf.Tensor 'rpn_model_3/rpn_class_xxx/truediv:0' shape=(?, ?, 2) dtype=float32&gt;, &lt;tf.Tensor 'rpn_model_3/lambda_3/Reshape:0' shape=(?, ?, 4) dtype=float32&gt;], [&lt;tf.Tensor 'rpn_model_4/lambda_2/Reshape:0' shape=(?, ?, 2) dtype=float32&gt;, &lt;tf.Tensor 'rpn_model_4/rpn_class_xxx/truediv:0' shape=(?, ?, 2) dtype=float32&gt;, &lt;tf.Tensor 'rpn_model_4/lambda_3/Reshape:0' shape=(?, ?, 4) dtype=float32&gt;]] 1234567891011121314# Loop through pyramid layers layer_outputs = [] # list of lists for p in rpn_feature_maps: layer_outputs.append(rpn([p]))# Concatenate layer outputs# Convert from list of lists of level outputs to list of lists# of outputs across levels.# e.g. [[a1, b1, c1], [a2, b2, c2]] =&gt; [[a1, a2], [b1, b2], [c1, c2]]output_names = [\"rpn_class_logits\", \"rpn_class\", \"rpn_bbox\"]outputs = list(zip(*layer_outputs))outputs = [KL.Concatenate(axis=1, name=n)(list(o)) for o, n in zip(outputs, output_names)]rpn_class_logits, rpn_class, rpn_bbox = outputs RPN_GRAPHrpn_logits: [batch, H, W, 2] Anchor classifier logits (before softmax)rpn_probs: [batch, W, W, 2] Anchor classifier probabilities.rpn_bbox: [batch, H, W, (dy, dx, log(dh), log(dw))] Deltas to be applied to anchors. 在上方的時候有執行了這行, 三個參數對應到anchor_stride, anchors_per_location, depth, 為啥要這樣包的原因是因為可以用同樣的weight好幾次 12345678910111213141516171819202122232425262728##RPN_ANCHOR_RATIOS = [0.5, 1, 2]##RPN_ANCHOR_STRIDE = 1rpn = build_rpn_model(config.RPN_ANCHOR_STRIDE, len(config.RPN_ANCHOR_RATIOS), 256)Input() is used to instantiate a Keras tensor. # https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/contrib/keras/layers/InputFor instance, if a, b and c and Keras tensors, it becomes possible to do: model = Model(input=[a, b], output=c)def build_rpn_model(anchor_stride, anchors_per_location, depth): \"\"\" Builds a Keras model of the Region Proposal Network. It wraps the RPN graph so it can be used multiple times with shared weights. anchors_per_location: number of anchors per pixel in the feature map anchor_stride: Controls the density of anchors. Typically 1 (anchors for every pixel in the feature map), or 2 (every other pixel). depth: Depth of the backbone feature map. Returns a Keras Model object. The model outputs, when called, are: rpn_logits: [batch, H, W, 2] Anchor classifier logits (before softmax) rpn_probs: [batch, W, W, 2] Anchor classifier probabilities. rpn_bbox: [batch, H, W, (dy, dx, log(dh), log(dw))] Deltas to be applied to anchors. \"\"\" input_feature_map = KL.Input(shape=[None, None, depth], name=\"input_rpn_feature_map\") outputs = rpn_graph(input_feature_map, anchors_per_location, anchor_stride) return KM.Model([input_feature_map], outputs, name=\"rpn_model\") feature map-&gt;Conv(3x3,512)-&gt;RELU 後得到shared feature map接下來就兵分兩路 這邊有點奇怪的是 下面這邊理論上來說應該是要 len(RATIOS)*len(RPN_ANCHOR_SCALES) # 3 x 5但實質上只有3而已 123RPN_ANCHOR_RATIOS = [0.5, 1, 2]RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)anchors_per_location = len(config.RPN_ANCHOR_RATIOS) 計算分數shared feature map-&gt;Conv(1x1,2*3)-&gt;linear_activation接著得到的東西會reshape成2xN的樣式的到rpn_class_logits, 接著才做softmax # Reshape to [batch, anchors, 2] rpn_class_logits = KL.Lambda( lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x) 計算BBOX 偏移shared feature map-&gt;Conv(1x1,4*3)-&gt;linear_activation接著得到的東西會reshape成2xN的樣式的到rpn_class_logits # Reshape to [batch, anchors, 4] rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x) 12345678910111213141516171819202122232425262728293031323334353637383940def rpn_graph(feature_map, anchors_per_location, anchor_stride): \"\"\"Builds the computation graph of Region Proposal Network. feature_map: backbone features [batch, height, width, depth] anchors_per_location: number of anchors per pixel in the feature map anchor_stride: Controls the density of anchors. Typically 1 (anchors for every pixel in the feature map), or 2 (every other pixel). Returns: rpn_logits: [batch, H, W, 2] Anchor classifier logits (before softmax) rpn_probs: [batch, H, W, 2] Anchor classifier probabilities. rpn_bbox: [batch, H, W, (dy, dx, log(dh), log(dw))] Deltas to be applied to anchors. \"\"\" # TODO: check if stride of 2 causes alignment issues if the featuremap # is not even. # Shared convolutional base of the RPN shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu', strides=anchor_stride, name='rpn_conv_shared')(feature_map) # Anchor Score. [batch, height, width, anchors per location * 2]. x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding='valid', activation='linear', name='rpn_class_raw')(shared) # Reshape to [batch, anchors, 2] rpn_class_logits = KL.Lambda( lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x) # Softmax on last dimension of BG/FG. rpn_probs = KL.Activation( \"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits) # Bounding box refinement. [batch, H, W, anchors per location, depth] # where depth is [x, y, log(w), log(h)] x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\", activation='linear', name='rpn_bbox_pred')(shared) # Reshape to [batch, anchors, 4] rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x) return [rpn_class_logits, rpn_probs, rpn_bbox]","tags":[]},{"title":"Mask RCNN Code Reading for ResNet & FPN","date":"2018-05-29T20:54:24.000Z","path":"2018/05/30/Mask-RCNN-Code-Reading-ResNet/","text":"Mask_RCNN V2.1版本 檔案：Model.py Resnet的程式碼是由下面這專案參考改來的https://github.com/fchollet/deep-learning-models/blob/master/resnet50.py identity_block這function主要是為了後面的resnet_graph做準備的, 主要是為了重複利用,對應到的就是Resnet裡面的identity_block,然後這是純粹的Resnet版本,在shortcut上沒有操作 input_tensor: 其實就是接上一層的outputkernel_size : 基本上是3,filters:程式碼中filters通常是一個list, 而通常是三個, 像是[128,128,512],代表三層分別的filter數量stage: 命名用,知道是哪一個stage的block: 命名用,知道是stage中的第幾個idenity_blockuse_bias: conv layer需不需要biastrain_bn: 這個identity_block要freeze還是train 從裡面可以看到一個完整的Identity_BlockInput-&gt;(Conv-&gt;BN-&gt;RELU)-&gt;(Conv-&gt;BN-&gt;RELU)-&gt;(Conv-&gt;BN)-&gt;ADD(prev,Input)-&gt;RELU 123456789101112131415161718192021222324252627282930313233def identity_block(input_tensor, kernel_size, filters, stage, block, use_bias=True, train_bn=True): \"\"\"The identity_block is the block that has no conv layer at shortcut # Arguments input_tensor: input tensor kernel_size: defualt 3, the kernel size of middle conv layer at main path filters: list of integers, the nb_filters of 3 conv layer at main path stage: integer, current stage label, used for generating layer names block: 'a','b'..., current block label, used for generating layer names use_bias: Boolean. To use or not use a bias in conv layers. train_bn: Boolean. Train or freeze Batch Norm layres \"\"\" nb_filter1, nb_filter2, nb_filter3 = filters conv_name_base = 'res' + str(stage) + block + '_branch' bn_name_base = 'bn' + str(stage) + block + '_branch' x = KL.Conv2D(nb_filter1, (1, 1), name=conv_name_base + '2a', use_bias=use_bias)(input_tensor) x = BatchNorm(name=bn_name_base + '2a')(x, training=train_bn) x = KL.Activation('relu')(x) x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same', name=conv_name_base + '2b', use_bias=use_bias)(x) x = BatchNorm(name=bn_name_base + '2b')(x, training=train_bn) x = KL.Activation('relu')(x) x = KL.Conv2D(nb_filter3, (1, 1), name=conv_name_base + '2c', use_bias=use_bias)(x) x = BatchNorm(name=bn_name_base + '2c')(x, training=train_bn) x = KL.Add()([x, input_tensor]) x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x) return x Conv_block這邊就是純粹的Conv_block, input參數與上方相仿, 除了從stage3開始第一個Conv有subsample=(2,2), 而shortcut因此也需要subsample=(2,2) 123456789101112131415161718192021222324252627282930313233343536373839def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2), use_bias=True, train_bn=True): \"\"\"conv_block is the block that has a conv layer at shortcut # Arguments input_tensor: input tensor kernel_size: defualt 3, the kernel size of middle conv layer at main path filters: list of integers, the nb_filters of 3 conv layer at main path stage: integer, current stage label, used for generating layer names block: 'a','b'..., current block label, used for generating layer names use_bias: Boolean. To use or not use a bias in conv layers. train_bn: Boolean. Train or freeze Batch Norm layres Note that from stage 3, the first conv layer at main path is with subsample=(2,2) And the shortcut should have subsample=(2,2) as well \"\"\" nb_filter1, nb_filter2, nb_filter3 = filters conv_name_base = 'res' + str(stage) + block + '_branch' bn_name_base = 'bn' + str(stage) + block + '_branch' x = KL.Conv2D(nb_filter1, (1, 1), strides=strides, name=conv_name_base + '2a', use_bias=use_bias)(input_tensor) x = BatchNorm(name=bn_name_base + '2a')(x, training=train_bn) x = KL.Activation('relu')(x) x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same', name=conv_name_base + '2b', use_bias=use_bias)(x) x = BatchNorm(name=bn_name_base + '2b')(x, training=train_bn) x = KL.Activation('relu')(x) x = KL.Conv2D(nb_filter3, (1, 1), name=conv_name_base + '2c', use_bias=use_bias)(x) x = BatchNorm(name=bn_name_base + '2c')(x, training=train_bn) shortcut = KL.Conv2D(nb_filter3, (1, 1), strides=strides, name=conv_name_base + '1', use_bias=use_bias)(input_tensor) shortcut = BatchNorm(name=bn_name_base + '1')(shortcut, training=train_bn) x = KL.Add()([x, shortcut]) x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x) return x Resnet_GraphResnet主體程式碼 剩下就是call上面的function而ResNet50與ResNet101差異就在於中間使用for來判斷要增加幾個identity_block 123456789101112131415161718192021222324252627282930313233343536def resnet_graph(input_image, architecture, stage5=False, train_bn=True): \"\"\"Build a ResNet graph. architecture: Can be resnet50 or resnet101 stage5: Boolean. If False, stage5 of the network is not created train_bn: Boolean. Train or freeze Batch Norm layres \"\"\" assert architecture in [\"resnet50\", \"resnet101\"] # Stage 1 x = KL.ZeroPadding2D((3, 3))(input_image) x = KL.Conv2D(64, (7, 7), strides=(2, 2), name='conv1', use_bias=True)(x) x = BatchNorm(name='bn_conv1')(x, training=train_bn) x = KL.Activation('relu')(x) C1 = x = KL.MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\")(x) # Stage 2 x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1), train_bn=train_bn) x = identity_block(x, 3, [64, 64, 256], stage=2, block='b', train_bn=train_bn) C2 = x = identity_block(x, 3, [64, 64, 256], stage=2, block='c', train_bn=train_bn) # Stage 3 x = conv_block(x, 3, [128, 128, 512], stage=3, block='a', train_bn=train_bn) x = identity_block(x, 3, [128, 128, 512], stage=3, block='b', train_bn=train_bn) x = identity_block(x, 3, [128, 128, 512], stage=3, block='c', train_bn=train_bn) C3 = x = identity_block(x, 3, [128, 128, 512], stage=3, block='d', train_bn=train_bn) # Stage 4 x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a', train_bn=train_bn) block_count = &#123;\"resnet50\": 5, \"resnet101\": 22&#125;[architecture] for i in range(block_count): x = identity_block(x, 3, [256, 256, 1024], stage=4, block=chr(98 + i), train_bn=train_bn) C4 = x # Stage 5 if stage5: x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a', train_bn=train_bn) x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b', train_bn=train_bn) C5 = x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c', train_bn=train_bn) else: C5 = None return [C1, C2, C3, C4, C5] FPN Feature Pyramid NetworkFPN被寫在了下面 並沒有獨立一個function, 但這邊順便連帶一起講 C5先通過了一個1x1的conv layer調整channel數量,得到P5 1P5 = KL.Conv2D(256, (1, 1), name='fpn_c5p5')(C5) 接下來P5為了要跟P4層進行運算,所以得resize, 這裡用單純的upsampling, 沒有dialated conv或是transposed conv或是bilinear interpolation. 1KL.UpSampling2D(size=(2, 2),name=\"fpn_p5upsampled\")(P5) 所以P4層就是把原本的C4調整過channel數量以後 跟upsamplinge過後的P5相加 123P4 = KL.Add(name=\"fpn_p4add\") ([KL.UpSampling2D(size=(2, 2), name=\"fpn_p5upsampled\")(P5), KL.Conv2D(256,(1, 1), name='fpn_c4p4')(C4)]) FPN完整程式碼如下P6是用來給RPN用的, 不是給FPN用的另外P2~P5又做了一次3x3的conv 是為了消除upsampling的混疊效應 123456789101112131415161718192021222324252627282930# Build the shared convolutional layers.# Bottom-up Layers# Returns a list of the last layers of each stage, 5 in total.# Don't create the thead (stage 5), so we pick the 4th item in the list._, C2, C3, C4, C5 = resnet_graph(input_image, config.BACKBONE, stage5=True, train_bn=config.TRAIN_BN)# Top-down Layers# TODO: add assert to varify feature map sizes match what's in configP5 = KL.Conv2D(256, (1, 1), name='fpn_c5p5')(C5)P4 = KL.Add(name=\"fpn_p4add\")([ KL.UpSampling2D(size=(2, 2), name=\"fpn_p5upsampled\")(P5), KL.Conv2D(256, (1, 1), name='fpn_c4p4')(C4)])P3 = KL.Add(name=\"fpn_p3add\")([ KL.UpSampling2D(size=(2, 2), name=\"fpn_p4upsampled\")(P4), KL.Conv2D(256, (1, 1), name='fpn_c3p3')(C3)])P2 = KL.Add(name=\"fpn_p2add\")([ KL.UpSampling2D(size=(2, 2), name=\"fpn_p3upsampled\")(P3), KL.Conv2D(256, (1, 1), name='fpn_c2p2')(C2)])# Attach 3x3 conv to all P layers to get the final feature maps.P2 = KL.Conv2D(256, (3, 3), padding=\"SAME\", name=\"fpn_p2\")(P2)P3 = KL.Conv2D(256, (3, 3), padding=\"SAME\", name=\"fpn_p3\")(P3)P4 = KL.Conv2D(256, (3, 3), padding=\"SAME\", name=\"fpn_p4\")(P4)P5 = KL.Conv2D(256, (3, 3), padding=\"SAME\", name=\"fpn_p5\")(P5)# P6 is used for the 5th anchor scale in RPN. Generated by# subsampling from P5 with stride of 2.P6 = KL.MaxPooling2D(pool_size=(1, 1), strides=2, name=\"fpn_p6\")(P5)# Note that P6 is used in RPN, but not in the classifier heads.rpn_feature_maps = [P2, P3, P4, P5, P6]mrcnn_feature_maps = [P2, P3, P4, P5]","tags":[]},{"title":"Mask RCNN Code Reading","date":"2018-05-29T20:52:59.000Z","path":"2018/05/30/Mask-RCNN-Code-Reading/","text":"BackBone : ResNet and FPN FPN的code其實作者寫在了model.py的Class MaskRCNN裡面並沒有獨立出來, 而其餘ResNet有 Region Proposal Network: Anchor Boxes Generate 產生anchor boxes的code RPN RPN網路架構的code Proposal Layer 將Anchor Boxes Generate與RPN結合的部分並會經過NMS TASK_HEAD: FPN Classifier Graph 有關於ROI Align Layer還有分類的網路架構的部分 Build FPN Mask Graph 有關於ROI Align Layer還有Mask的網路架構的部分 Detection Layer FPN Classifier Graph後,按照confidence還有數量篩選, 算是後處理了 Detection Target Layer Proposal Layer後,Training的code Mask RCNN Loss有關於所有的loss 如果是training的話 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263if mode == \"training\": # Class ID mask to mark class IDs supported by the dataset the image # came from. active_class_ids = KL.Lambda( lambda x: parse_image_meta_graph(x)[\"active_class_ids\"] )(input_image_meta) if not config.USE_RPN_ROIS: # Ignore predicted ROIs and use ROIs provided as an input. input_rois = KL.Input(shape=[config.POST_NMS_ROIS_TRAINING, 4], name=\"input_roi\", dtype=np.int32) # Normalize coordinates target_rois = KL.Lambda(lambda x: norm_boxes_graph( x, K.shape(input_image)[1:3]))(input_rois) else: target_rois = rpn_rois # Generate detection targets # Subsamples proposals and generates target outputs for training # Note that proposal class IDs, gt_boxes, and gt_masks are zero # padded. Equally, returned rois and targets are zero padded. rois, target_class_ids, target_bbox, target_mask =\\ DetectionTargetLayer(config, name=\"proposal_targets\")([ target_rois, input_gt_class_ids, gt_boxes, input_gt_masks]) # Network Heads # TODO: verify that this handles zero padded ROIs mrcnn_class_logits, mrcnn_class, mrcnn_bbox =\\ fpn_classifier_graph(rois, mrcnn_feature_maps, input_image_meta, config.POOL_SIZE, config.NUM_CLASSES, train_bn=config.TRAIN_BN) mrcnn_mask = build_fpn_mask_graph(rois, mrcnn_feature_maps, input_image_meta, config.MASK_POOL_SIZE, config.NUM_CLASSES, train_bn=config.TRAIN_BN) # TODO: clean up (use tf.identify if necessary) output_rois = KL.Lambda(lambda x: x * 1, name=\"output_rois\")(rois) # Losses rpn_class_loss = KL.Lambda(lambda x: rpn_class_loss_graph(*x), name=\"rpn_class_loss\")( [input_rpn_match, rpn_class_logits]) rpn_bbox_loss = KL.Lambda(lambda x: rpn_bbox_loss_graph(config, *x), name=\"rpn_bbox_loss\")( [input_rpn_bbox, input_rpn_match, rpn_bbox]) class_loss = KL.Lambda(lambda x: mrcnn_class_loss_graph(*x), name=\"mrcnn_class_loss\")( [target_class_ids, mrcnn_class_logits, active_class_ids]) bbox_loss = KL.Lambda(lambda x: mrcnn_bbox_loss_graph(*x), name=\"mrcnn_bbox_loss\")( [target_bbox, target_class_ids, mrcnn_bbox]) mask_loss = KL.Lambda(lambda x: mrcnn_mask_loss_graph(*x), name=\"mrcnn_mask_loss\")( [target_mask, target_class_ids, mrcnn_mask]) # Model inputs = [input_image, input_image_meta, input_rpn_match, input_rpn_bbox, input_gt_class_ids, input_gt_boxes, input_gt_masks] if not config.USE_RPN_ROIS: inputs.append(input_rois) outputs = [rpn_class_logits, rpn_class, rpn_bbox, mrcnn_class_logits, mrcnn_class, mrcnn_bbox, mrcnn_mask, rpn_rois, output_rois, rpn_class_loss, rpn_bbox_loss, class_loss, bbox_loss, mask_loss] model = KM.Model(inputs, outputs, name='mask_rcnn') 如果是測試的話 else: # Network Heads # Proposal classifier and BBox regressor heads mrcnn_class_logits, mrcnn_class, mrcnn_bbox =\\ fpn_classifier_graph(rpn_rois, mrcnn_feature_maps, input_image_meta, config.POOL_SIZE, config.NUM_CLASSES, train_bn=config.TRAIN_BN) # Detections # output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in # normalized coordinates detections = DetectionLayer(config, name=\"mrcnn_detection\")( [rpn_rois, mrcnn_class, mrcnn_bbox, input_image_meta]) # Create masks for detections detection_boxes = KL.Lambda(lambda x: x[..., :4])(detections) mrcnn_mask = build_fpn_mask_graph(detection_boxes, mrcnn_feature_maps, input_image_meta, config.MASK_POOL_SIZE, config.NUM_CLASSES, train_bn=config.TRAIN_BN) model = KM.Model([input_image, input_image_meta, input_anchors], [detections, mrcnn_class, mrcnn_bbox, mrcnn_mask, rpn_rois, rpn_class, rpn_bbox], name='mask_rcnn') 以下是別人網誌上的舊圖 不太依樣了 build_classfier_grpah拿掉了 上圖是inference下圖是train","tags":[]},{"title":"Optimize /","date":"2018-04-07T13:15:01.000Z","path":"2018/04/07/Optimize/","text":"","tags":[]},{"title":"Deep Residual Network","date":"2018-04-07T13:15:01.000Z","path":"2018/04/07/Resnet-Deep-Residual-Network/","text":"","tags":[{"name":"CNN","slug":"CNN","permalink":"https://world4jason.github.io/tags/CNN/"},{"name":"Model Architecture","slug":"Model-Architecture","permalink":"https://world4jason.github.io/tags/Model-Architecture/"},{"name":"Convolution","slug":"Convolution","permalink":"https://world4jason.github.io/tags/Convolution/"},{"name":"Classification","slug":"Classification","permalink":"https://world4jason.github.io/tags/Classification/"}]},{"title":"VGG-VERY DEEP CONVOLUTIONAL NETWORK SFOR LARGE-SCALE IMAGE RECOGNITION","date":"2018-04-07T13:05:01.000Z","path":"2018/04/07/VGG-VERY-DEEP-CONVOLUTIONAL-NETWORK-SFOR-LARGE-SCALE-IMAGE-RECOGNITION/","text":"前言這篇的摘要寫說針對AlexNet的基礎上主要做了兩方面的改進：1.使用了最小的3x3卷積核尺寸和最小間隔。 在整個圖片和multi-scale上訓練和測試圖片。論文原句：1. Use smaller receptive window size and smaller stride of the first convolutional layer.2.Training and testing the networks densely over the whole image and over multiple scales. 1AlexNet做改進, 所以VGG本身的問題並不會在這篇討論, 會在ResNet那篇再探討。 架構 採用較小的Filter尺寸-3x3，卷積的間隔s=1:1：3x3是最小的能夠捕獲上下左右和中心概念的尺寸。2：兩個3x3的捲基層的有限感受野是5x5；三個3x3的感受野是7x7，可以替代大的filter尺寸。3：多個3x3的捲基層比一個大尺寸filter卷基層有更多的非線性，使得判決函數更加具有判決性。4：多個3x3的捲積層比一個大尺寸的filter有更少的參數，假設卷基層的輸入和輸出的特徵圖大小相同為C，那麼三個3x3的捲積層參數個數3x（3x3xCxC）=27CC；一個7x7的捲積層參數為49CC；所以可以把三個3x3的filter看成是一個7x7filter的分解（中間層有非線性的分解）。也使用過1x1 filter:作用是在不影響輸入輸出維數的情況下，對輸入進行線性形變，然後通過Relu進行非線性處理，增加網絡的非線性表達能力。Max-Pooling：2x2，間隔s=2； 論文中解釋關於僅使用3x3 conv kernel，因為兩個3x3的conv kernel疊合的reception field等效於一個5x5 conv kernel(亦即每個pixel可以correlate到周圍的5x5個pixel), 而三個3x3則可以等效於一個7x7，但兩層3x3的參數量僅有一層5x5的(3x3x2)/(5x5) = 0.72倍，而三層3x3參數量是一層7x7的(3x3x3)/(7x7)=0.55倍，對應到的範圍等效並且可使得需參數量更少，並且疊越多層Conv+ReLU的特徵學習能力比單一層Conv+ReLU來的更好。 Filter Size相對於AlexNet 每個Stage僅含有一個Conv層, Filter 7x7, VGG每個Stage有2~4個Conv層,而Filter只有3x3, 原論文內”This can be seen as imposing a regularization on the 7 × 7 conv. filters, forcing them to have a decomposition through the 3 × 3 filters”他說7x7 filter可以被分解成若干个3x3的filter的叠加。類比一下n維空間的向量x，x的正交分解x = x1(1, 0, 0, ….) + x2(0, 1, 0, …) + x3(0, 0, 1,…) + … + xn(0, 0, 0, …, 1) 每一組的每一層的filter被類比成n維歐幾里得空間的基底。若VGG的一組含有3層3x3的filter，則我們則假設一個7x7的filter可以被分解成3種“正交”的3x3的filter。 作者原文：First, we incorporate three non-linearrectification layers instead of a single one, which makes the decision function more discriminative.Second, we decrease the number of parameters: assuming that both the input and the output of athree-layer 3 × 3 convolution stack has C channels, the stack is parametrised by 3 32C^2 = 27C^2weights; at the same time, a single 7 × 7 conv. layer would require 72C^2 = 49C^2 後來googleNet將3×3的卷積分解成31和13的卷積，可以減少33％的計算量，如果將3×3分解為兩個2×2的，可以減少11％的計算量，而且利用非對稱卷積的效果還更好。實踐表明，不要過早的使用這種分解操作，在特徵映射大小為（12〜20）之間，使用它，效果是比較好的。","tags":[{"name":"CNN","slug":"CNN","permalink":"https://world4jason.github.io/tags/CNN/"},{"name":"Model Architecture","slug":"Model-Architecture","permalink":"https://world4jason.github.io/tags/Model-Architecture/"},{"name":"Convolution","slug":"Convolution","permalink":"https://world4jason.github.io/tags/Convolution/"},{"name":"Classification","slug":"Classification","permalink":"https://world4jason.github.io/tags/Classification/"}]},{"title":"CNN模型壓縮系列","date":"2018-04-07T13:05:01.000Z","path":"2018/04/07/Model-Compression/","text":"網絡修剪網絡修剪，採用當網絡權重非常小的時候（小於某個設定的閾值），把它置0，就像二值網絡一般;然後屏蔽被設置為0的權重更新，繼續進行訓練;以此循環，每隔訓練幾輪過後，繼續進行修剪。例如Deep-Compression這篇paper 權重共享對於每一層的參數，我們進行k-均值聚類，進行量化，對於歸屬於同一個聚類中心的權重，採用共享一個權重，進行重新訓練。需要注意的是這個權重共享並不是層之間的權重共享，這是對於每一層的單獨共享 增加L2權重增加L2權重可以讓更多的權重，靠近0，這樣每次修剪的比例大大增加。 從結構上，簡化網絡計算，這些需要自己閱讀比較多相關文獻，才能設計出合理，速度更快的網絡，比如引入消防模塊，NIN，除全連接層等一些設計思想，這邊不進行具體詳述。SqueezeNet 其他Huffman Coding","tags":[{"name":"CNN","slug":"CNN","permalink":"https://world4jason.github.io/tags/CNN/"},{"name":"Model Architecture","slug":"Model-Architecture","permalink":"https://world4jason.github.io/tags/Model-Architecture/"},{"name":"Convolution","slug":"Convolution","permalink":"https://world4jason.github.io/tags/Convolution/"},{"name":"Model Compression","slug":"Model-Compression","permalink":"https://world4jason.github.io/tags/Model-Compression/"}]},{"title":"Instance Segmentation Series","date":"2018-03-16T05:53:47.000Z","path":"2018/03/16/2018-03-27/","text":"MNCFCISMask R-CNNPANet","tags":[]},{"title":"Light Head R-CNN","date":"2018-03-16T05:53:47.000Z","path":"2018/03/16/Light Head R-CNN/","text":"旷视face++研究院解读Light-Head R-CNN","tags":[]},{"title":"Tensorflow Function Usage And Example","date":"2018-03-16T05:53:47.000Z","path":"2018/03/16/Tensorflow-Function-Usage-And-Example/","text":"Mini Tensorflow example12345678import tensorflow as tf a = tf.placeholder(\"float\") b = tf.placeholder(\"float\") y = tf.mul(a, b) sess = tf.Session()print sess.run(y, feed_dict=&#123;a: 3, b: 3&#125;) sess.close() 操作組 操作 Maths Add, Sub, Mul, Div, Exp, Log, Greater, Less, Equal Array Concat, Slice, Split, Constant, Rank, Shape, Shuffle Matrix MatMul, MatrixInverse, MatrixDeterminant Neuronal Network SoftMax, Sigmoid, ReLU, Convolution2D, MaxPool Checkpointing Save, Restore Queues and syncronizations Enqueue, Dequeue, MutexAcquire, MutexRelease Flow control Merge, Switch, Enter, Leave, NextIteration TensorFlow的算術操作如下： 操作組 操作 tf.add(x, y, name=None) 求和 tf.sub(x, y, name=None) 減法 tf.mul(x, y, name=None) 乘法 tf.div(x, y, name=None) 除法 tf.mod(x, y, name=None) 取模 tf.abs(x, name=None) 求絕對值 tf.neg(x, name=None) 取負 (y = -x). tf.sign(x, name=None) 返回符號 y = sign(x) = -1 if x &lt; 0; 0 if x == 0; 1 if x &gt; 0. tf.inv(x, name=None) 取反 tf.square(x, name=None) 計算平方 (y = x * x = x^2). tf.round(x, name=None) 舍入最接近的整數 # ‘a’ is [0.9, 2.5, 2.3, -4.4] tf.round(a) ==&gt; [ 1.0, 3.0, 2.0, -4.0 ] tf.sqrt(x, name=None) 開根號 (y = \\sqrt{x} = x^{1/2}). tf.pow(x, y, name=None) 冪次方 # tensor ‘x’ is [[2, 2], [3, 3]] # tensor ‘y’ is [[8, 16], [2, 3]] tf.pow(x, y) ==&gt; [[256, 65536], [9, 27]] tf.exp(x, name=None) 計算e的次方 tf.log(x, name=None) 計算log，一個輸入計算e的ln，兩輸入以第二輸入為底 tf.maximum(x, y, name=None) 返回最大值 (x &gt; y ? x : y) tf.minimum(x, y, name=None) 返回最小值 (x &lt; y ? x : y) tf.cos(x, name=None) 三角函數cosine tf.sin(x, name=None) 三角函數sine tf.tan(x, name=None) 三角函數tan tf.atan(x, name=None) 三角函數ctan 張量操作Tensor Transformations####數據類型轉換Casting| 操作組 | 操作 ||—————————-|——————————————————|| tf.string_to_number | || (string_tensor, out_type=None, name=None) | 字符串轉為數字 || tf.to_double(x, name=’ToDouble’) | 轉為64位浮點類型–float64 || tf.to_float(x, name=’ToFloat’) | 轉為32位浮點類型–float32 || tf.to_int32(x, name=’ToInt32’) | 轉為32位整型–int32 || tf.to_int64(x, name=’ToInt64’) | 轉為64位整型–int64 || tf.cast(x, dtype, name=None) | 將x或者x.values轉換為dtype || # tensor a is [1.8, 2.2], dtype=tf.float | || tf.cast(a, tf.int32) ==&gt; [1, 2] # dtype=tf.int32 | | 形狀操作Shapes and Shaping 操作組 操作 tf.shape(input, name=None) 返回數據的shape # ‘t’ is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]] shape(t) ==&gt; [2, 2, 3] tf.size(input, name=None) 返回數據的元素數量 # ‘t’ is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]] size(t) ==&gt; 12 tf.rank(input, name=None) 返回tensor的rank 注意：此rank不同於矩陣的rank， tensor的rank表示一個tensor需要的索引數目來唯一表示任何一個元素 也就是通常所説的 “order”, “degree”或”ndims” #’t’ is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]] # shape of tensor ‘t’ is [2, 2, 3] rank(t) ==&gt; 3 tf.reshape(tensor, shape, name=None) 改變tensor的形狀 # tensor ‘t’ is [1, 2, 3, 4, 5, 6, 7, 8, 9] # tensor ‘t’ has shape [9] reshape(t, [3, 3]) ==&gt; [[1, 2, 3], [4, 5, 6], [7, 8, 9]] #如果shape有元素[-1],表示在該維度打平至一維 # -1 將自動推導得為 9: reshape(t, [2, -1]) ==&gt; [[1, 1, 1, 2, 2, 2, 3, 3, 3], [4, 4, 4, 5, 5, 5, 6, 6, 6]] tf.expand_dims(input, dim, name=None) 插入維度1進入一個tensor中 #該操作要求-1-input.dims() # ‘t’ is a tensor of shape [2] shape(expand_dims(t, 0)) ==&gt; [1, 2] shape(expand_dims(t, 1)) ==&gt; [2, 1] shape(expand_dims(t, -1)) ==&gt; [2, 1] &lt;= dim &lt;= input.dims() https://hk.saowen.com/a/2766b13f38b54ab09e6d478975f5feca8abbf01d842e2ca8f5111160fcbb0e36 tf.reshapetf.reshape(tensor,shape, name=None) 重組 12345678910# tensor 't' is [1, 2, 3, 4, 5, 6, 7, 8, 9]# tensor 't'has shape [9]reshape(t, [3, 3]) ==&gt; [[1, 2, 3],[4, 5, 6],[7, 8, 9]] 降為 12345678910# tensor 't'is [[[1, 1], [2, 2]], [[3, 3], [4, 4]]]# tensor 't'has shape [2, 2, 2]reshape(t, [2, 4]) ==&gt; [[1, 1, 2, 2],[3, 3, 4, 4]] 平坦 12345678# tensor 't' has shape [3, 2, 3]# pass '[-1]' to flatten 't'reshape(t, [-1]) ==&gt; [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6] tf.splittf.split(split_dim, num_split, value, name=’split’)將大的tensor分割成更小的tensor，第一個參數代表沿著那一維開始分割，第二個參數代表切成幾段 tf.nn.max_poolmax pooling 是CNN 當中的最大值池化操作，其實用法和卷積很類似 tf.nn.max_pool(value,ksize, strides, padding, name=None) 12345678910參數是四個，和卷積很類似：第一個參數value ：需要池化的輸入，一般池化層接在卷積層後面，所以輸入通常是feature map ，依然是[batch, height, width, channels] 這樣的shape第二個參數ksize ：池化窗口的大小，取一個四維向量，一般是[1, height, width, 1] ，因為我們不想在batch 和channels 上做池化，所以這兩個維度設為了1第三個參數strides ：和卷積類似，窗口在每一個維度上滑動的步長，一般也是[1, stride, stride ,1]第四個參數padding ：和卷積類似，可以取'VALID' 或者'SAME'返回一個Tensor ，類型不變，shape 仍然是[batch, height, width, channels] 這種形式 假設有這樣一張圖，雙通道第一個通道： 第二個通道： 1234567891011121314151617181920212223242526272829303132import tensorflow as tfa=tf.constant([ [[1.0,2.0,3.0,4.0], [5.0,6.0,7.0,8.0], [8.0,7.0,6.0,5.0], [4.0,3.0,2.0,1.0]], [[4.0,3.0,2.0,1.0], [8.0,7.0,6.0,5.0], [1.0,2.0,3.0,4.0], [5.0,6.0,7.0,8.0]] ])a=tf.reshape(a,[1,4,4,2])pooling=tf.nn.max_pool(a,[1,2,2,1],[1,1,1,1],padding='VALID')with tf.Session() as sess: print(\"image:\") image=sess.run(a) print (image) print(\"reslut:\") result=sess.run(pooling) print (result) 這裡步長為1 ，窗口大小2×2 ，輸出結果： image: 1234567891011121314151617181920212223242526272829303132333435[[[[ 1. 2.][ 3. 4.][ 5. 6.][ 7. 8.]][[ 8. 7.][ 6. 5.][ 4. 3.][ 2. 1.]][[ 4. 3.][ 2. 1.][ 8. 7.][ 6. 5.]][[ 1. 2.][ 3. 4.][ 5. 6.][ 7. 8.]]]]reslut:[[[[ 8. 7.][ 6. 6.][ 7. 8.]][[ 8. 7.][ 8. 7.][ 8. 7.]][[ 4. 4.][ 8. 7.][ 8. 8.]]]] 池化後的圖就是： 證明了程序的結果是正確的。我們還可以改變步長 123456789pooling=tf.nn.max_pool(a,[1,2,2,1],[1,2,2,1],padding='VALID')最後的result 就變成：reslut:[[[[ 8. 7.][ 7. 8.]][[ 4. 4.][ 8. 8.]]]]","tags":[]},{"title":"Mask R-CNN","date":"2018-03-09T18:13:59.000Z","path":"2018/03/10/Mask R-CNN/","text":"Overview ConceptMask R-CNN 利用了相當簡潔與彈性的方法進行實例分割, 主要跟 Faster R-CNN 不同的地方在於原架構有 2 個分支 Classification branch Bounding box branch 而 Mask R-CNN 的方法則是多加了另一個分支 — Mask branch。 Mask branchLoss functionLtotal = Lcls + Lbox + Lmask Lcls 跟 Lbox的可以參考Fast-RCNN 1J(θ)=−1m[∑mi=1y(i)loghθ(x(i))+(1−y(i))log(1−hθ(x(i)))] ROI AlignmentROI POOLINGROI POOLING 概念如下圖所示 1\\mathcal &#123;F_&#123;i&#125;^&#123;RoI&#125;&#125;_&#123;(u^\\prime,v^\\prime)&#125; = \\sum_&#123;(u,v)&#125;^&#123;W \\times H&#125; G(u,v;u^\\prime, v^\\prime|B_i) \\mathcal F_(u,v)","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://world4jason.github.io/tags/Deep-Learning/"},{"name":"R-CNN","slug":"R-CNN","permalink":"https://world4jason.github.io/tags/R-CNN/"},{"name":"Segmentation","slug":"Segmentation","permalink":"https://world4jason.github.io/tags/Segmentation/"},{"name":"Pose Estimation","slug":"Pose-Estimation","permalink":"https://world4jason.github.io/tags/Pose-Estimation/"}]},{"title":"Mask R-CNN","date":"2018-03-09T18:13:59.000Z","path":"2018/03/10/Mask R-CNN Code Reading/","text":"https://github.com/matterport/Mask_RCNN #model.py import 需要的東西 tf ver&gt;1.3 &amp;&amp; keras ver&gt;2.0.8 12345678910111213141516171819202122232425262728import osimport sysimport globimport randomimport mathimport datetimeimport itertoolsimport jsonimport reimport loggingfrom collections import OrderedDictimport multiprocessingimport numpy as npimport skimage.transformimport tensorflow as tfimport kerasimport keras.backend as Kimport keras.layers as KLimport keras.initializers as KIimport keras.engine as KEimport keras.models as KMimport utils# Requires TensorFlow 1.3+ and Keras 2.0.8+.from distutils.version import LooseVersionassert LooseVersion(tf.__version__) &gt;= LooseVersion(\"1.3\")assert LooseVersion(keras.__version__) &gt;= LooseVersion('2.0.8') 封裝Mask R-CNN 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744class MaskRCNN(): \"\"\"Encapsulates the Mask RCNN model functionality. The actual Keras model is in the keras_model property. \"\"\" ## Config 設定是由外部傳入，像是ballon.py裡面有關於batch_size、Iteration等等，詳細內容去看config.py def __init__(self, mode, config, model_dir): \"\"\" mode: Either \"training\" or \"inference\" config: A Sub-class of the Config class model_dir: Directory to save training logs and trained weights \"\"\" assert mode in ['training', 'inference'] self.mode = mode self.config = config self.model_dir = model_dir self.set_log_dir() self.keras_model = self.build(mode=mode, config=config) def build(self, mode, config): \"\"\"Build Mask R-CNN architecture. input_shape: The shape of the input image. mode: Either \"training\" or \"inference\". The inputs and outputs of the model differ accordingly. \"\"\" assert mode in ['training', 'inference'] # 強迫要求一定要是 長寬一定要是32的倍數，不然downscaling跟upscaling會有問題 # 尤其是convultion到後面Resnet是縮放了32倍 # Image size must be dividable by 2 multiple times h, w = config.IMAGE_SHAPE[:2] if h / 2**6 != int(h / 2**6) or w / 2**6 != int(w / 2**6): raise Exception(\"Image size must be dividable by 2 at least 6 times \" \"to avoid fractions when downscaling and upscaling.\" \"For example, use 256, 320, 384, 448, 512, ... etc. \") # config.IMAGE_SHAPE.tolist() = [1024,1024,3] # input_image_meta有點意義不明 # 此段落是建立需要的輸入，都用KL.Input來轉化 # Inputs input_image = KL.Input( shape=config.IMAGE_SHAPE.tolist(), name=\"input_image\") input_image_meta = KL.Input(shape=[None], name=\"input_image_meta\") if mode == \"training\": # RPN GT input_rpn_match = KL.Input( shape=[None, 1], name=\"input_rpn_match\", dtype=tf.int32) input_rpn_bbox = KL.Input( shape=[None, 4], name=\"input_rpn_bbox\", dtype=tf.float32) # Detection GT (class IDs, bounding boxes, and masks) # 1. GT Class IDs (zero padded) input_gt_class_ids = KL.Input( shape=[None], name=\"input_gt_class_ids\", dtype=tf.int32) # 2. GT Boxes in pixels (zero padded) # [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)] in image coordinates input_gt_boxes = KL.Input( shape=[None, 4], name=\"input_gt_boxes\", dtype=tf.float32) # Normalize coordinates h, w = K.shape(input_image)[1], K.shape(input_image)[2] image_scale = K.cast(K.stack([h, w, h, w], axis=0), tf.float32) gt_boxes = KL.Lambda(lambda x: x / image_scale)(input_gt_boxes) # 3. GT Masks (zero padded) # [batch, height, width, MAX_GT_INSTANCES] if config.USE_MINI_MASK: input_gt_masks = KL.Input( shape=[config.MINI_MASK_SHAPE[0], config.MINI_MASK_SHAPE[1], None], name=\"input_gt_masks\", dtype=bool) else: input_gt_masks = KL.Input( shape=[config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1], None], name=\"input_gt_masks\", dtype=bool) # Build the shared convolutional layers. # Bottom-up Layers # Returns a list of the last layers of each stage, 5 in total. # Don't create the thead (stage 5), so we pick the 4th item in the list. _, C2, C3, C4, C5 = resnet_graph(input_image, config.BACKBONE, stage5=True) # Top-down Layers # TODO: add assert to varify feature map sizes match what's in config P5 = KL.Conv2D(256, (1, 1), name='fpn_c5p5')(C5) P4 = KL.Add(name=\"fpn_p4add\")([ KL.UpSampling2D(size=(2, 2), name=\"fpn_p5upsampled\")(P5), KL.Conv2D(256, (1, 1), name='fpn_c4p4')(C4)]) P3 = KL.Add(name=\"fpn_p3add\")([ KL.UpSampling2D(size=(2, 2), name=\"fpn_p4upsampled\")(P4), KL.Conv2D(256, (1, 1), name='fpn_c3p3')(C3)]) P2 = KL.Add(name=\"fpn_p2add\")([ KL.UpSampling2D(size=(2, 2), name=\"fpn_p3upsampled\")(P3), KL.Conv2D(256, (1, 1), name='fpn_c2p2')(C2)]) # Attach 3x3 conv to all P layers to get the final feature maps. P2 = KL.Conv2D(256, (3, 3), padding=\"SAME\", name=\"fpn_p2\")(P2) P3 = KL.Conv2D(256, (3, 3), padding=\"SAME\", name=\"fpn_p3\")(P3) P4 = KL.Conv2D(256, (3, 3), padding=\"SAME\", name=\"fpn_p4\")(P4) P5 = KL.Conv2D(256, (3, 3), padding=\"SAME\", name=\"fpn_p5\")(P5) # P6 is used for the 5th anchor scale in RPN. Generated by # subsampling from P5 with stride of 2. P6 = KL.MaxPooling2D(pool_size=(1, 1), strides=2, name=\"fpn_p6\")(P5) # Note that P6 is used in RPN, but not in the classifier heads. rpn_feature_maps = [P2, P3, P4, P5, P6] mrcnn_feature_maps = [P2, P3, P4, P5] # Generate Anchors self.anchors = utils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES, config.RPN_ANCHOR_RATIOS, config.BACKBONE_SHAPES, config.BACKBONE_STRIDES, config.RPN_ANCHOR_STRIDE) # RPN Model rpn = build_rpn_model(config.RPN_ANCHOR_STRIDE, len(config.RPN_ANCHOR_RATIOS), 256) # Loop through pyramid layers layer_outputs = [] # list of lists for p in rpn_feature_maps: layer_outputs.append(rpn([p])) # Concatenate layer outputs # Convert from list of lists of level outputs to list of lists # of outputs across levels. # e.g. [[a1, b1, c1], [a2, b2, c2]] =&gt; [[a1, a2], [b1, b2], [c1, c2]] output_names = [\"rpn_class_logits\", \"rpn_class\", \"rpn_bbox\"] outputs = list(zip(*layer_outputs)) outputs = [KL.Concatenate(axis=1, name=n)(list(o)) for o, n in zip(outputs, output_names)] rpn_class_logits, rpn_class, rpn_bbox = outputs # Generate proposals # Proposals are [batch, N, (y1, x1, y2, x2)] in normalized coordinates # and zero padded. proposal_count = config.POST_NMS_ROIS_TRAINING if mode == \"training\"\\ else config.POST_NMS_ROIS_INFERENCE rpn_rois = ProposalLayer(proposal_count=proposal_count, nms_threshold=config.RPN_NMS_THRESHOLD, name=\"ROI\", anchors=self.anchors, config=config)([rpn_class, rpn_bbox]) if mode == \"training\": # Class ID mask to mark class IDs supported by the dataset the image # came from. _, _, _, active_class_ids = KL.Lambda(lambda x: parse_image_meta_graph(x), mask=[None, None, None, None])(input_image_meta) if not config.USE_RPN_ROIS: # Ignore predicted ROIs and use ROIs provided as an input. input_rois = KL.Input(shape=[config.POST_NMS_ROIS_TRAINING, 4], name=\"input_roi\", dtype=np.int32) # Normalize coordinates to 0-1 range. target_rois = KL.Lambda(lambda x: K.cast( x, tf.float32) / image_scale[:4])(input_rois) else: target_rois = rpn_rois # Generate detection targets # Subsamples proposals and generates target outputs for training # Note that proposal class IDs, gt_boxes, and gt_masks are zero # padded. Equally, returned rois and targets are zero padded. rois, target_class_ids, target_bbox, target_mask =\\ DetectionTargetLayer(config, name=\"proposal_targets\")([ target_rois, input_gt_class_ids, gt_boxes, input_gt_masks]) # Network Heads # TODO: verify that this handles zero padded ROIs mrcnn_class_logits, mrcnn_class, mrcnn_bbox =\\ fpn_classifier_graph(rois, mrcnn_feature_maps, config.IMAGE_SHAPE, config.POOL_SIZE, config.NUM_CLASSES) mrcnn_mask = build_fpn_mask_graph(rois, mrcnn_feature_maps, config.IMAGE_SHAPE, config.MASK_POOL_SIZE, config.NUM_CLASSES) # TODO: clean up (use tf.identify if necessary) output_rois = KL.Lambda(lambda x: x * 1, name=\"output_rois\")(rois) # Losses rpn_class_loss = KL.Lambda(lambda x: rpn_class_loss_graph(*x), name=\"rpn_class_loss\")( [input_rpn_match, rpn_class_logits]) rpn_bbox_loss = KL.Lambda(lambda x: rpn_bbox_loss_graph(config, *x), name=\"rpn_bbox_loss\")( [input_rpn_bbox, input_rpn_match, rpn_bbox]) class_loss = KL.Lambda(lambda x: mrcnn_class_loss_graph(*x), name=\"mrcnn_class_loss\")( [target_class_ids, mrcnn_class_logits, active_class_ids]) bbox_loss = KL.Lambda(lambda x: mrcnn_bbox_loss_graph(*x), name=\"mrcnn_bbox_loss\")( [target_bbox, target_class_ids, mrcnn_bbox]) mask_loss = KL.Lambda(lambda x: mrcnn_mask_loss_graph(*x), name=\"mrcnn_mask_loss\")( [target_mask, target_class_ids, mrcnn_mask]) # Model inputs = [input_image, input_image_meta, input_rpn_match, input_rpn_bbox, input_gt_class_ids, input_gt_boxes, input_gt_masks] if not config.USE_RPN_ROIS: inputs.append(input_rois) outputs = [rpn_class_logits, rpn_class, rpn_bbox, mrcnn_class_logits, mrcnn_class, mrcnn_bbox, mrcnn_mask, rpn_rois, output_rois, rpn_class_loss, rpn_bbox_loss, class_loss, bbox_loss, mask_loss] model = KM.Model(inputs, outputs, name='mask_rcnn') else: # Network Heads # Proposal classifier and BBox regressor heads mrcnn_class_logits, mrcnn_class, mrcnn_bbox =\\ fpn_classifier_graph(rpn_rois, mrcnn_feature_maps, config.IMAGE_SHAPE, config.POOL_SIZE, config.NUM_CLASSES) # Detections # output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in image coordinates detections = DetectionLayer(config, name=\"mrcnn_detection\")( [rpn_rois, mrcnn_class, mrcnn_bbox, input_image_meta]) # Convert boxes to normalized coordinates # TODO: let DetectionLayer return normalized coordinates to avoid # unnecessary conversions h, w = config.IMAGE_SHAPE[:2] detection_boxes = KL.Lambda( lambda x: x[..., :4] / np.array([h, w, h, w]))(detections) # Create masks for detections mrcnn_mask = build_fpn_mask_graph(detection_boxes, mrcnn_feature_maps, config.IMAGE_SHAPE, config.MASK_POOL_SIZE, config.NUM_CLASSES) model = KM.Model([input_image, input_image_meta], [detections, mrcnn_class, mrcnn_bbox, mrcnn_mask, rpn_rois, rpn_class, rpn_bbox], name='mask_rcnn') # Add multi-GPU support. if config.GPU_COUNT &gt; 1: from parallel_model import ParallelModel model = ParallelModel(model, config.GPU_COUNT) return model def find_last(self): \"\"\"Finds the last checkpoint file of the last trained model in the model directory. Returns: log_dir: The directory where events and weights are saved checkpoint_path: the path to the last checkpoint file \"\"\" # Get directory names. Each directory corresponds to a model dir_names = next(os.walk(self.model_dir))[1] key = self.config.NAME.lower() dir_names = filter(lambda f: f.startswith(key), dir_names) dir_names = sorted(dir_names) if not dir_names: return None, None # Pick last directory dir_name = os.path.join(self.model_dir, dir_names[-1]) # Find the last checkpoint checkpoints = next(os.walk(dir_name))[2] checkpoints = filter(lambda f: f.startswith(\"mask_rcnn\"), checkpoints) checkpoints = sorted(checkpoints) if not checkpoints: return dir_name, None checkpoint = os.path.join(dir_name, checkpoints[-1]) return dir_name, checkpoint def load_weights(self, filepath, by_name=False, exclude=None): \"\"\"Modified version of the correspoding Keras function with the addition of multi-GPU support and the ability to exclude some layers from loading. exlude: list of layer names to excluce \"\"\" import h5py from keras.engine import topology if exclude: by_name = True if h5py is None: raise ImportError('`load_weights` requires h5py.') f = h5py.File(filepath, mode='r') if 'layer_names' not in f.attrs and 'model_weights' in f: f = f['model_weights'] # In multi-GPU training, we wrap the model. Get layers # of the inner model because they have the weights. keras_model = self.keras_model layers = keras_model.inner_model.layers if hasattr(keras_model, \"inner_model\")\\ else keras_model.layers # Exclude some layers if exclude: layers = filter(lambda l: l.name not in exclude, layers) if by_name: topology.load_weights_from_hdf5_group_by_name(f, layers) else: topology.load_weights_from_hdf5_group(f, layers) if hasattr(f, 'close'): f.close() # Update the log directory self.set_log_dir(filepath) def get_imagenet_weights(self): \"\"\"Downloads ImageNet trained weights from Keras. Returns path to weights file. \"\"\" from keras.utils.data_utils import get_file TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/'\\ 'releases/download/v0.2/'\\ 'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5' weights_path = get_file('resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5', TF_WEIGHTS_PATH_NO_TOP, cache_subdir='models', md5_hash='a268eb855778b3df3c7506639542a6af') return weights_path def compile(self, learning_rate, momentum): \"\"\"Gets the model ready for training. Adds losses, regularization, and metrics. Then calls the Keras compile() function. \"\"\" # Optimizer object optimizer = keras.optimizers.SGD(lr=learning_rate, momentum=momentum, clipnorm=self.config.GRADIENT_CLIP_NORM) # Add Losses # First, clear previously set losses to avoid duplication self.keras_model._losses = [] self.keras_model._per_input_losses = &#123;&#125; loss_names = [\"rpn_class_loss\", \"rpn_bbox_loss\", \"mrcnn_class_loss\", \"mrcnn_bbox_loss\", \"mrcnn_mask_loss\"] for name in loss_names: layer = self.keras_model.get_layer(name) if layer.output in self.keras_model.losses: continue self.keras_model.add_loss( tf.reduce_mean(layer.output, keep_dims=True)) # Add L2 Regularization # Skip gamma and beta weights of batch normalization layers. reg_losses = [keras.regularizers.l2(self.config.WEIGHT_DECAY)(w) / tf.cast(tf.size(w), tf.float32) for w in self.keras_model.trainable_weights if 'gamma' not in w.name and 'beta' not in w.name] self.keras_model.add_loss(tf.add_n(reg_losses)) # Compile self.keras_model.compile(optimizer=optimizer, loss=[ None] * len(self.keras_model.outputs)) # Add metrics for losses for name in loss_names: if name in self.keras_model.metrics_names: continue layer = self.keras_model.get_layer(name) self.keras_model.metrics_names.append(name) self.keras_model.metrics_tensors.append(tf.reduce_mean( layer.output, keep_dims=True)) def set_trainable(self, layer_regex, keras_model=None, indent=0, verbose=1): \"\"\"Sets model layers as trainable if their names match the given regular expression. \"\"\" # Print message on the first call (but not on recursive calls) if verbose &gt; 0 and keras_model is None: log(\"Selecting layers to train\") keras_model = keras_model or self.keras_model # In multi-GPU training, we wrap the model. Get layers # of the inner model because they have the weights. layers = keras_model.inner_model.layers if hasattr(keras_model, \"inner_model\")\\ else keras_model.layers for layer in layers: # Is the layer a model? if layer.__class__.__name__ == 'Model': print(\"In model: \", layer.name) self.set_trainable( layer_regex, keras_model=layer, indent=indent + 4) continue if not layer.weights: continue # Is it trainable? trainable = bool(re.fullmatch(layer_regex, layer.name)) # Update layer. If layer is a container, update inner layer. if layer.__class__.__name__ == 'TimeDistributed': layer.layer.trainable = trainable else: layer.trainable = trainable # Print trainble layer names if trainable and verbose &gt; 0: log(\"&#123;&#125;&#123;:20&#125; (&#123;&#125;)\".format(\" \" * indent, layer.name, layer.__class__.__name__)) def set_log_dir(self, model_path=None): \"\"\"Sets the model log directory and epoch counter. model_path: If None, or a format different from what this code uses then set a new log directory and start epochs from 0. Otherwise, extract the log directory and the epoch counter from the file name. \"\"\" # Set date and epoch counter as if starting a new model self.epoch = 0 now = datetime.datetime.now() # If we have a model path with date and epochs use them if model_path: # Continue from we left of. Get epoch and date from the file name # A sample model path might look like: # /path/to/logs/coco20171029T2315/mask_rcnn_coco_0001.h5 regex = r\".*/\\w+(\\d&#123;4&#125;)(\\d&#123;2&#125;)(\\d&#123;2&#125;)T(\\d&#123;2&#125;)(\\d&#123;2&#125;)/mask\\_rcnn\\_\\w+(\\d&#123;4&#125;)\\.h5\" m = re.match(regex, model_path) if m: now = datetime.datetime(int(m.group(1)), int(m.group(2)), int(m.group(3)), int(m.group(4)), int(m.group(5))) self.epoch = int(m.group(6)) + 1 # Directory for training logs self.log_dir = os.path.join(self.model_dir, \"&#123;&#125;&#123;:%Y%m%dT%H%M&#125;\".format( self.config.NAME.lower(), now)) # Path to save after each epoch. Include placeholders that get filled by Keras. self.checkpoint_path = os.path.join(self.log_dir, \"mask_rcnn_&#123;&#125;_*epoch*.h5\".format( self.config.NAME.lower())) self.checkpoint_path = self.checkpoint_path.replace( \"*epoch*\", \"&#123;epoch:04d&#125;\") def train(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation=None): \"\"\"Train the model. train_dataset, val_dataset: Training and validation Dataset objects. learning_rate: The learning rate to train with epochs: Number of training epochs. Note that previous training epochs are considered to be done alreay, so this actually determines the epochs to train in total rather than in this particaular call. layers: Allows selecting wich layers to train. It can be: - A regular expression to match layer names to train - One of these predefined values: heaads: The RPN, classifier and mask heads of the network all: All the layers 3+: Train Resnet stage 3 and up 4+: Train Resnet stage 4 and up 5+: Train Resnet stage 5 and up augmentation: Optional. An imgaug (https://github.com/aleju/imgaug) augmentation. For example, passing imgaug.augmenters.Fliplr(0.5) flips images right/left 50% of the time. \"\"\" assert self.mode == \"training\", \"Create model in training mode.\" # Pre-defined layer regular expressions layer_regex = &#123; # all layers but the backbone \"heads\": r\"(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\", # From a specific Resnet stage and up \"3+\": r\"(res3.*)|(bn3.*)|(res4.*)|(bn4.*)|(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\", \"4+\": r\"(res4.*)|(bn4.*)|(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\", \"5+\": r\"(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\", # All layers \"all\": \".*\", &#125; if layers in layer_regex.keys(): layers = layer_regex[layers] # Data generators train_generator = data_generator(train_dataset, self.config, shuffle=True, augmentation=augmentation, batch_size=self.config.BATCH_SIZE) val_generator = data_generator(val_dataset, self.config, shuffle=True, batch_size=self.config.BATCH_SIZE) # Callbacks callbacks = [ keras.callbacks.TensorBoard(log_dir=self.log_dir, histogram_freq=0, write_graph=True, write_images=False), keras.callbacks.ModelCheckpoint(self.checkpoint_path, verbose=0, save_weights_only=True), ] # Train log(\"\\nStarting at epoch &#123;&#125;. LR=&#123;&#125;\\n\".format(self.epoch, learning_rate)) log(\"Checkpoint Path: &#123;&#125;\".format(self.checkpoint_path)) self.set_trainable(layers) self.compile(learning_rate, self.config.LEARNING_MOMENTUM) # Work-around for Windows: Keras fails on Windows when using # multiprocessing workers. See discussion here: # https://github.com/matterport/Mask_RCNN/issues/13#issuecomment-353124009 if os.name is 'nt': workers = 0 else: workers = multiprocessing.cpu_count() self.keras_model.fit_generator( train_generator, initial_epoch=self.epoch, epochs=epochs, steps_per_epoch=self.config.STEPS_PER_EPOCH, callbacks=callbacks, validation_data=val_generator, validation_steps=self.config.VALIDATION_STEPS, max_queue_size=100, workers=workers, use_multiprocessing=True, ) self.epoch = max(self.epoch, epochs) def mold_inputs(self, images): \"\"\"Takes a list of images and modifies them to the format expected as an input to the neural network. images: List of image matricies [height,width,depth]. Images can have different sizes. Returns 3 Numpy matricies: molded_images: [N, h, w, 3]. Images resized and normalized. image_metas: [N, length of meta data]. Details about each image. windows: [N, (y1, x1, y2, x2)]. The portion of the image that has the original image (padding excluded). \"\"\" molded_images = [] image_metas = [] windows = [] for image in images: # Resize image to fit the model expected size # TODO: move resizing to mold_image() molded_image, window, scale, padding = utils.resize_image( image, min_dim=self.config.IMAGE_MIN_DIM, max_dim=self.config.IMAGE_MAX_DIM, padding=self.config.IMAGE_PADDING) molded_image = mold_image(molded_image, self.config) # Build image_meta image_meta = compose_image_meta( 0, image.shape, window, np.zeros([self.config.NUM_CLASSES], dtype=np.int32)) # Append molded_images.append(molded_image) windows.append(window) image_metas.append(image_meta) # Pack into arrays molded_images = np.stack(molded_images) image_metas = np.stack(image_metas) windows = np.stack(windows) return molded_images, image_metas, windows def unmold_detections(self, detections, mrcnn_mask, image_shape, window): \"\"\"Reformats the detections of one image from the format of the neural network output to a format suitable for use in the rest of the application. detections: [N, (y1, x1, y2, x2, class_id, score)] mrcnn_mask: [N, height, width, num_classes] image_shape: [height, width, depth] Original size of the image before resizing window: [y1, x1, y2, x2] Box in the image where the real image is excluding the padding. Returns: boxes: [N, (y1, x1, y2, x2)] Bounding boxes in pixels class_ids: [N] Integer class IDs for each bounding box scores: [N] Float probability scores of the class_id masks: [height, width, num_instances] Instance masks \"\"\" # How many detections do we have? # Detections array is padded with zeros. Find the first class_id == 0. zero_ix = np.where(detections[:, 4] == 0)[0] N = zero_ix[0] if zero_ix.shape[0] &gt; 0 else detections.shape[0] # Extract boxes, class_ids, scores, and class-specific masks boxes = detections[:N, :4] class_ids = detections[:N, 4].astype(np.int32) scores = detections[:N, 5] masks = mrcnn_mask[np.arange(N), :, :, class_ids] # Compute scale and shift to translate coordinates to image domain. h_scale = image_shape[0] / (window[2] - window[0]) w_scale = image_shape[1] / (window[3] - window[1]) scale = min(h_scale, w_scale) shift = window[:2] # y, x scales = np.array([scale, scale, scale, scale]) shifts = np.array([shift[0], shift[1], shift[0], shift[1]]) # Translate bounding boxes to image domain boxes = np.multiply(boxes - shifts, scales).astype(np.int32) # Filter out detections with zero area. Often only happens in early # stages of training when the network weights are still a bit random. exclude_ix = np.where( (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]) &lt;= 0)[0] if exclude_ix.shape[0] &gt; 0: boxes = np.delete(boxes, exclude_ix, axis=0) class_ids = np.delete(class_ids, exclude_ix, axis=0) scores = np.delete(scores, exclude_ix, axis=0) masks = np.delete(masks, exclude_ix, axis=0) N = class_ids.shape[0] # Resize masks to original image size and set boundary threshold. full_masks = [] for i in range(N): # Convert neural network mask to full size mask full_mask = utils.unmold_mask(masks[i], boxes[i], image_shape) full_masks.append(full_mask) full_masks = np.stack(full_masks, axis=-1)\\ if full_masks else np.empty((0,) + masks.shape[1:3]) return boxes, class_ids, scores, full_masks def detect(self, images, verbose=0): \"\"\"Runs the detection pipeline. images: List of images, potentially of different sizes. Returns a list of dicts, one dict per image. The dict contains: rois: [N, (y1, x1, y2, x2)] detection bounding boxes class_ids: [N] int class IDs scores: [N] float probability scores for the class IDs masks: [H, W, N] instance binary masks \"\"\" assert self.mode == \"inference\", \"Create model in inference mode.\" assert len( images) == self.config.BATCH_SIZE, \"len(images) must be equal to BATCH_SIZE\" if verbose: log(\"Processing &#123;&#125; images\".format(len(images))) for image in images: log(\"image\", image) # Mold inputs to format expected by the neural network molded_images, image_metas, windows = self.mold_inputs(images) if verbose: log(\"molded_images\", molded_images) log(\"image_metas\", image_metas) # Run object detection detections, mrcnn_class, mrcnn_bbox, mrcnn_mask, \\ rois, rpn_class, rpn_bbox =\\ self.keras_model.predict([molded_images, image_metas], verbose=0) # Process detections results = [] for i, image in enumerate(images): final_rois, final_class_ids, final_scores, final_masks =\\ self.unmold_detections(detections[i], mrcnn_mask[i], image.shape, windows[i]) results.append(&#123; \"rois\": final_rois, \"class_ids\": final_class_ids, \"scores\": final_scores, \"masks\": final_masks, &#125;) return results def ancestor(self, tensor, name, checked=None): \"\"\"Finds the ancestor of a TF tensor in the computation graph. tensor: TensorFlow symbolic tensor. name: Name of ancestor tensor to find checked: For internal use. A list of tensors that were already searched to avoid loops in traversing the graph. \"\"\" checked = checked if checked is not None else [] # Put a limit on how deep we go to avoid very long loops if len(checked) &gt; 500: return None # Convert name to a regex and allow matching a number prefix # because Keras adds them automatically if isinstance(name, str): name = re.compile(name.replace(\"/\", r\"(\\_\\d+)*/\")) parents = tensor.op.inputs for p in parents: if p in checked: continue if bool(re.fullmatch(name, p.name)): return p checked.append(p) a = self.ancestor(p, name, checked) if a is not None: return a return None def find_trainable_layer(self, layer): \"\"\"If a layer is encapsulated by another layer, this function digs through the encapsulation and returns the layer that holds the weights. \"\"\" if layer.__class__.__name__ == 'TimeDistributed': return self.find_trainable_layer(layer.layer) return layer def get_trainable_layers(self): \"\"\"Returns a list of layers that have weights.\"\"\" layers = [] # Loop through all layers for l in self.keras_model.layers: # If layer is a wrapper, find inner trainable layer l = self.find_trainable_layer(l) # Include layer if it has weights if l.get_weights(): layers.append(l) return layers def run_graph(self, images, outputs): \"\"\"Runs a sub-set of the computation graph that computes the given outputs. outputs: List of tuples (name, tensor) to compute. The tensors are symbolic TensorFlow tensors and the names are for easy tracking. Returns an ordered dict of results. Keys are the names received in the input and values are Numpy arrays. \"\"\" model = self.keras_model # Organize desired outputs into an ordered dict outputs = OrderedDict(outputs) for o in outputs.values(): assert o is not None # Build a Keras function to run parts of the computation graph inputs = model.inputs if model.uses_learning_phase and not isinstance(K.learning_phase(), int): inputs += [K.learning_phase()] kf = K.function(model.inputs, list(outputs.values())) # Run inference molded_images, image_metas, windows = self.mold_inputs(images) # TODO: support training mode? # if TEST_MODE == \"training\": # model_in = [molded_images, image_metas, # target_rpn_match, target_rpn_bbox, # gt_boxes, gt_masks] # if not config.USE_RPN_ROIS: # model_in.append(target_rois) # if model.uses_learning_phase and not isinstance(K.learning_phase(), int): # model_in.append(1.) # outputs_np = kf(model_in) # else: model_in = [molded_images, image_metas] if model.uses_learning_phase and not isinstance(K.learning_phase(), int): model_in.append(0.) outputs_np = kf(model_in) # Pack the generated Numpy arrays into a a dict and log the results. outputs_np = OrderedDict([(k, v) for k, v in zip(outputs.keys(), outputs_np)]) for k, v in outputs_np.items(): log(k, v) return outputs_np 工具 一個是輸出log的function，另一個是去修改batch norm，畢竟這repo是2 or 4，太小了會造成反效果 123456789101112131415161718192021222324def log(text, array=None): \"\"\"Prints a text message. And, optionally, if a Numpy array is provided it prints it's shape, min, and max values. \"\"\" if array is not None: text = text.ljust(25) text += (\"shape: &#123;:20&#125; min: &#123;:10.5f&#125; max: &#123;:10.5f&#125;\".format( str(array.shape), array.min() if array.size else \"\", array.max() if array.size else \"\")) print(text)class BatchNorm(KL.BatchNormalization): \"\"\"Batch Normalization class. Subclasses the Keras BN class and hardcodes training=False so the BN layer doesn't update during training. Batch normalization has a negative effect on training if batches are small so we disable it here. \"\"\" def call(self, inputs, training=None): return super(self.__class__, self).call(inputs, training=False) 4.resnet graph Resnet All 12345678910111213141516171819202122232425262728293031def resnet_graph(input_image, architecture, stage5=False): assert architecture in [\"resnet50\", \"resnet101\"] # Stage 1 x = KL.ZeroPadding2D((3, 3))(input_image) x = KL.Conv2D(64, (7, 7), strides=(2, 2), name='conv1', use_bias=True)(x) x = BatchNorm(axis=3, name='bn_conv1')(x) x = KL.Activation('relu')(x) C1 = x = KL.MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\")(x) # Stage 2 x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1)) x = identity_block(x, 3, [64, 64, 256], stage=2, block='b') C2 = x = identity_block(x, 3, [64, 64, 256], stage=2, block='c') # Stage 3 x = conv_block(x, 3, [128, 128, 512], stage=3, block='a') x = identity_block(x, 3, [128, 128, 512], stage=3, block='b') x = identity_block(x, 3, [128, 128, 512], stage=3, block='c') C3 = x = identity_block(x, 3, [128, 128, 512], stage=3, block='d') # Stage 4 x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a') block_count = &#123;\"resnet50\": 5, \"resnet101\": 22&#125;[architecture] for i in range(block_count): x = identity_block(x, 3, [256, 256, 1024], stage=4, block=chr(98 + i)) C4 = x # Stage 5 if stage5: x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a') x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b') C5 = x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c') else: C5 = None return [C1, C2, C3, C4, C5] Convolution 12345678910111213141516171819202122232425262728293031323334353637ef conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2), use_bias=True): \"\"\"conv_block is the block that has a conv layer at shortcut # Arguments input_tensor: input tensor kernel_size: defualt 3, the kernel size of middle conv layer at main path filters: list of integers, the nb_filters of 3 conv layer at main path stage: integer, current stage label, used for generating layer names block: 'a','b'..., current block label, used for generating layer names Note that from stage 3, the first conv layer at main path is with subsample=(2,2) And the shortcut should have subsample=(2,2) as well \"\"\" nb_filter1, nb_filter2, nb_filter3 = filters conv_name_base = 'res' + str(stage) + block + '_branch' bn_name_base = 'bn' + str(stage) + block + '_branch' x = KL.Conv2D(nb_filter1, (1, 1), strides=strides, name=conv_name_base + '2a', use_bias=use_bias)(input_tensor) x = BatchNorm(axis=3, name=bn_name_base + '2a')(x) x = KL.Activation('relu')(x) x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same', name=conv_name_base + '2b', use_bias=use_bias)(x) x = BatchNorm(axis=3, name=bn_name_base + '2b')(x) x = KL.Activation('relu')(x) x = KL.Conv2D(nb_filter3, (1, 1), name=conv_name_base + '2c', use_bias=use_bias)(x) x = BatchNorm(axis=3, name=bn_name_base + '2c')(x) shortcut = KL.Conv2D(nb_filter3, (1, 1), strides=strides, name=conv_name_base + '1', use_bias=use_bias)(input_tensor) shortcut = BatchNorm(axis=3, name=bn_name_base + '1')(shortcut) x = KL.Add()([x, shortcut]) x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x) return x Identity Block 12345678910111213141516171819202122232425262728293031def identity_block(input_tensor, kernel_size, filters, stage, block, use_bias=True): \"\"\"The identity_block is the block that has no conv layer at shortcut # Arguments input_tensor: input tensor kernel_size: defualt 3, the kernel size of middle conv layer at main path filters: list of integers, the nb_filters of 3 conv layer at main path stage: integer, current stage label, used for generating layer names block: 'a','b'..., current block label, used for generating layer names \"\"\" nb_filter1, nb_filter2, nb_filter3 = filters conv_name_base = 'res' + str(stage) + block + '_branch' bn_name_base = 'bn' + str(stage) + block + '_branch' x = KL.Conv2D(nb_filter1, (1, 1), name=conv_name_base + '2a', use_bias=use_bias)(input_tensor) x = BatchNorm(axis=3, name=bn_name_base + '2a')(x) x = KL.Activation('relu')(x) x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same', name=conv_name_base + '2b', use_bias=use_bias)(x) x = BatchNorm(axis=3, name=bn_name_base + '2b')(x) x = KL.Activation('relu')(x) x = KL.Conv2D(nb_filter3, (1, 1), name=conv_name_base + '2c', use_bias=use_bias)(x) x = BatchNorm(axis=3, name=bn_name_base + '2c')(x) x = KL.Add()([x, input_tensor]) x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x) return x","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://world4jason.github.io/tags/Deep-Learning/"},{"name":"R-CNN","slug":"R-CNN","permalink":"https://world4jason.github.io/tags/R-CNN/"},{"name":"Segmentation","slug":"Segmentation","permalink":"https://world4jason.github.io/tags/Segmentation/"},{"name":"Pose Estimation","slug":"Pose-Estimation","permalink":"https://world4jason.github.io/tags/Pose-Estimation/"}]},{"title":"CapsuleNet","date":"2018-02-06T05:53:47.000Z","path":"2018/02/06/CapsuleNet/","text":"","tags":[]},{"title":"GAN - 手推","date":"2018-02-04T05:53:47.000Z","path":"2018/02/04/untitled-1517723627245/","text":"Outline GAN 機率分佈 f-divergence fenchel conjugate GAN公式 GAN 機率分佈機率質量函數(PMF) 可數-&gt;離散機率密度函數(PDF) 不可數-&gt; 不可數 ISSUE 如果換成凹函式呢?-&gt; 大於小於對調 但生成結果好像沒差(? —未完成 待續 \\mathcal {F{i}^{RoI}}{(u^\\prime,v^\\prime)} = \\sum_{(u,v)}^{W \\times H} G(u,v;u^\\prime, v^\\prime|Bi) \\mathcal F(u,v)","tags":[]},{"title":"ICNET code Analysis","date":"2018-01-31T06:21:27.000Z","path":"2018/01/31/ICNET code Analysis/","text":"","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://world4jason.github.io/tags/Deep-Learning/"},{"name":"Segmentation","slug":"Segmentation","permalink":"https://world4jason.github.io/tags/Segmentation/"},{"name":"Tensorflow","slug":"Tensorflow","permalink":"https://world4jason.github.io/tags/Tensorflow/"},{"name":"real-time","slug":"real-time","permalink":"https://world4jason.github.io/tags/real-time/"}]},{"title":"ICNET for Real-Time Semantic Segmentation on High-Resolution Images","date":"2018-01-28T08:50:31.000Z","path":"2018/01/28/ICNET-for-Real-Time-Semantic-Segmentation-on-High-Resolution-Images/","text":"RecapQuote:其實很討厭這作者的paper效果都很好, 但是每次都是用matlab, 而且PSPNet作者還說training code因為公司問題不能發布, 傻眼 https://www.zhihu.com/question/53356671 Paperhttps://arxiv.org/pdf/1704.08545.pdfCodehttps://github.com/hszhao/ICNethttps://github.com/aitorzip/Keras-ICNethttps://github.com/hellochick/ICNet-tensorflow Key Difference之前的那些方法，如FCN、SegNet、UNet、RefineNet等，用高解析度圖片當input以後，強調Single scale或是Multi Scale在不同層之間的特徵融合，所有的Data需要在整個網絡中運行，因為高解析度的輸入而導致了昂貴的計算費用.而本文的方法，使用低解析度圖片作為主要輸入，採用高解析度圖片進行refine，保留細節的同時減少了開銷. Abstract1We propose an compressed-PSPNet-based image cascade network (ICNet) that incorporates multi-resolution branches under proper label guidance to address this challenge. ICNet是一個基於PSPNet的real-time semantic segmentation network，論文內對PSPNet做深入的分析，並且找出影響inference speed*的缺點。並且用搭配multi-resolution cascade combination。 *註:inference speed是單指DeConv的階段。 Introduction 在論文發表的時刻(2017 March), CityScapes上所有的Model表現基本上分成兩種類型, 一種是擁有高精準度但速度不行, 另一種是速度快但精准度不行。此論文在PSPNet的基礎上來增進速度，並找一個速度跟精準度的平衡點。 論文貢獻: 可以在1024x2048的解析度下保持30 fps的計算速度(Tensorflow版本實測可行, 但要去掉preproccess部分) 相對PSPNet來說, 可疑提高5倍速度並可以減少五倍RAM消耗 低解析的速度+高解析的細節做cascade的整合 Speed Analysis從PSPNet做解析 藍色是1024x2048, 綠色是512x1024 (1/4大小)從上圖可知 圖越大速度越慢 網路寬度越大速度越慢 Kernel越多速度越慢, 以圖中例子來說stage4跟stage5在解析同樣的input時, inference speed差距十分驚人, 因為這部分的kernel number差距了一倍。 Intuitive Speedup加速方法 1: 輸入向下採樣(Downsampling Input)在resolution只有原本的0.5跟0.25的狀況下雖然速度變快但精准度如上圖所示可以看出效果很差。 加速方法 2 : 利用較小的feature map來做inference(Downsampling Feature)FCN Downsampling到32倍, Deep Lab到 8倍, 而下方是用作者之前的PSPNet50, 縮小到了1:8, 1:16, 1:32整理的Table, 但可以看到最快的速度也只有132ms, 不太能符合real-time的標準。 加速方法 3 : 減少模型複雜度(Model Compression)採用了其他篇paper(Pruning filters for efficient convnets)，作法就是減少Filter數量, 但一樣差強人意 FCN:Fully Convolutional Networks for Fully Convolutional Networks這裡額外多講一下FCN，算是CNN做semantic segmentation的始祖，本質上的區別大概就是…FCN是沒有全連結層的CNN，好處是可以接受任意大小輸入。 CNN要如何轉FCN? 以此篇paper為例，input是一個224x224x3的圖，經過一系列Conv跟Downsampling之後是7x7x512。AlexNet使用了兩個4096的全連接層，最後一個有1000個神經元的全連接層用於計算分類評分。我們可以將這3個全連接層轉化為Convolution層。 任一全連結層轉化為Conv的方式以以下為例： 例如 K=4096 的全連接層，輸入是7x7x512，這個全連接層可以被等效地看做一個F=7,Padding=0,Stride=1,Filter Number=4096 的Conv層。換句話說，就是將Filter Size設置的和Input Data Size一致了。輸出將變成 1x1x4096，這個結果就和使用初始的那個全連接層一樣了。針對第一個連接區域是[7x7x512]的全連接層，令其Filter Size為F=7（Filter Size為7x7），這樣輸出為[1x1x4096]。針對第二個全連接層，令其Filter Size為F=1（Filter Size為1x1），這樣輸出為[1x1x4096]。對最後一個全連接層也做類似的，令其F=1（Filter Size為1x1），最終輸出為[1x1x1000] Step 1:下圖是是原始CNN結構，CNN中輸入的圖像大小是統一固定resize成227x227大小的圖像，第一層pooling後為55x55，第二層pooling後為27x27，第五層pooling後的圖像大小為13*13。 Step 2:FCN輸入的圖像是假設是H*W，第一層pooling後變為原圖大小的1/4，第二層變為的1/8，第五層變為1/ 16，第八層變為1/32 Step 3:Convolution本質上就是DownSampling（下採樣）。經過多次Convolution和pooling以後，得到的圖像越來越小，解析度越來越低。其中圖像到H/32∗W/32 的時候圖片是最小的一層時，所產生圖叫做heatmap，heatmap就是我們最重要的高維特徵圖，得到高維特徵的heatmap之後就是最重要的一步也是最後的一步，就是對此heatmap進行UpSampling(Deconvolution)，把圖像進行放大到原圖像的大小。 Step 4:最後的輸出是1000張heatmap經過UpSampling變為原圖大小的圖片。 Upsampling 其實這篇paper內雖然叫做Deconvolution，但之前CS231n課程內的大神也有說到，叫做Transposed Convolution比較適合。舉個例子來說： 4x4的圖片輸入，Filter Size為3x3, 沒有Padding / Stride, 則輸出為2x2。 輸入矩陣可展開為16維向量，記作x輸出矩陣可展開為4維向量，記作yConvolution運算可表示為y=CxC其實就是如下的稀疏陣，而Forwarding就改成了的矩陣運算 BackPropagation的話，假若已經從更深的網路得到了 那麼就可以導出以下公式: Deconvolution其實就是Forwarding時乘CT，而BackPropagation時乘(CT)T，即C。總結來說，Deconvolution等於Convolution在神經網絡結構的正向和反向傳播中的計算，做相反的計算。 Skip Architecture由於縮小32倍結果超糟糕，所以FCN在前面的Pooling Layer進行Upsampling，然後結合這些結果來優化輸出。 Architecture總結一下前面速度分析的結果，一系列的優化方法： Downsampling Input：降低輸入解析度能都大幅度的加速，但同時會讓預測非常模糊 Downsampling Feature：可以加速但同時會降低準確率 Model Compression：壓縮訓練好的模型，通過減輕模型達到加速效果，可惜實驗效果不佳 針對以上的分析，發現，低解析度的圖片能夠有效降低運行時間，但是失去很多細節，而且邊界模糊；但是高解析度的計算時間難以忍受，ICNet總結了上述幾個問題，提出了一個綜合性的方法：使用低解析度加速捕捉語義，使用高解析度獲取細節，使用特徵融合(CFF)結合，同時使用guide label來監督，在限制的時間內獲得有效的結果。 Branch Analysis圖中用了原尺寸,1/2,1/4當input，低解析度分枝超過50層Convolution，來提取更多的語義信息(inference 18 ms)，中解析度分枝有17層Convolution，但是由於權重共享，只有inference 6ms，而高解析度分枝是3 Convolution，有inference 9ms. 分枝 過程 耗時 低解析 低解析是FCN-based PSPNet的架構，總和有超過50層的Convolution，在中解析度的1/16輸出的基礎上，再縮放到1/32.經過Convolution後，然後使用幾個dilated convolution擴展接受野但不縮小尺寸，最終以原圖的1/32大小輸出feature map。 雖然層數較多，但是解析度低，速度快，且與分枝二共享一部分權重，耗時為18 ms 中解析 以原圖的1/2的解析度作為輸入，經過17層Convolution後以1/8縮放，得到原圖的1/16大小feature map，再將低解析度分枝的輸出feature map通過CFF(cascade feature fusion )單元相融合得到最終輸出。值得注意的是：低解析度和中解析度的捲積參數是共享的。 有17個Convolution層，與分枝一共享一部分權重，與分枝一一起一共耗時6ms 高解析 原圖輸入，經過三層的Convolution(Stride=2,Size=3x3)得到原圖的1/8大小的feature map，再將中解析度處理後的輸出通過CFF單元融合 有3個卷積層，雖然解析度高，因為少，耗時為9ms 對於每個分枝的輸出特徵，首先會上採樣2倍做輸出，在訓練的時候，會以Ground truth的1/16、1/8/、1/4來指導各個分枝的訓練，這樣的輔助訓練使得梯度優化更為平滑，便於訓練收斂，隨著每個分枝學習能力的增強，預測沒有被任何一個分枝主導。利用這樣的漸變的特徵融合和級聯引導結構可以產生合理的預測結果。 ICNet使用低解析度完成語義分割，使用高解析度幫助細化結果。在結構上，產生的feature大大減少，同時仍然保持必要的細節。 Cascade Label GuidanceBranch Output不同分枝的預測效果如下: 可以看到第三個分枝輸出效果無疑是最好的。在測試時，只保留第三分枝的結果。 Cascade Feature Fusion圖中的Loss是輔助Loss,F1是較低解析的分枝, F2是較高解析的分枝， Loss FunctionL=λ1L1+λ2L2+λ3L3Loss是對應到每個downsampled score maps使用cross-entropy loss 依據CFF的設置，下分枝的lossL3的佔比λ3設置為1的話,則中分枝的lossL2的佔比λ2設置為0.4，上分枝的lossL1的佔比λ1設置為0.16 Experiment 項目 設置 平台 Caffe，CUDA7.5 cudnnV5，TitanX一張 測量時間 Caffe Time 100次取平均 Batch Size 16 學習速率 Poly, Learning Rate 0.01, Momentum 0.9 迭代次數 30K 權重衰減 0.0001 數據增強 Random flip, 0.5 to 2 random scale 資料集 Cityscapes model Compression以PSPNet50為例，直接壓縮結果如下表Baseline： mIoU降低了，但時間170ms達不到realtime。這表明只有模型壓縮是達不到有良好分割結果的實時性能。對比ICNet，有類似的分割結果，但速度提升了5倍多。 Cascade Structure Experiment sub4代表只有低解析度輸入的結果，sub24代表前兩個分枝，sub124全部分枝。注意到全部分枝的速度很快，並且性能接近PSPNet了，且能保持30fps。而且Ram消耗也明顯減少了。 Visualization Cityscape Comparison","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://world4jason.github.io/tags/Deep-Learning/"},{"name":"Computer Vision","slug":"Computer-Vision","permalink":"https://world4jason.github.io/tags/Computer-Vision/"},{"name":"Segmentation","slug":"Segmentation","permalink":"https://world4jason.github.io/tags/Segmentation/"},{"name":"real-time","slug":"real-time","permalink":"https://world4jason.github.io/tags/real-time/"}]},{"title":"機器學習中的相似性度量","date":"2018-01-13T18:12:19.000Z","path":"2018/01/14/Distance/","text":"Euclidean Distance(歐幾里和距離)二維與三維中就是兩點之間的距離。 上圖中的綠線為歐氏距離，又稱歐幾里和距離(Euclidean Distance)，其餘的藍色與黃色還有紅色皆為曼哈頓距離。 Manhattan distance(曼哈頓距離)平面上，坐標（x1, y1）的點P1與坐標（x2, y2）的點P2的曼哈頓距離為： L1-距離 Mahalanobis distance(馬氏距離)Covariance Matrix 1Mahalanobis distance can be defined as a dissimilarity measure between two random vectors x and y of the same distribution with the covariance matrix S: Chebyshev distance(切比雪夫距離)Minkowski distance (明可夫斯基距離)http://www.cnblogs.com/heaad/archive/2011/03/08/1977733.html","tags":[]},{"title":"Deep Sort: Simple Online and Realtime Tracking with a Deep Association Metric","date":"2018-01-13T17:38:43.000Z","path":"2018/01/14/Deep-Sort/","text":"此篇文章是基於SORT的改進具體來說就是一句話 1We adopt a conventional single hypothesis tracking methodology with recursive kalman filtering and frame-by-frame data association.","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://world4jason.github.io/tags/Deep-Learning/"},{"name":"Tracking","slug":"Tracking","permalink":"https://world4jason.github.io/tags/Tracking/"},{"name":"Computer Vision","slug":"Computer-Vision","permalink":"https://world4jason.github.io/tags/Computer-Vision/"},{"name":"MOT","slug":"MOT","permalink":"https://world4jason.github.io/tags/MOT/"}]},{"title":"Multiple Object Tracking Summary","date":"2018-01-10T12:03:49.000Z","path":"2018/01/10/Multiple-Object-Tracking-Summary/","text":"http://perception.yale.edu/Brian/refGuides/MOT.html","tags":[{"name":"Tracking","slug":"Tracking","permalink":"https://world4jason.github.io/tags/Tracking/"},{"name":"Computer Vision","slug":"Computer-Vision","permalink":"https://world4jason.github.io/tags/Computer-Vision/"},{"name":"MOT","slug":"MOT","permalink":"https://world4jason.github.io/tags/MOT/"},{"name":"Summary","slug":"Summary","permalink":"https://world4jason.github.io/tags/Summary/"}]},{"title":"Deep Learning and Computer Vision Recommended Paper","date":"2018-01-09T13:50:49.000Z","path":"2018/01/09/Deep-Learning-Recommended-Papers/","text":"Image ClassificationMust Read : LeNet, AlexNet, VGG-16, GoogleNet, ResNet Title Authors Pub. Links Figure LeNet-5, convolutional neural networks Y. LeCun ??? 199X Web LeNet ImageNet Classification with Deep Convolutional Neural Networks Alex Krizhevsky,Ilya Sutskever,Geoffrey E. Hinton NIPS 2014 paper AlexNet Very Deep Convolutional Networks for Large-Scale Image Recognition Karen Simonyan, Andrew Zisserman ICLR 2014 paper VGG16 Going Deeper with Convolutions Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed CVPR 2015 paper GoogLeNet Deep Residual Learning for Image Recognition Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun CVPR 2016 best paper github ResNet Residual Attention Network for Image Classification Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, Xiaoou Tang CVPR 2017 paper github Res-Attention-Network Aggregated Residual Transformations for Deep Neural Networks Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He CVPR 2017 paper github ResNeXt Densely Connected Convolutional Networks Gao Huang, Zhuang Liu, Kilian Q. Weinberger CVPR 2017 best paper github DenseNet Deep Pyramidal Residual Networks Dongyoon Han, Jiwhan Kim, Junmo Kim CVPR 2017 paper github PyramidNet Object DetectionMust Read : R-CNN, Fast R-CNN, Faster R-CNN, YOLO, SSD Title Authors Pub. Links Figure Rich feature hierarchies for accurate object detection and semantic segmentation Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik CVPR 2014 paper github R-CNN Fast R-CNN Ross Girshick ICCV 2015 paper github Fast-R-CNN Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun TPAMI 2015 paper SPP Net Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun NIPS 2015 paper matlab python pytorch Faster-R-CNN You Only Look Once: Unified, Real-Time Object Detection Joseph Redmon,Santosh Divvala,Ross Girshick, Ali Farhadi CVPR 2016 paper YOLO SSD: Single Shot MultiBox Detector Wei Liu1, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg CVPR 2016 paper SSD Convolutional Feature Masking for Joint Object and Stuff Segmentation Jifeng Dai, Kaiming He, Jian Sun CVPR 2015 paper CFM Instance-aware Semantic Segmentation via Multi-task Network Cascades Jifeng Dai, Kaiming He, Jian Sun CVPR 2016 paper github MNC R-FCN: Object Detection via Region-based Fully Convolutional Networks Jifeng Dai, Yi Li, Kaiming He, Jian Sun NIPS 2016 paper github Region-FCN Feature Pyramid Networks for Object Detection Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie CVPR 2017 paper FPN Mask R-CNN Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick ICCV 2017 paper Mask-R-CNN A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection Xiaolong Wang, Abhinav Shrivastava, Abhinav Gupta CVPR 2017 paper github A-Fast-R-CNN Multiple Instance Detection Network with Online Instance Classifier Refinement Peng Tang, Xinggang Wang, Xiang Bai, Wenyu Liu CVPR 2017 paper MIDN R-FCN-3000 at 30fps: Decoupling Detection and Classification Bharat Singh, Hengdou Li, Abhishek Sharma and Larry S. Davis Tech Report paper R-FCN-3000 Semantic Segmentation and Scene ParsingMust Read : FCN, Learning Deconvolution Network for Semantic Segmentation, U-Net Title Authors Pub. Links Figure Fully Convolutional Networks for Semantic Segmentation Jonathan Long, Evan Shelhamer, Trevor Darrell CVPR 2015 paper FCN Learning to Segment Object Candidates Pedro O. Pinheiro, Ronan Collobert, Piotr Dollar NIPS 2015 paper LSOC Learning to Refine Object Segments Pedro O. Pinheiro , Tsung-Yi Lin , Ronan Collobert, Piotr Doll ́ar arXiv 1603.08695 paper LROS Conditional Random Fields as Recurrent Neural Networks Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, ZhiZhong Su, Dalong Du, Chang Huang, and Philip H. S. Torr ICCV 2015 paper CRFRNN Learning Deconvolution Network for Semantic Segmentation Heonwoo Noh, Seunghoon Hong, Bohyung Han ICCV 2015 paper LDN U-Net: Convolutional Networks for Biomedical Image Segmentation Olaf Ronneberger, Philipp Fischer, Thomas Brox MICCAI 2015 paper U-Net Instance-sensitive Fully Convolutional Networks Jifeng Dai, Kaiming He, Yi Li, Shaoqing Ren, Jian Sun ECCV 2016 paper ISFCN Laplacian Pyramid Reconstruction and Refinement for Semantic Segmentation Golnaz Ghiasi, Charless C. Fowlkes ECCV 2016 paper github LPRR Attention to Scale: Scale-aware Semantic Image Segmentation Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu CVPR 2016 paper DeepLab Attention-to-scale RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation Guosheng Lin, Anton Milan, Chunhua Shen, Ian Reid CVPR 2017 paper github RefineNet Pyramid Scene Parsing Network Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia CVPR 2017 paper github PSPNet ICNet for Real-Time Semantic Segmentation on High-Resolution Images Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, Jiaya Jia Tech Report paper ICNet Dilated Residual Networks Fisher Yu, Vladlen Koltun, Thomas Funkhouser CVPR 2017 paper github DRN Fully Convolutional Instance-aware Semantic Segmentation Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, Yichen Wei CVPR 2017 paper github FCIS Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes Tobias Pohlen, Alexander Hermans, Markus Mathias, Bastian Leibe CVPR 2017 paper github FRRN Object Region Mining with Adversarial Erasing: A Simple Classification toSemantic Segmentation Approach Yunchao Wei, Jiashi Feng, Xiaodan Liang, Ming-Ming Cheng, Yao Zhao, Shuicheng Yan CVPR 2017 paper A-Erasing Not All Pixels Are Equal: Difficulty-Aware Semantic Segmentation via Deep Layer Cascade Xiaoxiao Li, Ziwei Liu, Ping Luo, Chen Change Loy, Xiaoou Tang CVPR 2017 paper Not-All-Pixels-Are-Equal Semantic Segmentation with Reverse Attention Qin Huang, Chunyang Xia, Wuchi Hao, Siyang Li, Ye Wang, Yuhang Song and C.-C. Jay Kuo BMVC 2017 paper code Rev-Attention Predicting Deeper into the Future of Semantic Segmentation Pauline Luc, Natalia Neverova, Camille Couprie, Jakob Verbeek and Yann LeCun ICCV 2017 paper project page Deeper-into-Future Learning to Segment Every Thing Ronghang Hu, Piotr Dollar, Kaiming He, Trevor Darrell, Ross Girshick Tech Report paper Seg-Everything Regularization Dropout- A Simple Way to Prevent Neural Networks from Overfitting Batch Normalization- Accelerating Deep Network Training by Reducing Internal Covariate Shift RNN Generating Sequences With Recurrent Neural Networks Word embedding Distributed Representations of Words and Phrases and their Compositionality Image captioningShow and Tell: A Neural Image Caption GeneratorShow, Attend and Tell: Neural Image Caption Generation with Visual Attention","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://world4jason.github.io/tags/Deep-Learning/"},{"name":"Computer Vision","slug":"Computer-Vision","permalink":"https://world4jason.github.io/tags/Computer-Vision/"},{"name":"Paper List","slug":"Paper-List","permalink":"https://world4jason.github.io/tags/Paper-List/"},{"name":"Self-Study","slug":"Self-Study","permalink":"https://world4jason.github.io/tags/Self-Study/"}]},{"title":"Person re-ID Summary","date":"2018-01-02T13:50:49.000Z","path":"2018/01/02/person_re-ID_summary/","text":"目標難度 目標遮擋（Occlusion）導致部分特徵丟失 不同的 View，Illumination 導致同一目標的特徵差異 不同目標衣服顏色近似、特徵近似導致區分度下降 解決方案1. Representation learning + ReID看做分類(Classification/Identification)問題或者驗證(Verification)問題：(1) 分類問題是指利用行人的ID或者屬性等作為訓練標籤來訓練模型；(2) 驗證問題是指輸入一對（兩張）行人圖片，讓網絡來學習這兩張圖片是否屬於同一個行人。 Classification/Identification loss和verification loss 額外改進方向[2]是在加上許多行人的label，像是性別、頭髮以及服裝等等。 2. Metric learning + ReID常用於圖像檢索的方法，通過網絡學習出兩張圖片的相似度。(Contrastive loss)[5]、三元組損失(Triplet loss)、 四元組損失(Quadruplet loss)、難樣本採樣三元組損失(Triplet hard loss with batch hard mining, TriHard loss)、邊界挖掘損失(Margin sample mining loss, MSML Contrastive loss 基本上就是Siamese CNN 訓練時是三個正樣本一個副樣本，test時未知 3. Local Feature + ReID論文[3]用local feature而不用global feature，切割好以後送到LSTM去學 但論文[3]會有對齊問題，所以論文[4]用pose跟skeleton來做姿勢預測，再通過仿射變換對齊 論文[5]直接拿關節點切出ROI，14個人體關節點，得到7個ROI區域，(頭、上身、下身和四肢) 4. Video Sequence + ReID這方向不熟 貼兩張圖參考參考而已 5. GAN + ReIDReID數據集目前最大的也只有幾千個ID，跟萬張圖片而已，CNN based還容易overfittingGAN主要是用在遷移學習跟基於條件的生成 第一篇就是ICCV2017的論文[5]以及後來同作者改進的論文[6]，是可以避免overfitting但生成效果就很慘 為了處理不同數據集，甚至是不同camera所造成bias的問題，論文[7]是利用cycleGAN based的設計，利用遷移學習來處理兩個數同數據集的問題，先切割分前景跟背景，在轉換過去。D有兩個loss(還是有兩個D不確定，paper內沒架構圖)一個是前景的絕對誤差loss，一個是正常的判別器loss。判別器loss是用來判斷生成的圖屬於哪個domain，前景的loss是為了保證行人前景儘可能逼真不變。mask用PSPnet來找的。 Pose Normalization[8] 資料種類 Video-based Image-based Long-term activity Individual action 資料庫Robust Systems Lab 程式碼简单行人重识别代码到88%准确率https://github.com/layumi/Person_reID_baseline_pytorch ICCV 2017 Cross-view Asymmetric Metric Learning for Unsupervised Re-id Deeply-Learned Part-Aligned Representations for Person Re-Identification In Defense of the Triplet Loss for Person Re-Identification Jointly Attentive Spatial-Temporal Pooling Networks for Video-based Person Re-Identification SVDNet for Pedestrian Retrieval Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro CVPR 2017 Spindle Net: Person Re-Identification With Human Body Region Guided Feature Decomposition and Fusion Person Re-Identification in the Wild Joint Detection and Identification Feature Learning for Person Search Quality Aware Network for Set to Set Recognition Paper List- Point to Set Similarity Based Deep Feature Learning for Person Re-Identification - Fast Person Re-Identification via Cross-Camera Semantic Binary Transformation - See the Forest for the Trees: Joint Spatial and Temporal Recurrent Neural Networks for Video-Based Person Re-Identification - Learning Deep Context-Aware Features Over Body and Latent Parts for Person Re-Identification - Consistent-Aware Deep Learning for Person Re-Identification in a Camera Network - Re-Ranking Person Re-Identification With k-Reciprocal Encoding - Multiple People Tracking by Lifted Multicut and Person Re-Identification [1] Mengyue Geng, Yaowei Wang, Tao Xiang, Yonghong Tian. Deep transfer learning for person reidentification[J]. arXiv preprint arXiv:1611.05244, 2016. [2] Yutian Lin, Liang Zheng, Zhedong Zheng, YuWu, Yi Yang. Improving person re-identification by attribute and identity learning[J]. arXiv preprint arXiv:1703.07220, 2017. [3] Rahul Rama Varior, Bing Shuai, Jiwen Lu, Dong Xu, Gang Wang. A siamese long short-term memory architecture for human re-identification[C]//European Conference on Computer Vision. Springer, 2016:135–153. [4]Liang Zheng, Yujia Huang, Huchuan Lu, Yi Yang. Pose invariant embedding for deep person reidentification[J]. arXiv preprint arXiv:1701.07732, 2017. [5] Haiyu Zhao, Maoqing Tian, Shuyang Sun, Jing Shao, Junjie Yan, Shuai Yi, Xiaogang Wang, Xiaoou Tang. Spindle net: Person re-identification with human body region guided feature decomposition and fusion[C]. CVPR, 2017. [6] Zhong Z, Zheng L, Zheng Z, et al. Camera Style Adaptation for Person Re-identification[J]. arXiv preprint arXiv:1711.10295, 2017. [7] Wei L, Zhang S, Gao W, et al. Person Transfer GAN to Bridge Domain Gap for Person Re-Identification[J]. arXiv preprint arXiv:1711.08565, 2017. [8] Qian X, Fu Y, Wang W, et al. Pose-Normalized Image Generation for Person Re-identification[J]. arXiv preprint arXiv:1712.02225, 2017.","tags":[{"name":"Tracking","slug":"Tracking","permalink":"https://world4jason.github.io/tags/Tracking/"},{"name":"image retrieval","slug":"image-retrieval","permalink":"https://world4jason.github.io/tags/image-retrieval/"},{"name":"survey","slug":"survey","permalink":"https://world4jason.github.io/tags/survey/"}]},{"title":"PYTHON中如何使用*ARGS和**KWARGS","date":"2017-12-24T10:48:14.000Z","path":"2017/12/24/PYTHON中如何使用-ARGS和-KWARGS/","text":"範例與翻譯理解自連結與連結 *args跟 **kwargs是類似的東西，是可有可無的參數。 一顆星的*args是tuple，可以接受很多的值。兩顆星的**kwargs一樣是可以接受很多值，但是是接受dictionary。###一顆星用法 範例 123456def test_var_args(farg, *args): print \"formal arg:\", farg for arg in args: print \"another arg:\", arg test_var_args(1, \"two\", 3) 结果 123formal arg: 1another arg: twoanother arg: 3 ###兩顆星用法 範例 123456def test_var_kwargs(farg, **kwargs): print \"formal arg:\", farg for key in kwargs: print \"another keyword arg: %s: %s\" % (key, kwargs[key])test_var_kwargs(farg=1, myarg2=\"two\", myarg3=3) 结果 123formal arg: 1another keyword arg: myarg2: twoanother keyword arg: myarg3: 3 順序問題如果function定義時如上圖先放了tuple才是dictionary，呼叫時參數先放dictionary再放tuple會跳error。","tags":[{"name":"code","slug":"code","permalink":"https://world4jason.github.io/tags/code/"},{"name":"python","slug":"python","permalink":"https://world4jason.github.io/tags/python/"}]},{"title":"Python 筆記","date":"2017-12-13T07:35:34.000Z","path":"2017/12/13/python-note/","text":"Different in Py2 and Py3 Pickle module 12345import sysif sys.version_info[0]&lt;3: import cPickle as pickleelse import _pickle as pickle","tags":[{"name":"code","slug":"code","permalink":"https://world4jason.github.io/tags/code/"}]},{"title":"zi2zi: Master Chinese Calligraphy with Conditional Adversarial Networks","date":"2017-11-30T19:36:19.000Z","path":"2017/12/01/zi2zi/","text":"Generated samples. Related code can be found here 目標字體風格轉換 動機直接用CNN進行風格轉換會有下列問題 生成常常是模糊的 多數生成結果是失敗的 只能做一對一生成 結論：用GAN試試看 用GAN秒殺一切！這篇借鑒了三篇paper內容 Image-to-Image Translation with Conditional Adversarial NetworksConditional Image Synthesis With Auxiliary Classifier GANsUnsupervised Cross-Domain Image Generation 主要是由pix2pix這篇修改而來的 其中Encoder跟Decoder還有Discriminator是直接用pix2pix的, 尤其是裡面的Unet模型","tags":[{"name":"Generation","slug":"Generation","permalink":"https://world4jason.github.io/tags/Generation/"},{"name":"Generative Model","slug":"Generative-Model","permalink":"https://world4jason.github.io/tags/Generative-Model/"},{"name":"GAN","slug":"GAN","permalink":"https://world4jason.github.io/tags/GAN/"},{"name":"Style Transfer","slug":"Style-Transfer","permalink":"https://world4jason.github.io/tags/Style-Transfer/"}]},{"title":"Faster R-CNN","date":"2017-11-10T18:13:59.000Z","path":"2017/11/11/Faster R-CNN/","text":"#Outline R-CNN, Fast R-CNN, Faster R-CNN 關係 流程圖 BackBone RPN Anchor ROI Pooling Bounding box regression 訓練 測試 R-CNN, Fast R-CNN, Faster R-CNN 關係 網路架構 使用方法 流程 缺點 重點貢獻 R-CNN 1.使用Selective Search 提取 Region Propose 2. 利用在ImageNet 訓練的CNN網路提取特徵 3.Bounding Box regression做邊框計算 4.由K個二分類SVM做分類(Ｋ= 任務分類數量) 1.先利用SS提取2K個候選區(圖片) 2.每個候選區經過CNN取得特徵 3.特徵交由每個SVM計算分數以及Bbox Regressor調整邊框範圍 1.訓練步驟繁瑣（微調網絡+訓練SVM +訓練bbox）2.訓練，測試均速度慢 3.訓練佔空間(每張圖片SS提取了2K個候選區) 1.從DPM HSC的34.3％直接提升到了66％（mAP）2.引入 CNN來做檢測問題 Fast R-CNN 1.使用Selective Search 提取 Region Propose 2. 利用在ImageNet 訓練的CNN網路提取特徵 3.Bounding Box regression配合多任務損失函數做邊框計算 4.Softmax做分類 1.先利用SS提取2K個候選區(邊框資訊) 2.整張圖用CNN取得特徵圖 3.將每個圖提取的邊框資訊(2K個候選區)對應到特徵圖上取得2K個特徵4.透過ROI Pooling將不同大小的特徵統一成同樣大小5.每個特徵交經過處理後由Softmax計算分數以及Bbox Regressor調整邊框範圍 1.依舊用SS提取RP（耗時2-3s，特徵提取耗時0.32s）2.無法滿足實時應用，沒有真正實現端到端訓練測試3.利用了GPU，但是RP是在CPU上運算 1.由66.9％提升至70％2.每張圖像耗時約為3s。 Faster R-CNN 1.使用Region Propose Network 提取 Region Propose 2. 利用在ImageNet 訓練的CNN網路提取特徵 3. BBox Regressor 配合多任務損失函數做邊框計算4.Softmax做分類 1.原圖先經過網路得到特徵圖 2.特徵圖經過RPN得到Proposed Region以及相應對分數 3. 採用NMS篩出Top-N個框(N=300) 4.透過ROI Pooling將不同大小的特徵統一成同樣大小5.N個篩選出的框對應回原特徵圖取得N特徵 6.每個特徵交經過處理後由Softmax計算分數以及Bbox Regressor調整邊框範圍 1.還是無法達到Real-Time detection 2.計算量還是比較大，因為經過RPN得到多個Proposed Region，再對每個RP做Classification的話，還是有大量的重複計算。 1.提出了RPN，換掉了速度瓶頸的關鍵(SS)，提高了檢測精度和速度2.真正實現end-to-end的目標檢測框架3.RPN生成bbox僅需約10ms。 流程圖 (1）輸入測試圖像;(2）將整張圖片輸入CNN，進行特徵提取;(3）用RPN生成建議窗口(建議)，每張圖片生成300個建議窗口;(4）把建議窗口映射到CNN的最後一層卷積feature map上;(5）通過RoI pooling層使每個RoI生成固定尺寸的特徵圖;(6）利用Softmax Loss（探測分類概率）和Smooth L1 Loss（探測邊框回歸）對分類概率和邊框回歸（Bounding box regression）聯合訓練。 Backbone原論文沒有特別提網路架構 一般來說是用ZF Net後來都是用VGG架構，甚至是後來用RESNET或是FPN這裡就VGG16來討論，此結構中需要注意的是所有的Conv層都是：kernel_size = 3，pad= 1所有的Pooling都是：kernel_size = 2，stride= 2Faster RCNN 中Conv層對所有的捲積都做了擴邊處理（pad = 1，即填充一圈0），導致原圖變為（M+2）x（N+2）大小，再做3x3卷積後輸出MxN 1輸出圖片大小計算：[（M + 2） - 3 +1] x [（N + 2） - 3 +1 ] 因此Conv層中的轉換次數層不改變輸入和輸出矩陣大小 說明：卷積核mxm，輸入圖片WxH（擴邊之後的尺寸），則輸出圖片的尺寸是（W-m + 1）x（H-m + 1） RPN AnchorROI POOLINGROI POOLING 概念如下圖所示","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://world4jason.github.io/tags/Deep-Learning/"},{"name":"R-CNN","slug":"R-CNN","permalink":"https://world4jason.github.io/tags/R-CNN/"},{"name":"CNN","slug":"CNN","permalink":"https://world4jason.github.io/tags/CNN/"},{"name":"Object Detection","slug":"Object-Detection","permalink":"https://world4jason.github.io/tags/Object-Detection/"}]},{"title":"Conditional GAN - 條件式生成對抗網路","date":"2017-11-08T05:48:12.000Z","path":"2017/11/08/Conditional-GAN/","text":"","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://world4jason.github.io/tags/Deep-Learning/"},{"name":"Generation","slug":"Generation","permalink":"https://world4jason.github.io/tags/Generation/"},{"name":"Generative Model","slug":"Generative-Model","permalink":"https://world4jason.github.io/tags/Generative-Model/"},{"name":"GAN","slug":"GAN","permalink":"https://world4jason.github.io/tags/GAN/"}]},{"title":"GAN - Generative Adversarial Network","date":"2017-11-07T15:00:02.000Z","path":"2017/11/07/GAN-Generative-Adversarial-Network/","text":"Generative Adversarial Networks (GANs)Lists Name Paper Link Value Function GAN Arxiv LSGAN Arxiv WGAN Arxiv WGAN-GP Arxiv DRAGAN Arxiv CGAN Arxiv infoGAN Arxiv ACGAN Arxiv EBGAN Arxiv BEGAN Arxiv Variants of GAN structure Results for mnistNetwork architecture of generator and discriminator is the exaclty sames as in infoGAN paper.For fair comparison of core ideas in all gan variants, all implementations for network architecture are kept same except EBGAN and BEGAN. Small modification is made for EBGAN/BEGAN, since those adopt auto-encoder strucutre for discriminator. But I tried to keep the capacity of discirminator. Random generationAll results are randomly sampled. Name Epoch 2 Epoch 10 Epoch 25 GAN LSGAN WGAN WGAN-GP DRAGAN EBGAN BEGAN Conditional generationEach row has the same noise vector and each column has the same label condition. Name Epoch 1 Epoch 10 Epoch 25 CGAN ACGAN infoGAN InfoGAN : Manipulating two continous codes Results for fashion-mnistComments on network architecture in mnist are also applied to here.Fashion-mnist is a recently proposed dataset consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. (T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot) Random generationAll results are randomly sampled. Name Epoch 1 Epoch 20 Epoch 40 GAN LSGAN WGAN WGAN-GP DRAGAN EBGAN BEGAN Conditional generationEach row has the same noise vector and each column has the same label condition. Name Epoch 1 Epoch 20 Epoch 40 CGAN ACGAN infoGAN Without hyper-parameter tuning from mnist-version, ACGAN/infoGAN does not work well as compared with CGAN.ACGAN tends to fall into mode-collapse.infoGAN tends to ignore noise-vector. It results in that various style within the same class can not be represented. InfoGAN : Manipulating two continous codes Some results for celebA(to be added) Variational Auto-Encoders (VAEs)Lists Name Paper Link Loss Function VAE Arxiv CVAE Arxiv DVAE Arxiv (to be added) AAE Arxiv (to be added) Variants of VAE structure Results for mnistNetwork architecture of decoder(generator) and encoder(discriminator) is the exaclty sames as in infoGAN paper. The number of output nodes in encoder is different. (2x z_dim for VAE, 1 for GAN) Random generationAll results are randomly sampled. Name Epoch 1 Epoch 10 Epoch 25 VAE GAN Results of GAN is also given to compare images generated from VAE and GAN.The main difference (VAE generates smooth and blurry images, otherwise GAN generates sharp and artifact images) is cleary observed from the results. Conditional generationEach row has the same noise vector and each column has the same label condition. Name Epoch 1 Epoch 10 Epoch 25 CVAE CGAN Results of CGAN is also given to compare images generated from CVAE and CGAN. Learned manifoldThe following results can be reproduced with command:1python main.py --dataset mnist --gan_type VAE --epoch 25 --batch_size 64 --dim_z 2 Please notice that dimension of noise-vector z is 2. Name Epoch 1 Epoch 10 Epoch 25 VAE Results for fashion-mnistComments on network architecture in mnist are also applied to here. The following results can be reproduced with command:1python main.py --dataset fashion-mnist --gan_type &lt;TYPE&gt; --epoch 40 --batch_size 64 Random generationAll results are randomly sampled. Name Epoch 1 Epoch 20 Epoch 40 VAE GAN Results of GAN is also given to compare images generated from VAE and GAN. Conditional generationEach row has the same noise vector and each column has the same label condition. Name Epoch 1 Epoch 20 Epoch 40 CVAE CGAN Results of CGAN is also given to compare images generated from CVAE and CGAN. Learned manifoldThe following results can be reproduced with command:1python main.py --dataset fashion-mnist --gan_type VAE --epoch 25 --batch_size 64 --dim_z 2 Please notice that dimension of noise-vector z is 2. Name Epoch 1 Epoch 10 Epoch 25 VAE","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://world4jason.github.io/tags/Deep-Learning/"},{"name":"Generation","slug":"Generation","permalink":"https://world4jason.github.io/tags/Generation/"},{"name":"Generative Model","slug":"Generative-Model","permalink":"https://world4jason.github.io/tags/Generative-Model/"},{"name":"GAN","slug":"GAN","permalink":"https://world4jason.github.io/tags/GAN/"}]},{"title":"Self-Study Courses List","date":"2017-11-07T03:26:18.000Z","path":"2017/11/07/Self-Study-Courses-List/","text":"學習路程 Python 政大 or MIT python都好 後者比較舊但作業比較多 ML 先看莫凡 看完再看NTU基礎的 剩下就看要看進階的還是standford的 Basic Concept of Machine Learning Morvan莫凡 有趣的機器學習 Machine Learning Udacity ud120 Machine Learning + Deep Learning Series National Taiwan University - 李宏毅 BASIC Machine Learning (2016,Fall) Machine Learning (2017,Spring) Machine Learning (2017,Spring) ADV Machine Learning and having it deep and structured (2017,Spring Machine Learning and having it deep and structured (2017,Fall)- National Taiwan University - 林軒田 機器學習基石 機器學習技法 Coursera Deep Learning Specialization blog Blog 資料科學・機器・人 GitHub-sjchoi86 https://github.com/sjchoi86/dl-workshop https://github.com/sjchoi86/dl_tutorials https://github.com/sjchoi86/Deep-Learning-101 https://github.com/sjchoi86/dl_tutorials_10weeks https://github.com/sjchoi86/dl_tutorials_3rd Deep Learning Specialize 網易雲中文版 Colorado Neural Networks and Deep Learning 2017 Fall Book 花書 Google Brain Hugo Larochelle Mooc.aiVisual Stanford CS213 2016 Syllabus 2016 Winter 2016 winter 中文版 2017 Syllabus 2017 Spring 2017 winter 中文版 Python NCCU Data science with Python MIT MIT Python Blog saltycrane 30天python雜談 Effective Python 59 Specific Ways to Write Better Python簡中翻譯-Python慣用語&amp;Effective Python Deep Learning Framework Tutorial Tensorflow Tensorflow 官方 Tensorflow 中文 Tensorflow 中文2 Stanford CS20SI - TensorFlow Udacity ud730 cognitiveclass.ai learning Tensorflow Web Stanford CS 20 Data Camp Pytorch Keras Data Camp 緯創資通 Tech Paper dl_tutorials dl_tutorials_10weeks Deep Learning in practice Fast.ai Fast.ai 2018 Course Fast.ai 2017 Course Tensorflow Tensorflow 101 Keras 緯創 MXNET -Gluon -Gluon-Github Udacity Self-drive self-driving-car Concept-CapsuleNet 李宏毅 Git Git Online IDE repl.it Online practice codewars Two Minutes Paper Two Minute Papers雷鋒網也有兩分鐘系列 Leiphone Video Collection Scikit-Learn-Data Camp 尚未整理http://open.163.com/special/opencourse/daishu.html http://www.kaierlong.me/blog/post/kaierlong/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%80%E8%AF%BE%E6%B1%87%E6%80%BB Cognitive Classhttps://cognitiveclass.ai/Cognitive Class - IBM Big Data Learnhttps://cognitiveclass.ai/learn/big-data/Cognitive Class - IBM Hadoop Foundations Learnhttps://cognitiveclass.ai/learn/hadoop/Cognitive Class - IBM Data Science Foundations Learnhttps://cognitiveclass.ai/learn/data-science/Cognitive Class - IBM Data Science for Business Learnhttps://cognitiveclass.ai/learn/data-science-business/Cognitive Class - IBM Deep Learning Learnhttps://cognitiveclass.ai/learn/deep-learning/ http://dlib.net/ml_guide.svg http://www.deepideas.net/deep-learning-from-scratch-theory-and-implementation/ https://www.ctolib.com/ http://blog.csdn.net/u011974639/article/details/73196349 https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html# https://my.oschina.net/hardbone/blog/798552https://jhui.github.io/ https://developers.google.cn/machine-learning/crash-course/ml-intro","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://world4jason.github.io/tags/Deep-Learning/"},{"name":"Self-Study","slug":"Self-Study","permalink":"https://world4jason.github.io/tags/Self-Study/"},{"name":"Courses","slug":"Courses","permalink":"https://world4jason.github.io/tags/Courses/"}]},{"title":"GAN - Github List","date":"2017-11-07T02:34:33.000Z","path":"2017/11/07/GAN-Github-List/","text":"paperListhttps://github.com/zhangqianhui/AdversarialNetsPapershttps://github.com/hindupuravinash/the-gan-zoohttps://github.com/nightrome/really-awesome-ganhttps://github.com/nashory/gans-awesome-applicationshttps://github.com/GKalliatakis/Delving-deep-into-GANs ====IMPLEMENTATION====https://github.com/YadiraF/GANhttps://github.com/jonbruner/generative-adversarial-networkshttps://github.com/tjwei/GANotebookshttps://github.com/AaronYALai/Generative_Adversarial_Networks_PyTorch 最完整的GAN實作https://github.com/znxlwm/pytorch-generative-model-collectionshttps://github.com/hwalsuklee/tensorflow-generative-model-collectionshttps://github.com/eriklindernoren/Keras-GAN 用貓玩GANhttps://github.com/AlexiaJM/Deep-learning-with-cats tensorflow跟GAN都有https://github.com/wiseodd/generative-models https://github.com/jtoy/awesome-tensorflow","tags":[{"name":"Generation","slug":"Generation","permalink":"https://world4jason.github.io/tags/Generation/"},{"name":"Generative Model","slug":"Generative-Model","permalink":"https://world4jason.github.io/tags/Generative-Model/"},{"name":"GAN","slug":"GAN","permalink":"https://world4jason.github.io/tags/GAN/"}]},{"title":"R-CNN:Region proposals+CNN","date":"2017-11-06T07:42:19.000Z","path":"2017/11/06/R-CNN/","text":"RCNN- 將CNN引入目標檢測的開山之作 CS231n lecture8 RCNN (論文：Rich feature hierarchies for accurate object detection and semantic segmentation) 是將CNN方法引入目標檢測領域， 大大提高了目標檢測效果，可以說改變了目標檢測領域的主要研究思路， 緊隨其後的系列文章： （ RCNN ）, Fast RCNN , Faster RCNN 代表該領域當前最高水準。 【論文主要特點】（相對傳統方法的改進） 速度： 經典的目標檢測算法使用滑動窗法依次判斷所有可能的區域。 本文則(採用Selective Search方法)預先提取一系列較可能是物體的候選區域，之後僅在這些候選區域上(採用CNN)提取特徵，進行判斷。訓練集： 經典的目標檢測算法在區域中提取人工設定的特徵。 本文則採用深度網絡進行特徵提取。 使用兩個數據庫： 一個較大的識​​別庫（ImageNet ILSVC 2012）：標定每張圖片中物體的類別。 一千萬圖像，1000類。 一個較小的檢測庫（PASCAL VOC 2007）：標定每張圖片中，物體的類別和位置，一萬圖像，20類。 本文使用識別庫進行預訓練得到CNN（有監督預訓練），而後用檢測庫調優參數，最後在檢測庫上評測。看到這裡也許你已經對很多名詞很困惑，下面會解釋。 先來看看它的基本流程： 基本流程 RCNN算法分為4個步驟 候選區域生成： 一張圖像生成1K~2K個候選區域（採用Selective Search 方法）特徵提取： 對每個候選區域，使用深度卷積網絡提取特徵（CNN）類別判斷： 特徵送入每一類的SVM 分類器，判別是否屬於該類位置精修： 使用回歸器精細修正候選框位置 【基礎知識===================================】 Selective Search主要思想:使用一種過分割手段，將圖像分割成小區域(1k~2k 個)查看現有小區域，按照合併規則合併可能性最高的相鄰兩個區域。 重複直到整張圖像合併成一個區域位置輸出所有曾經存在過的區域，所謂候選區域其中合併規則如下： 優先合併以下四種區域： 顏色（顏色直方圖）相近的紋理（梯度直方圖）相近的合併後總面積小的： 保證合併操作的尺度較為均勻，避免一個大區域陸續“吃掉”其他小區域（例：設有區域abcdefgh。較好的合併方式是：ab-cd-ef-gh - &gt; abcd-efgh -&gt; abcdefgh。 不好的合併方法是：ab-cdefgh -&gt;abcd-efgh -&gt;abcdef-gh -&gt; abcdefgh）合併後，總面積在其BBOX中所佔比例大的： 保證合併後形狀規則。 上述四條規則只涉及區域的顏色直方圖、梯度直方圖、面積和位置。 合併後的區域特徵可以直接由子區域特徵計算而來，速度較快。 有監督預訓練與無監督預訓練: (1)無監督預訓練(Unsupervised pre-training) 預訓練階段的樣本不需要人工標註數據，所以就叫做無監督預訓練。 (2)有監督預訓練(Supervised pre-training) 所謂的有監督預訓練也可以把它稱之為遷移學習。 比如你已經有一大堆標註好的人臉年齡分類的圖片數據，訓練了一個CNN，用於人臉的年齡識別。 然後當你遇到新的項目任務時：人臉性別識別，那麼這個時候你可以利用已經訓練好的年齡識別CNN模型，去掉最後一層，然後其它的網絡層參數就直接複製過來，繼續進行訓練，讓它輸出性別。 這就是所謂的遷移學習，說的簡單一點就是把一個任務訓練好的參數，拿到另外一個任務，作為神經網絡的初始參數值,這樣相比於你直接採用隨機初始化的方法，精度可以有很大的提高。 對於目標檢測問題： 圖片分類標註好的訓練數據非常多，但是物體檢測的標註數據卻很少，如何用少量的標註數據，訓練高質量的模型，這就是文獻最大的特點，這篇論文采用了遷移學習的思想：先用了ILSVRC2012這個訓練數據庫（這是一個圖片分類訓練數據庫），先進行網絡圖片分類訓練。 這個數據庫有大量的標註數據，共包含了1000種類別物體，因此預訓練階段CNN模型的輸出是1000個神經元（當然也直接可以採用Alexnet訓練好的模型參數）。 重疊度（IOU）:物體檢測需要定位出物體的bounding box，就像下面的圖片一樣，我們不僅要定位出車輛的bounding box 我們還要識別出bounding box 裡面的物體就是車輛。 對於bounding box的定位精度，有一個很重要的概念： 因為我們算法不可能百分百跟人工標註的數據完全匹配，因此就存在一個定位精度評價公式：IOU。 它定義了兩個bounding box的重疊度，如下圖所示 就是矩形框A、B的重疊面積佔A、B並集的面積比例。 非極大值抑制（ NMS ）：RCNN會從一張圖片中找出n個可能是物體的矩形框，然後為每個矩形框為做類別分類概率： 就像上面的圖片一樣，定位一個車輛，最後算法就找出了一堆的方框，我們需要判別哪些矩形框是沒用的。 非極大值抑制的方法是：先假設有6個矩形框，根據分類器的類別分類概率做排序，假設從小到大屬於車輛的概率分別為A、B、C、D、E、F。 (1)從最大概率矩形框F開始，分別判斷A~E與F的重疊度IOU是否大於某個設定的閾值; (2)假設B、D與F的重疊度超過閾值，那麼就扔掉B、D；並標記第一個矩形框F，是我們保留下來的。 (3)從剩下的矩形框A、C、E中，選擇概率最大的E，然後判斷E與A、C的重疊度，重疊度大於一定的閾值，那麼就扔掉；並標記E是我們保留下來的第二個矩形框。 就這樣一直重複，找到所有被保留下來的矩形框。 非極大值抑制（NMS）顧名思義就是抑制不是極大值的元素，搜索局部的極大值。 這個局部代表的是一個鄰域，鄰域有兩個參數可變，一是鄰域的維數，二是鄰域的大小。 這裡不討論通用的NMS算法，而是用於在目標檢測中用於提取分數最高的窗口的。 例如在行人檢測中，滑動窗口經提取特徵，經分類器分類識別後，每個窗口都會得到一個分數。 但是滑動窗口會導致很多窗口與其他窗口存在包含或者大部分交叉的情況。 這時就需要用到NMS來選取那些鄰域里分數最高（是行人的概率最大），並且抑制那些分數低的窗口。 ###VOC物體檢測任務: 相當於一個競賽，裡麵包含了20個物體類別： PASCAL VOC2011 Example Images 還有一個背景，總共就相當於21個類別，因此一會設計fine-tuning CNN的時候，我們softmax分類輸出層為21個神經元。 【各個階段詳解===================================】 總體思路再回顧： 首先對每一個輸入的圖片產生近2000個不分種類的候選區域（region proposals），然後使用CNNs從每個候選框中提取一個固定長度的特徵向量（4096維度），接著對每個取出的特徵向量使用特定種類的線性SVM進行分類。 也就是總個過程分為三個程序：a、找出候選框；b、利用CNN提取特徵向量；c、利用SVM進行特徵向量分類。 候選框搜索階段：當我們輸入一張圖片時，我們要搜索出所有可能是物體的區域，這裡採用的就是前面提到的Selective Search方法，通過這個算法我們搜索出2000個候選框。 然後從上面的總流程圖中可以看到，搜出的候選框是矩形的，而且是大小各不相同。 然而CNN對輸入圖片的大小是有固定的，如果把搜索到的矩形選框不做處理，就扔進CNN中，肯定不行。 因此對於每個輸入的候選框都需要縮放到固定的大小。 下面我們講解要怎麼進行縮放處理，為了簡單起見我們假設下一階段CNN所需要的輸入圖片大小是個正方形圖片227*227。 因為我們經過selective search 得到的是矩形框，paper試驗了兩種不同的處理方法： (1)各向異性縮放 這種方法很簡單，就是不管圖片的長寬比例，管它是否扭曲，進行縮放就是了，全部縮放到CNN輸入的大小227*227，如下圖(D)所示； (2)各向同性縮放 因為圖片扭曲後，估計會對後續CNN的訓練精度有影響，於是作者也測試了“各向同性縮放”方案。 有兩種辦法 A、先擴充後裁剪： 直接在原始圖片中，把bounding box的邊界進行擴展延伸成正方形，然後再進行裁剪；如果已經延伸到了原始圖片的外邊界，那麼就用bounding box中的顏色均值填充；如上圖(B)所示; B、先裁剪後擴充：先把bounding box圖片裁剪出來，然後用固定的背景顏色填充成正方形圖片(背景顏色也是採用bounding box的像素顏色均值),如上圖(C)所示; 對於上面的異性、同性縮放，文獻還有個padding處理，上面的示意圖中第1、3行就是結合了padding=0,第2、4行結果圖採用padding=16的結果。 經過最後的試驗，作者發現採用各向異性縮放、padding=16的精度最高。 （備註：候選框的搜索策略作者也考慮過使用一個滑動窗口的方法，然而由於更深的網絡，更大的輸入圖片和滑動步長，使得使用滑動窗口來定位的方法充滿了挑戰。） CNN特徵提取階段： 1、算法實現 a、網絡結構設計階段 網絡架構兩個可選方案：第一選擇經典的Alexnet；第二選擇VGG16。 經過測試Alexnet精度為58.5%，VGG16精度為66%。 VGG這個模型的特點是選擇比較小的捲積核、選擇較小的跨步，這個網絡的精度高，不過計算量是Alexnet的7倍。 後面為了簡單起見，我們就直接選用Alexnet，並進行講解；Alexnet特徵提取部分包含了5個卷積層、2個全連接層，在Alexnet中p5層神經元個數為9216、 f6、f7的神經元個數都是4096，通過這個網絡訓練完畢後，最後提取特徵每個輸入候選框圖片都能得到一個4096維的特徵向量。 b、網絡有監督預訓練階段（圖片數據庫：ImageNet ILSVC ） 參數初始化部分：物體檢測的一個難點在於，物體標籤訓練數據少，如果要直接採用隨機初始化CNN參數的方法，那麼目前的訓練數據量是遠遠不夠的。 這種情況下，最好的是採用某些方法，把參數初始化了，然後在進行有監督的參數微調，這里文獻採用的是有監督的預訓練。 所以paper在設計網絡結構的時候，是直接用Alexnet的網絡，然後連參數也是直接採用它的參數，作為初始的參數值，然後再fine-tuning訓練。 網絡優化求解時採用隨機梯度下降法，學習率大小為0.001； c、fine-tuning階段（圖片數據庫： PASCAL VOC） 我們接著採用selective search 搜索出來的候選框（PASCAL VOC 數據庫中的圖片） 繼續對上面預訓練的CNN模型進行fine-tuning訓練。 假設要檢測的物體類別有N類，那麼我們就需要把上面預訓練階段的CNN模型的最後一層給替換掉，替換成N+1個輸出的神經元(加1，表示還有一個背景) (20 + 1bg = 21)，然後這一層直接採用參數隨機初始化的方法，其它網絡層的參數不變；接著就可以開始繼續SGD訓練了。 開始的時候，SGD學習率選擇0.001，在每次訓練的時候，我們batch size大小選擇128，其中32個事正樣本、96個事負樣本。 關於正負樣本問題： 一張照片我們得到了2000個候選框。 然而人工標註的數據一張圖片中就只標註了正確的bounding box，我們搜索出來的2000個矩形框也不可能會出現一個與人工標註完全匹配的候選框。 因此在CNN階段我們需要用IOU為2000個bounding box打標籤。 如果用selective search挑選出來的候選框與物體的人工標註矩形框（PASCAL VOC的圖片都有人工標註）的重疊區域IoU大於0.5，那麼我們就把這個候選框標註成物體類別（正樣本），否則我們就把它當做背景類別（負樣本）。 （備註： 如果不針對特定任務進行fine-tuning，而是把CNN當做特徵提取器，卷積層所學到的特徵其實就是基礎的共享特徵提取層，就類似於SIFT算法一樣，可以用於提取各種圖片的特徵，而f6、f7所學習到的特徵是用於針對特定任務的特徵。打個比方：對於人臉性別識別來說，一個CNN模型前面的捲積層所學習到的特徵就類似於學習人臉共性特徵，然後全連接層所學習的特徵就是針對性別分類的特徵了） 2.疑惑點 ： CNN訓練的時候，本來就是對bounding box的物體進行識別分類訓練，在訓練的時候最後一層softmax就是分類層。 那麼為什麼作者閒著沒事幹要先用CNN做特徵提取（提取fc7層數據），然後再把提取的特徵用於訓練svm分類器？ 這個是因為svm訓練和cnn訓練過程的正負樣本定義方式各有不同，導致最後採用CNN softmax輸出比採用svm精度還低。 事情是這樣的，cnn在訓練的時候，對訓練數據做了比較寬鬆的標註，比如一個bounding box可能只包含物體的一部分，那麼我也把它標註為正樣本，用於訓練cnn；採用這個方法的主要原因在於因為CNN容易過擬合，所以需要大量的訓練數據，所以在CNN訓練階段我們是對Bounding box的位置限制條件限制的比較鬆(IOU只要大於0.5都被標註為正樣本了)；然而svm訓練的時候，因為svm適用於少樣本訓練，所以對於訓練樣本數據的IOU要求比較嚴格，我們只有當bounding box把整個物體都包含進去了，我們才把它標註為物體類別，然後訓練svm ，具體請看下文。 SVM訓練、測試階段 訓練階段 ： 這是一個二分類問題，我麼假設我們要檢測車輛。 我們知道只有當bounding box把整量車都包含在內，那才叫正樣本；如果bounding box 沒有包含到車輛，那麼我們就可以把它當做負樣本。 但問題是當我們的檢測窗口只有部分包含物體，那該怎麼定義正負樣本呢？ 作者測試了IOU閾值各種方案數值0,0.1,0.2,0.3,0.4,0.5。 最後通過訓練發現，如果選擇IOU閾值為0.3 效果最好 （選擇為0精度下降了4個百分點，選擇0.5精度下降了5個百分點）,即當重疊度小於0.3的時候，我們就把它標註為負樣本。 一旦CNN f7層特徵被提取出來，那麼我們將為每個物體類訓練一個svm分類器。 當我們用CNN提取2000個候選框，可以得到2000x4096這樣的特徵向量矩陣，然後我們只需要把這樣的一個矩陣與svm權值矩陣4096xN點乘(N為分類類別數目，因為我們訓練的N個svm，每個svm包含了4096個權值w)，就可以得到結果了。 得到的特徵輸入到SVM進行分類看看這個feature vector所對應的region proposal是需要的物體還是無關的實物(background) 。 排序，canny邊界檢測之後就得到了我們需要的bounding-box。 再回顧總結一下：整個系統分為三個部分：1.產生不依賴與特定類別的region proposals，這些region proposals定義了一個整個檢測器可以獲得的候選目標2.一個大的捲積神經網絡，對每個region產生一個固定長度的特徵向量3.一系列特定類別的線性SVM分類器。 位置精修： 目標檢測問題的衡量標準是重疊面積：許多看似準確的檢測結果，往往因為候選框不夠準確，重疊面積很小。 故需要一個位置精修步驟。 回歸器：對每一類目標，使用一個線性脊回歸器進行精修。 正則項λ=10000。 輸入為深度網絡pool5層的4096維特徵，輸出為xy方向的縮放和平移。 訓練樣本：判定為本類的候選框中和真值重疊面積大於0.6的候選框。 測試階段 ： 使用selective search的方法在測試圖片上提取2000個region propasals ，將每個region proposals歸一化到227x227，然後再CNN中正向傳播，將最後一層得到的特徵提取出來。 然後對於每一個類別，使用為這一類訓練的SVM分類器對提取的特徵向量進行打分，得到測試圖片中對於所有region proposals的對於這一類的分數，再使用貪心的非極大值抑制（ NMS）去除相交的多餘的框。 再對這些框進行canny邊緣檢測，就可以得到bounding-box(then B-BoxRegression)。 （非極大值抑制（NMS）先計算出每一個bounding box的面積，然後根據score進行排序，把score最大的bounding box作為選定的框，計算其餘bounding box與當前最大score與box的IoU，去除IoU大於設定的閾值的bounding box。然後重複上面的過程，直至候選bounding box為空，然後再將score小於一定閾值的選定框刪除得到這一類的結果（然後繼續進行下一個分類） 。作者提到花費在region propasals和提取特徵的時間是13s/張-GPU和53s/張-CPU，可以看出時間還是很長的，不能夠達到及時性。 完。 本文主要整理自以下文章： RCNN學習筆記(0):rcnn簡介RCNN學習筆記(1):Rich feature hierarchies for accurate object detection and semantic segmentationRCNN學習筆記(2):Rich feature hierarchies for accurate object detection and semantic segmentation《Rich feature hierarchies for Accurate Object Detection and Segmentation》《Spatial 《Pyramid Pooling in Deep Convolutional Networks for Visual Recognition》","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://world4jason.github.io/tags/Deep-Learning/"},{"name":"R-CNN","slug":"R-CNN","permalink":"https://world4jason.github.io/tags/R-CNN/"},{"name":"CNN","slug":"CNN","permalink":"https://world4jason.github.io/tags/CNN/"},{"name":"Object Detection","slug":"Object-Detection","permalink":"https://world4jason.github.io/tags/Object-Detection/"}]},{"title":"Object Detection with Convolution Neural Network Series","date":"2017-11-05T18:51:33.000Z","path":"2017/11/06/Object-Detection-with-Convolution-Neural-Network/","text":"Object Detection with Convolution Neural Network SeriesOutline 檢測任務? R-CNN Fast R-CNN Faster R-CNN SSD YOLO 檢測任務 R-CNN 步驟 候選區域生成： 一張圖像生成1K~2K個候選區域（採用Selective Search 方法） 特徵提取： 對每個候選區域，使用深度卷積網絡提取特徵（CNN） 類別判斷： 特徵送入每一類的SVM 分類器，判別是否屬於該類 位置精修： 使用回歸器精細修正候選框位置 用Non-Maximum Selection 合併後選框 Fast R-CNN 步驟1. 候選區域生成： 一張圖像生成1K~2K個候選區域（採用Selective Search 方法）2. 特徵提取： 該張圖片，使用深度卷積網絡提取特徵（CNN）3. ROI Pooling：4. 類別判斷： 特徵送入每一類的SVM 分類器，判別是否屬於該類5. 位置精修： 使用回歸器精細修正候選框位置6. 用Non-Maximum Selection 合併後選框## Faster R-CNN SSD YOLO Reference: http://zh.gluon.ai/","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://world4jason.github.io/tags/Deep-Learning/"},{"name":"R-CNN","slug":"R-CNN","permalink":"https://world4jason.github.io/tags/R-CNN/"},{"name":"CNN","slug":"CNN","permalink":"https://world4jason.github.io/tags/CNN/"},{"name":"Object Detection","slug":"Object-Detection","permalink":"https://world4jason.github.io/tags/Object-Detection/"}]},{"title":"CPP 筆記","date":"2017-10-19T04:07:48.000Z","path":"2017/10/19/CPP-note/","text":"##C++中cin、cin.get()、cin.getline()、getline()、gets()等函数的用法学C++的时候，这几个输入函数弄的有点迷糊；这里做个小结，为了自己复习，也希望对后来者能有所帮助，如果有差错的地方还请各位多多指教 1、cin2、cin.get()3、cin.getline()4、getline()5、gets()6、getchar() 附:cin.ignore();cin.get()//跳过一个字符,例如不想要的回车,空格等字符 1、cin&gt;&gt; 用法1：最基本，也是最常用的用法，输入一个数字： 12345678#include &lt;iostream&gt; using namespace std; main () &#123; int a,b; cin&gt;&gt;a&gt;&gt;b; cout&lt;&lt;a+b&lt;&lt;endl; &#125; 输入：2[回车]3[回车]输出：5 注意:&gt;&gt; 是会过滤掉不可见字符（如 空格 回车，TAB 等）cin&gt;&gt;noskipws&gt;&gt;input[j];//不想略过空白字符，那就使用 noskipws 流控制 用法2：接受一个字符串，遇“空格”、“TAB”、“回车”都结束 12345678#include &lt;iostream&gt; using namespace std; main () &#123; char a[20]; cin&gt;&gt;a; cout&lt;&lt;a&lt;&lt;endl; &#125; 输入：jkljkljkl输出：jkljkljkl 输入：jkljkl jkljkl //遇空格结束输出：jkljkl 2、cin.get() 用法1： cin.get(字符变量名)可以用来接收字符 12345678#include &lt;iostream&gt; using namespace std; main () &#123; char ch; ch=cin.get(); //或者cin.get(ch); cout&lt;&lt;ch&lt;&lt;endl; &#125; 输入：jljkljkl输出：j 用法2：cin.get(字符数组名,接收字符数目)用来接收一行字符串,可以接收空格 12345678#include &lt;iostream&gt; using namespace std; main () &#123; char a[20]; cin.get(a,20); cout&lt;&lt;a&lt;&lt;endl; &#125; 输入：jkl jkl jkl输出：jkl jkl jkl 输入：abcdeabcdeabcdeabcdeabcde （输入25个字符）输出：abcdeabcdeabcdeabcd （接收19个字符+1个’\\0’） 用法3：cin.get(无参数)没有参数主要是用于舍弃输入流中的不需要的字符,或者舍弃回车,弥补cin.get(字符数组名,接收字符数目)的不足. 这个我还不知道怎么用，知道的前辈请赐教； 3、cin.getline() // 接受一个字符串，可以接收空格并输出 12345678#include &lt;iostream&gt; using namespace std; main () &#123; char m[20]; cin.getline(m,5); cout&lt;&lt;m&lt;&lt;endl; &#125; 输入：jkljkljkl输出：jklj 接受5个字符到m中，其中最后一个为’\\0’，所以只看到4个字符输出； 如果把5改成20：输入：jkljkljkl输出：jkljkljkl 输入：jklf fjlsjf fjsdklf输出：jklf fjlsjf fjsdklf //延伸：//cin.getline()实际上有三个参数，cin.getline(接受字符串的看哦那间m,接受个数5,结束字符)//当第三个参数省略时，系统默认为’\\0’//如果将例子中cin.getline()改为cin.getline(m,5,’a’);当输入jlkjkljkl时输出jklj，输入jkaljkljkl时，输出jk 当用在多维数组中的时候，也可以用cin.getline(m[i],20)之类的用法： 123456789101112131415161718#include&lt;iostream&gt; #include&lt;string&gt; using namespace std;main () &#123; char m[3][20]; for(int i=0;i&lt;3;i++) &#123; cout&lt;&lt;\"\\n请输入第\"&lt;&lt;i+1&lt;&lt;\"个字符串：\"&lt;&lt;endl; cin.getline(m[i],20); &#125;cout&lt;&lt;endl; for(int j=0;j&lt;3;j++) cout&lt;&lt;\"输出m[\"&lt;&lt;j&lt;&lt;\"]的值:\"&lt;&lt;m[j]&lt;&lt;endl;&#125; 请输入第1个字符串：kskr1 请输入第2个字符串：kskr2 请输入第3个字符串：kskr3 输出m[0]的值:kskr1输出m[1]的值:kskr2输出m[2]的值:kskr3 4、getline() // 接受一个字符串，可以接收空格并输出，需包含“#include” 123456789#include&lt;iostream&gt; #include&lt;string&gt; using namespace std; main () &#123; string str; getline(cin,str); cout&lt;&lt;str&lt;&lt;endl; &#125; 输入：jkljkljkl输出：jkljkljkl 输入：jkl jfksldfj jklsjfl输出：jkl jfksldfj jklsjfl 和cin.getline()类似，但是cin.getline()属于istream流，而getline()属于string流，是不一样的两个函数 在寫程式的時候遇到這個問題因為atoi函式只吃char*上網GOOGLE一下找到了解決之法 123string x;int temp=atoi(x.c_str());c_str()可以轉換string成為char* 12345678910111213141516171819#include &lt;iostream&gt;using namespace std ; int main()&#123; string s ; int a; while (getline(cin,s)) &#123; int n=s.length(); for(int i=0;i&lt;n;i++) &#123; a=s.at(i); cout &lt;&lt; a &lt;&lt; \" \"; &#125; cout &lt;&lt; endl; &#125; return 0 ;&#125;","tags":[{"name":"code","slug":"code","permalink":"https://world4jason.github.io/tags/code/"},{"name":"CPP","slug":"CPP","permalink":"https://world4jason.github.io/tags/CPP/"},{"name":"C++","slug":"C","permalink":"https://world4jason.github.io/tags/C/"}]},{"title":"Hello World","date":"2017-03-28T06:57:15.000Z","path":"2017/03/28/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]}]