[{"title":"Person re-ID 綜述","date":"2018-01-02T13:50:49.000Z","path":"2018/01/02/person_re-ID_summary/","text":"目標難度 目標遮擋（Occlusion）導致部分特徵丟失 不同的 View，Illumination 導致同一目標的特徵差異 不同目標衣服顏色近似、特徵近似導致區分度下降 解決方案1. Representation learning + ReID看做分類(Classification/Identification)問題或者驗證(Verification)問題：(1) 分類問題是指利用行人的ID或者屬性等作為訓練標籤來訓練模型；(2) 驗證問題是指輸入一對（兩張）行人圖片，讓網絡來學習這兩張圖片是否屬於同一個行人。 Classification/Identification loss和verification loss 額外改進方向[2]是在加上許多行人的label，像是性別、頭髮以及服裝等等。 2. Metric learning + ReID常用於圖像檢索的方法，通過網絡學習出兩張圖片的相似度。(Contrastive loss)[5]、三元組損失(Triplet loss)、 四元組損失(Quadruplet loss)、難樣本採樣三元組損失(Triplet hard loss with batch hard mining, TriHard loss)、邊界挖掘損失(Margin sample mining loss, MSML Contrastive loss 基本上就是Siamese CNN 訓練時是三個正樣本一個副樣本，test時未知 3. 基于局部特征的ReID方法論文[3]用local feature而不用global feature，切割好以後送到LSTM去學 但論文[3]會有對齊問題，所以論文[4]用pose跟skeleton來做姿勢預測，再通過仿射變換對齊 論文[5]直接拿關節點切出ROI，14個人體關節點，得到7個ROI區域，(頭、上身、下身和四肢) 4. Video Sequence + ReID這方向不熟 貼兩張圖參考參考而已 5. GAN + ReIDReID數據集目前最大的也只有幾千個ID，跟萬張圖片而已，CNN based還容易overfittingGAN主要是用在遷移學習跟基於條件的生成 第一篇就是ICCV2017的論文[5]以及後來同作者改進的論文[6]，是可以避免overfitting但生成效果就很慘 為了處理不同數據集，甚至是不同camera所造成bias的問題，論文[7]是利用cycleGAN based的設計，利用遷移學習來處理兩個數同數據集的問題，先切割分前景跟背景，在轉換過去。 Pose Normalization[8] 資料種類 Video-based Image-based Long-term activity Individual action 資料庫Robust Systems Lab 程式碼简单行人重识别代码到88%准确率https://github.com/layumi/Person_reID_baseline_pytorch ICCV 2017 Cross-view Asymmetric Metric Learning for Unsupervised Re-id Deeply-Learned Part-Aligned Representations for Person Re-Identification In Defense of the Triplet Loss for Person Re-Identification Jointly Attentive Spatial-Temporal Pooling Networks for Video-based Person Re-Identification SVDNet for Pedestrian Retrieval Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro 沒code CVPR 2017 沒code - Point to Set Similarity Based Deep Feature Learning for Person Re-Identification - Fast Person Re-Identification via Cross-Camera Semantic Binary Transformation - See the Forest for the Trees: Joint Spatial and Temporal Recurrent Neural Networks for Video-Based Person Re-Identification - Learning Deep Context-Aware Features Over Body and Latent Parts for Person Re-Identification - Consistent-Aware Deep Learning for Person Re-Identification in a Camera Network - Re-Ranking Person Re-Identification With k-Reciprocal Encoding - Multiple People Tracking by Lifted Multicut and Person Re-Identification [1] Mengyue Geng, Yaowei Wang, Tao Xiang, Yonghong Tian. Deep transfer learning for person reidentification[J]. arXiv preprint arXiv:1611.05244, 2016. [2] Yutian Lin, Liang Zheng, Zhedong Zheng, YuWu, Yi Yang. Improving person re-identification by attribute and identity learning[J]. arXiv preprint arXiv:1703.07220, 2017. [3] Rahul Rama Varior, Bing Shuai, Jiwen Lu, Dong Xu, Gang Wang. A siamese long short-term memory architecture for human re-identification[C]//European Conference on Computer Vision. Springer, 2016:135–153. [4]Liang Zheng, Yujia Huang, Huchuan Lu, Yi Yang. Pose invariant embedding for deep person reidentification[J]. arXiv preprint arXiv:1701.07732, 2017. [5] Zheng Z, Zheng L, Yang Y. Unlabeled samples generated by gan improve the person re-identification baseline in vitro[J]. arXiv preprint arXiv:1701.07717, 2017. [6] Zhong Z, Zheng L, Zheng Z, et al. Camera Style Adaptation for Person Re-identification[J]. arXiv preprint arXiv:1711.10295, 2017. [7] Wei L, Zhang S, Gao W, et al. Person Transfer GAN to Bridge Domain Gap for Person Re-Identification[J]. arXiv preprint arXiv:1711.08565, 2017. [8] Qian X, Fu Y, Wang W, et al. Pose-Normalized Image Generation for Person Re-identification[J]. arXiv preprint arXiv:1712.02225, 2017.","tags":[{"name":"tracking","slug":"tracking","permalink":"https://world4jason.github.io/tags/tracking/"},{"name":"image retrieval","slug":"image-retrieval","permalink":"https://world4jason.github.io/tags/image-retrieval/"}]},{"title":"PYTHON中如何使用*ARGS和**KWARGS","date":"2017-12-24T10:48:14.000Z","path":"2017/12/24/PYTHON中如何使用-ARGS和-KWARGS/","text":"範例與翻譯理解自連結與連結 *args跟 **kwargs是類似的東西，是可有可無的參數。 一顆星的*args是tuple，可以接受很多的值。兩顆星的**kwargs一樣是可以接受很多值，但是是接受dictionary。###一顆星用法 範例 123456def test_var_args(farg, *args): print \"formal arg:\", farg for arg in args: print \"another arg:\", arg test_var_args(1, \"two\", 3) 结果 123formal arg: 1another arg: twoanother arg: 3 ###兩顆星用法 範例 123456def test_var_kwargs(farg, **kwargs): print \"formal arg:\", farg for key in kwargs: print \"another keyword arg: %s: %s\" % (key, kwargs[key])test_var_kwargs(farg=1, myarg2=\"two\", myarg3=3) 结果 123formal arg: 1another keyword arg: myarg2: twoanother keyword arg: myarg3: 3 順序問題如果function定義時如上圖先放了tuple才是dictionary，呼叫時參數先放dictionary再放tuple會跳error。","tags":[{"name":"code","slug":"code","permalink":"https://world4jason.github.io/tags/code/"}]},{"title":"Python 筆記","date":"2017-12-13T07:35:34.000Z","path":"2017/12/13/python-note/","text":"Different in Py2 and Py3 Pickle module 12345import sysif sys.version_info[0]&lt;3: import cPickle as pickleelse import _pickle as pickle","tags":[{"name":"code","slug":"code","permalink":"https://world4jason.github.io/tags/code/"}]},{"title":"zi2zi: Master Chinese Calligraphy with Conditional Adversarial Networks","date":"2017-11-30T19:36:19.000Z","path":"2017/12/01/zi2zi/","text":"Generated samples. Related code can be found here 目標字體風格轉換 動機直接用CNN進行風格轉換會有下列問題 生成常常是模糊的 多數生成結果是失敗的 只能做一對一生成 結論：用GAN試試看 用GAN秒殺一切！這篇借鑒了三篇paper內容 Image-to-Image Translation with Conditional Adversarial NetworksConditional Image Synthesis With Auxiliary Classifier GANsUnsupervised Cross-Domain Image Generation 主要是由pix2pix這篇修改而來的 其中Encoder跟Decoder還有Discriminator是直接用pix2pix的, 尤其是裡面的Unet模型","tags":[{"name":"Generation","slug":"Generation","permalink":"https://world4jason.github.io/tags/Generation/"},{"name":"Generative Model","slug":"Generative-Model","permalink":"https://world4jason.github.io/tags/Generative-Model/"},{"name":"GAN","slug":"GAN","permalink":"https://world4jason.github.io/tags/GAN/"},{"name":"Style Transfer","slug":"Style-Transfer","permalink":"https://world4jason.github.io/tags/Style-Transfer/"}]},{"title":"Mask R-CNN","date":"2017-11-09T18:13:59.000Z","path":"2017/11/10/Mask R-CNN/","text":"Overview ConceptMask R-CNN 利用了相當簡潔與彈性的方法進行實例分割, 主要跟 Faster R-CNN 不同的地方在於原架構有 2 個分支 Classification branch Bounding box branch 而 Mask R-CNN 的方法則是多加了另一個分支 — Mask branch。 Mask branchLoss functionLtotal = Lcls + Lbox + Lmask Lcls 跟 Lbox的可以參考Fast-RCNN 1J(θ)=−1m[∑mi=1y(i)loghθ(x(i))+(1−y(i))log(1−hθ(x(i)))] ROI POOLINGROI POOLING 概念如下圖所示","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://world4jason.github.io/tags/Deep-Learning/"},{"name":"R-CNN","slug":"R-CNN","permalink":"https://world4jason.github.io/tags/R-CNN/"},{"name":"Segmentation","slug":"Segmentation","permalink":"https://world4jason.github.io/tags/Segmentation/"},{"name":"Pose Estimation","slug":"Pose-Estimation","permalink":"https://world4jason.github.io/tags/Pose-Estimation/"}]},{"title":"Conditional GAN - 條件式生成對抗網路","date":"2017-11-08T05:48:12.000Z","path":"2017/11/08/Conditional-GAN/","text":"","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://world4jason.github.io/tags/Deep-Learning/"},{"name":"Generation","slug":"Generation","permalink":"https://world4jason.github.io/tags/Generation/"},{"name":"Generative Model","slug":"Generative-Model","permalink":"https://world4jason.github.io/tags/Generative-Model/"},{"name":"GAN","slug":"GAN","permalink":"https://world4jason.github.io/tags/GAN/"}]},{"title":"GAN - Generative Adversarial Network","date":"2017-11-07T15:00:02.000Z","path":"2017/11/07/GAN-Generative-Adversarial-Network/","text":"Generative Adversarial Networks (GANs)Lists Name Paper Link Value Function GAN Arxiv LSGAN Arxiv WGAN Arxiv WGAN-GP Arxiv DRAGAN Arxiv CGAN Arxiv infoGAN Arxiv ACGAN Arxiv EBGAN Arxiv BEGAN Arxiv Variants of GAN structure Results for mnistNetwork architecture of generator and discriminator is the exaclty sames as in infoGAN paper.For fair comparison of core ideas in all gan variants, all implementations for network architecture are kept same except EBGAN and BEGAN. Small modification is made for EBGAN/BEGAN, since those adopt auto-encoder strucutre for discriminator. But I tried to keep the capacity of discirminator. Random generationAll results are randomly sampled. Name Epoch 2 Epoch 10 Epoch 25 GAN LSGAN WGAN WGAN-GP DRAGAN EBGAN BEGAN Conditional generationEach row has the same noise vector and each column has the same label condition. Name Epoch 1 Epoch 10 Epoch 25 CGAN ACGAN infoGAN InfoGAN : Manipulating two continous codes Results for fashion-mnistComments on network architecture in mnist are also applied to here.Fashion-mnist is a recently proposed dataset consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. (T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot) Random generationAll results are randomly sampled. Name Epoch 1 Epoch 20 Epoch 40 GAN LSGAN WGAN WGAN-GP DRAGAN EBGAN BEGAN Conditional generationEach row has the same noise vector and each column has the same label condition. Name Epoch 1 Epoch 20 Epoch 40 CGAN ACGAN infoGAN Without hyper-parameter tuning from mnist-version, ACGAN/infoGAN does not work well as compared with CGAN.ACGAN tends to fall into mode-collapse.infoGAN tends to ignore noise-vector. It results in that various style within the same class can not be represented. InfoGAN : Manipulating two continous codes Some results for celebA(to be added) Variational Auto-Encoders (VAEs)Lists Name Paper Link Loss Function VAE Arxiv CVAE Arxiv DVAE Arxiv (to be added) AAE Arxiv (to be added) Variants of VAE structure Results for mnistNetwork architecture of decoder(generator) and encoder(discriminator) is the exaclty sames as in infoGAN paper. The number of output nodes in encoder is different. (2x z_dim for VAE, 1 for GAN) Random generationAll results are randomly sampled. Name Epoch 1 Epoch 10 Epoch 25 VAE GAN Results of GAN is also given to compare images generated from VAE and GAN.The main difference (VAE generates smooth and blurry images, otherwise GAN generates sharp and artifact images) is cleary observed from the results. Conditional generationEach row has the same noise vector and each column has the same label condition. Name Epoch 1 Epoch 10 Epoch 25 CVAE CGAN Results of CGAN is also given to compare images generated from CVAE and CGAN. Learned manifoldThe following results can be reproduced with command:1python main.py --dataset mnist --gan_type VAE --epoch 25 --batch_size 64 --dim_z 2 Please notice that dimension of noise-vector z is 2. Name Epoch 1 Epoch 10 Epoch 25 VAE Results for fashion-mnistComments on network architecture in mnist are also applied to here. The following results can be reproduced with command:1python main.py --dataset fashion-mnist --gan_type &lt;TYPE&gt; --epoch 40 --batch_size 64 Random generationAll results are randomly sampled. Name Epoch 1 Epoch 20 Epoch 40 VAE GAN Results of GAN is also given to compare images generated from VAE and GAN. Conditional generationEach row has the same noise vector and each column has the same label condition. Name Epoch 1 Epoch 20 Epoch 40 CVAE CGAN Results of CGAN is also given to compare images generated from CVAE and CGAN. Learned manifoldThe following results can be reproduced with command:1python main.py --dataset fashion-mnist --gan_type VAE --epoch 25 --batch_size 64 --dim_z 2 Please notice that dimension of noise-vector z is 2. Name Epoch 1 Epoch 10 Epoch 25 VAE","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://world4jason.github.io/tags/Deep-Learning/"},{"name":"Generation","slug":"Generation","permalink":"https://world4jason.github.io/tags/Generation/"},{"name":"Generative Model","slug":"Generative-Model","permalink":"https://world4jason.github.io/tags/Generative-Model/"},{"name":"GAN","slug":"GAN","permalink":"https://world4jason.github.io/tags/GAN/"}]},{"title":"Self-Study Courses List","date":"2017-11-07T03:26:18.000Z","path":"2017/11/07/Self-Study-Courses-List/","text":"學習路程 Python 政大 or MIT python都好 後者比較舊但作業比較多 ML 先看莫凡 看完再看NTU基礎的 剩下就看要看進階的還是standford的 Basic Concept of Machine Learning Morvan莫凡 有趣的機器學習 Machine Learning-Udacity ud120 Machine Learning + Deep Learning Series National Taiwan University - 李宏毅 BASIC Machine Learning (2016,Fall) Machine Learning (2017,Spring) Machine Learning (2017,Spring) ADV Machine Learning and having it deep and structured (2017,Spring Machine Learning and having it deep and structured (2017,Fall)- National Taiwan University - 林軒田 機器學習基石 機器學習技法 Coursera Deep Learning Specialization blog Blog 資料科學・機器・人 Deep Learning Specialize 網易雲中文版 Colorado Neural Networks and Deep Learning 2017 Fall Book 花書 Visual Stanford CS213 2016 Syllabus 2016 Winter 2016 winter 中文版 2017 Syllabus 2017 Spring 2017 winter 中文版 Python NCCU Data science with Python MIT MIT Python Blog saltycrane 30天python雜談 Effective Python 59 Specific Ways to Write Better Python簡中翻譯-Python慣用語&amp;Effective Python Deep Learning Framework Tutorial Tensorflow Tensorflow 官方 Tensorflow 中文 Tensorflow 中文2 Stanford CS20SI - TensorFlow Udacity ud730 cognitiveclass.ai learning Tensorflow Web Pytorch Tech Paper dl_tutorials dl_tutorials_10weeks Deep Learning in practice Fast.ai Keras 緯創 MXNET -Gluon -Gluon-Github Concept-CapsuleNet 李宏毅 Online IDE repl.it Online practice codewars http://open.163.com/special/opencourse/daishu.html http://www.kaierlong.me/blog/post/kaierlong/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%80%E8%AF%BE%E6%B1%87%E6%80%BB Cognitive Classhttps://cognitiveclass.ai/Cognitive Class - IBM Big Data Learnhttps://cognitiveclass.ai/learn/big-data/Cognitive Class - IBM Hadoop Foundations Learnhttps://cognitiveclass.ai/learn/hadoop/Cognitive Class - IBM Data Science Foundations Learnhttps://cognitiveclass.ai/learn/data-science/Cognitive Class - IBM Data Science for Business Learnhttps://cognitiveclass.ai/learn/data-science-business/Cognitive Class - IBM Deep Learning Learnhttps://cognitiveclass.ai/learn/deep-learning/","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://world4jason.github.io/tags/Deep-Learning/"},{"name":"Courses","slug":"Courses","permalink":"https://world4jason.github.io/tags/Courses/"},{"name":"Self-Study","slug":"Self-Study","permalink":"https://world4jason.github.io/tags/Self-Study/"}]},{"title":"GAN - Github List","date":"2017-11-07T02:34:33.000Z","path":"2017/11/07/GAN-Github-List/","text":"paperListhttps://github.com/zhangqianhui/AdversarialNetsPapershttps://github.com/hindupuravinash/the-gan-zoohttps://github.com/nightrome/really-awesome-ganhttps://github.com/nashory/gans-awesome-applicationshttps://github.com/GKalliatakis/Delving-deep-into-GANs ====IMPLEMENTATION====https://github.com/YadiraF/GANhttps://github.com/jonbruner/generative-adversarial-networkshttps://github.com/tjwei/GANotebookshttps://github.com/AaronYALai/Generative_Adversarial_Networks_PyTorch 最完整的GAN實作https://github.com/znxlwm/pytorch-generative-model-collectionshttps://github.com/hwalsuklee/tensorflow-generative-model-collectionshttps://github.com/eriklindernoren/Keras-GAN 用貓玩GANhttps://github.com/AlexiaJM/Deep-learning-with-cats tensorflow跟GAN都有https://github.com/wiseodd/generative-models https://github.com/jtoy/awesome-tensorflow","tags":[{"name":"Generation","slug":"Generation","permalink":"https://world4jason.github.io/tags/Generation/"},{"name":"Generative Model","slug":"Generative-Model","permalink":"https://world4jason.github.io/tags/Generative-Model/"},{"name":"GAN","slug":"GAN","permalink":"https://world4jason.github.io/tags/GAN/"}]},{"title":"R-CNN:Region proposals+CNN","date":"2017-11-06T07:42:19.000Z","path":"2017/11/06/R-CNN/","text":"RCNN- 將CNN引入目標檢測的開山之作 CS231n lecture8 RCNN (論文：Rich feature hierarchies for accurate object detection and semantic segmentation) 是將CNN方法引入目標檢測領域， 大大提高了目標檢測效果，可以說改變了目標檢測領域的主要研究思路， 緊隨其後的系列文章： （ RCNN ）, Fast RCNN , Faster RCNN 代表該領域當前最高水準。 【論文主要特點】（相對傳統方法的改進） 速度： 經典的目標檢測算法使用滑動窗法依次判斷所有可能的區域。 本文則(採用Selective Search方法)預先提取一系列較可能是物體的候選區域，之後僅在這些候選區域上(採用CNN)提取特徵，進行判斷。訓練集： 經典的目標檢測算法在區域中提取人工設定的特徵。 本文則採用深度網絡進行特徵提取。 使用兩個數據庫： 一個較大的識​​別庫（ImageNet ILSVC 2012）：標定每張圖片中物體的類別。 一千萬圖像，1000類。 一個較小的檢測庫（PASCAL VOC 2007）：標定每張圖片中，物體的類別和位置，一萬圖像，20類。 本文使用識別庫進行預訓練得到CNN（有監督預訓練），而後用檢測庫調優參數，最後在檢測庫上評測。看到這裡也許你已經對很多名詞很困惑，下面會解釋。 先來看看它的基本流程： 基本流程 RCNN算法分為4個步驟 候選區域生成： 一張圖像生成1K~2K個候選區域（採用Selective Search 方法）特徵提取： 對每個候選區域，使用深度卷積網絡提取特徵（CNN）類別判斷： 特徵送入每一類的SVM 分類器，判別是否屬於該類位置精修： 使用回歸器精細修正候選框位置 【基礎知識===================================】 Selective Search主要思想:使用一種過分割手段，將圖像分割成小區域(1k~2k 個)查看現有小區域，按照合併規則合併可能性最高的相鄰兩個區域。 重複直到整張圖像合併成一個區域位置輸出所有曾經存在過的區域，所謂候選區域其中合併規則如下： 優先合併以下四種區域： 顏色（顏色直方圖）相近的紋理（梯度直方圖）相近的合併後總面積小的： 保證合併操作的尺度較為均勻，避免一個大區域陸續“吃掉”其他小區域（例：設有區域abcdefgh。較好的合併方式是：ab-cd-ef-gh - &gt; abcd-efgh -&gt; abcdefgh。 不好的合併方法是：ab-cdefgh -&gt;abcd-efgh -&gt;abcdef-gh -&gt; abcdefgh）合併後，總面積在其BBOX中所佔比例大的： 保證合併後形狀規則。 上述四條規則只涉及區域的顏色直方圖、梯度直方圖、面積和位置。 合併後的區域特徵可以直接由子區域特徵計算而來，速度較快。 有監督預訓練與無監督預訓練: (1)無監督預訓練(Unsupervised pre-training) 預訓練階段的樣本不需要人工標註數據，所以就叫做無監督預訓練。 (2)有監督預訓練(Supervised pre-training) 所謂的有監督預訓練也可以把它稱之為遷移學習。 比如你已經有一大堆標註好的人臉年齡分類的圖片數據，訓練了一個CNN，用於人臉的年齡識別。 然後當你遇到新的項目任務時：人臉性別識別，那麼這個時候你可以利用已經訓練好的年齡識別CNN模型，去掉最後一層，然後其它的網絡層參數就直接複製過來，繼續進行訓練，讓它輸出性別。 這就是所謂的遷移學習，說的簡單一點就是把一個任務訓練好的參數，拿到另外一個任務，作為神經網絡的初始參數值,這樣相比於你直接採用隨機初始化的方法，精度可以有很大的提高。 對於目標檢測問題： 圖片分類標註好的訓練數據非常多，但是物體檢測的標註數據卻很少，如何用少量的標註數據，訓練高質量的模型，這就是文獻最大的特點，這篇論文采用了遷移學習的思想：先用了ILSVRC2012這個訓練數據庫（這是一個圖片分類訓練數據庫），先進行網絡圖片分類訓練。 這個數據庫有大量的標註數據，共包含了1000種類別物體，因此預訓練階段CNN模型的輸出是1000個神經元（當然也直接可以採用Alexnet訓練好的模型參數）。 重疊度（IOU）:物體檢測需要定位出物體的bounding box，就像下面的圖片一樣，我們不僅要定位出車輛的bounding box 我們還要識別出bounding box 裡面的物體就是車輛。 對於bounding box的定位精度，有一個很重要的概念： 因為我們算法不可能百分百跟人工標註的數據完全匹配，因此就存在一個定位精度評價公式：IOU。 它定義了兩個bounding box的重疊度，如下圖所示 就是矩形框A、B的重疊面積佔A、B並集的面積比例。 非極大值抑制（ NMS ）：RCNN會從一張圖片中找出n個可能是物體的矩形框，然後為每個矩形框為做類別分類概率： 就像上面的圖片一樣，定位一個車輛，最後算法就找出了一堆的方框，我們需要判別哪些矩形框是沒用的。 非極大值抑制的方法是：先假設有6個矩形框，根據分類器的類別分類概率做排序，假設從小到大屬於車輛的概率分別為A、B、C、D、E、F。 (1)從最大概率矩形框F開始，分別判斷A~E與F的重疊度IOU是否大於某個設定的閾值; (2)假設B、D與F的重疊度超過閾值，那麼就扔掉B、D；並標記第一個矩形框F，是我們保留下來的。 (3)從剩下的矩形框A、C、E中，選擇概率最大的E，然後判斷E與A、C的重疊度，重疊度大於一定的閾值，那麼就扔掉；並標記E是我們保留下來的第二個矩形框。 就這樣一直重複，找到所有被保留下來的矩形框。 非極大值抑制（NMS）顧名思義就是抑制不是極大值的元素，搜索局部的極大值。 這個局部代表的是一個鄰域，鄰域有兩個參數可變，一是鄰域的維數，二是鄰域的大小。 這裡不討論通用的NMS算法，而是用於在目標檢測中用於提取分數最高的窗口的。 例如在行人檢測中，滑動窗口經提取特徵，經分類器分類識別後，每個窗口都會得到一個分數。 但是滑動窗口會導致很多窗口與其他窗口存在包含或者大部分交叉的情況。 這時就需要用到NMS來選取那些鄰域里分數最高（是行人的概率最大），並且抑制那些分數低的窗口。 ###VOC物體檢測任務: 相當於一個競賽，裡麵包含了20個物體類別： PASCAL VOC2011 Example Images 還有一個背景，總共就相當於21個類別，因此一會設計fine-tuning CNN的時候，我們softmax分類輸出層為21個神經元。 【各個階段詳解===================================】 總體思路再回顧： 首先對每一個輸入的圖片產生近2000個不分種類的候選區域（region proposals），然後使用CNNs從每個候選框中提取一個固定長度的特徵向量（4096維度），接著對每個取出的特徵向量使用特定種類的線性SVM進行分類。 也就是總個過程分為三個程序：a、找出候選框；b、利用CNN提取特徵向量；c、利用SVM進行特徵向量分類。 候選框搜索階段：當我們輸入一張圖片時，我們要搜索出所有可能是物體的區域，這裡採用的就是前面提到的Selective Search方法，通過這個算法我們搜索出2000個候選框。 然後從上面的總流程圖中可以看到，搜出的候選框是矩形的，而且是大小各不相同。 然而CNN對輸入圖片的大小是有固定的，如果把搜索到的矩形選框不做處理，就扔進CNN中，肯定不行。 因此對於每個輸入的候選框都需要縮放到固定的大小。 下面我們講解要怎麼進行縮放處理，為了簡單起見我們假設下一階段CNN所需要的輸入圖片大小是個正方形圖片227*227。 因為我們經過selective search 得到的是矩形框，paper試驗了兩種不同的處理方法： (1)各向異性縮放 這種方法很簡單，就是不管圖片的長寬比例，管它是否扭曲，進行縮放就是了，全部縮放到CNN輸入的大小227*227，如下圖(D)所示； (2)各向同性縮放 因為圖片扭曲後，估計會對後續CNN的訓練精度有影響，於是作者也測試了“各向同性縮放”方案。 有兩種辦法 A、先擴充後裁剪： 直接在原始圖片中，把bounding box的邊界進行擴展延伸成正方形，然後再進行裁剪；如果已經延伸到了原始圖片的外邊界，那麼就用bounding box中的顏色均值填充；如上圖(B)所示; B、先裁剪後擴充：先把bounding box圖片裁剪出來，然後用固定的背景顏色填充成正方形圖片(背景顏色也是採用bounding box的像素顏色均值),如上圖(C)所示; 對於上面的異性、同性縮放，文獻還有個padding處理，上面的示意圖中第1、3行就是結合了padding=0,第2、4行結果圖採用padding=16的結果。 經過最後的試驗，作者發現採用各向異性縮放、padding=16的精度最高。 （備註：候選框的搜索策略作者也考慮過使用一個滑動窗口的方法，然而由於更深的網絡，更大的輸入圖片和滑動步長，使得使用滑動窗口來定位的方法充滿了挑戰。） CNN特徵提取階段： 1、算法實現 a、網絡結構設計階段 網絡架構兩個可選方案：第一選擇經典的Alexnet；第二選擇VGG16。 經過測試Alexnet精度為58.5%，VGG16精度為66%。 VGG這個模型的特點是選擇比較小的捲積核、選擇較小的跨步，這個網絡的精度高，不過計算量是Alexnet的7倍。 後面為了簡單起見，我們就直接選用Alexnet，並進行講解；Alexnet特徵提取部分包含了5個卷積層、2個全連接層，在Alexnet中p5層神經元個數為9216、 f6、f7的神經元個數都是4096，通過這個網絡訓練完畢後，最後提取特徵每個輸入候選框圖片都能得到一個4096維的特徵向量。 b、網絡有監督預訓練階段（圖片數據庫：ImageNet ILSVC ） 參數初始化部分：物體檢測的一個難點在於，物體標籤訓練數據少，如果要直接採用隨機初始化CNN參數的方法，那麼目前的訓練數據量是遠遠不夠的。 這種情況下，最好的是採用某些方法，把參數初始化了，然後在進行有監督的參數微調，這里文獻採用的是有監督的預訓練。 所以paper在設計網絡結構的時候，是直接用Alexnet的網絡，然後連參數也是直接採用它的參數，作為初始的參數值，然後再fine-tuning訓練。 網絡優化求解時採用隨機梯度下降法，學習率大小為0.001； c、fine-tuning階段（圖片數據庫： PASCAL VOC） 我們接著採用selective search 搜索出來的候選框（PASCAL VOC 數據庫中的圖片） 繼續對上面預訓練的CNN模型進行fine-tuning訓練。 假設要檢測的物體類別有N類，那麼我們就需要把上面預訓練階段的CNN模型的最後一層給替換掉，替換成N+1個輸出的神經元(加1，表示還有一個背景) (20 + 1bg = 21)，然後這一層直接採用參數隨機初始化的方法，其它網絡層的參數不變；接著就可以開始繼續SGD訓練了。 開始的時候，SGD學習率選擇0.001，在每次訓練的時候，我們batch size大小選擇128，其中32個事正樣本、96個事負樣本。 關於正負樣本問題： 一張照片我們得到了2000個候選框。 然而人工標註的數據一張圖片中就只標註了正確的bounding box，我們搜索出來的2000個矩形框也不可能會出現一個與人工標註完全匹配的候選框。 因此在CNN階段我們需要用IOU為2000個bounding box打標籤。 如果用selective search挑選出來的候選框與物體的人工標註矩形框（PASCAL VOC的圖片都有人工標註）的重疊區域IoU大於0.5，那麼我們就把這個候選框標註成物體類別（正樣本），否則我們就把它當做背景類別（負樣本）。 （備註： 如果不針對特定任務進行fine-tuning，而是把CNN當做特徵提取器，卷積層所學到的特徵其實就是基礎的共享特徵提取層，就類似於SIFT算法一樣，可以用於提取各種圖片的特徵，而f6、f7所學習到的特徵是用於針對特定任務的特徵。打個比方：對於人臉性別識別來說，一個CNN模型前面的捲積層所學習到的特徵就類似於學習人臉共性特徵，然後全連接層所學習的特徵就是針對性別分類的特徵了） 2.疑惑點 ： CNN訓練的時候，本來就是對bounding box的物體進行識別分類訓練，在訓練的時候最後一層softmax就是分類層。 那麼為什麼作者閒著沒事幹要先用CNN做特徵提取（提取fc7層數據），然後再把提取的特徵用於訓練svm分類器？ 這個是因為svm訓練和cnn訓練過程的正負樣本定義方式各有不同，導致最後採用CNN softmax輸出比採用svm精度還低。 事情是這樣的，cnn在訓練的時候，對訓練數據做了比較寬鬆的標註，比如一個bounding box可能只包含物體的一部分，那麼我也把它標註為正樣本，用於訓練cnn；採用這個方法的主要原因在於因為CNN容易過擬合，所以需要大量的訓練數據，所以在CNN訓練階段我們是對Bounding box的位置限制條件限制的比較鬆(IOU只要大於0.5都被標註為正樣本了)；然而svm訓練的時候，因為svm適用於少樣本訓練，所以對於訓練樣本數據的IOU要求比較嚴格，我們只有當bounding box把整個物體都包含進去了，我們才把它標註為物體類別，然後訓練svm ，具體請看下文。 SVM訓練、測試階段 訓練階段 ： 這是一個二分類問題，我麼假設我們要檢測車輛。 我們知道只有當bounding box把整量車都包含在內，那才叫正樣本；如果bounding box 沒有包含到車輛，那麼我們就可以把它當做負樣本。 但問題是當我們的檢測窗口只有部分包含物體，那該怎麼定義正負樣本呢？ 作者測試了IOU閾值各種方案數值0,0.1,0.2,0.3,0.4,0.5。 最後通過訓練發現，如果選擇IOU閾值為0.3 效果最好 （選擇為0精度下降了4個百分點，選擇0.5精度下降了5個百分點）,即當重疊度小於0.3的時候，我們就把它標註為負樣本。 一旦CNN f7層特徵被提取出來，那麼我們將為每個物體類訓練一個svm分類器。 當我們用CNN提取2000個候選框，可以得到2000x4096這樣的特徵向量矩陣，然後我們只需要把這樣的一個矩陣與svm權值矩陣4096xN點乘(N為分類類別數目，因為我們訓練的N個svm，每個svm包含了4096個權值w)，就可以得到結果了。 得到的特徵輸入到SVM進行分類看看這個feature vector所對應的region proposal是需要的物體還是無關的實物(background) 。 排序，canny邊界檢測之後就得到了我們需要的bounding-box。 再回顧總結一下：整個系統分為三個部分：1.產生不依賴與特定類別的region proposals，這些region proposals定義了一個整個檢測器可以獲得的候選目標2.一個大的捲積神經網絡，對每個region產生一個固定長度的特徵向量3.一系列特定類別的線性SVM分類器。 位置精修： 目標檢測問題的衡量標準是重疊面積：許多看似準確的檢測結果，往往因為候選框不夠準確，重疊面積很小。 故需要一個位置精修步驟。 回歸器：對每一類目標，使用一個線性脊回歸器進行精修。 正則項λ=10000。 輸入為深度網絡pool5層的4096維特徵，輸出為xy方向的縮放和平移。 訓練樣本：判定為本類的候選框中和真值重疊面積大於0.6的候選框。 測試階段 ： 使用selective search的方法在測試圖片上提取2000個region propasals ，將每個region proposals歸一化到227x227，然後再CNN中正向傳播，將最後一層得到的特徵提取出來。 然後對於每一個類別，使用為這一類訓練的SVM分類器對提取的特徵向量進行打分，得到測試圖片中對於所有region proposals的對於這一類的分數，再使用貪心的非極大值抑制（ NMS）去除相交的多餘的框。 再對這些框進行canny邊緣檢測，就可以得到bounding-box(then B-BoxRegression)。 （非極大值抑制（NMS）先計算出每一個bounding box的面積，然後根據score進行排序，把score最大的bounding box作為選定的框，計算其餘bounding box與當前最大score與box的IoU，去除IoU大於設定的閾值的bounding box。然後重複上面的過程，直至候選bounding box為空，然後再將score小於一定閾值的選定框刪除得到這一類的結果（然後繼續進行下一個分類） 。作者提到花費在region propasals和提取特徵的時間是13s/張-GPU和53s/張-CPU，可以看出時間還是很長的，不能夠達到及時性。 完。 本文主要整理自以下文章： RCNN學習筆記(0):rcnn簡介RCNN學習筆記(1):Rich feature hierarchies for accurate object detection and semantic segmentationRCNN學習筆記(2):Rich feature hierarchies for accurate object detection and semantic segmentation《Rich feature hierarchies for Accurate Object Detection and Segmentation》《Spatial 《Pyramid Pooling in Deep Convolutional Networks for Visual Recognition》","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://world4jason.github.io/tags/Deep-Learning/"},{"name":"R-CNN","slug":"R-CNN","permalink":"https://world4jason.github.io/tags/R-CNN/"},{"name":"CNN","slug":"CNN","permalink":"https://world4jason.github.io/tags/CNN/"},{"name":"Object Detection","slug":"Object-Detection","permalink":"https://world4jason.github.io/tags/Object-Detection/"}]},{"title":"Object Detection with Convolution Neural Network Series","date":"2017-11-05T18:51:33.000Z","path":"2017/11/06/Object-Detection-with-Convolution-Neural-Network/","text":"Object Detection with Convolution Neural Network SeriesOutline 檢測任務? R-CNN Fast R-CNN Faster R-CNN SSD YOLO 檢測任務 R-CNN 步驟 候選區域生成： 一張圖像生成1K~2K個候選區域（採用Selective Search 方法） 特徵提取： 對每個候選區域，使用深度卷積網絡提取特徵（CNN） 類別判斷： 特徵送入每一類的SVM 分類器，判別是否屬於該類 位置精修： 使用回歸器精細修正候選框位置 用Non-Maximum Selection 合併後選框 Fast R-CNN 步驟1. 候選區域生成： 一張圖像生成1K~2K個候選區域（採用Selective Search 方法）2. 特徵提取： 該張圖片，使用深度卷積網絡提取特徵（CNN）3. ROI Pooling：4. 類別判斷： 特徵送入每一類的SVM 分類器，判別是否屬於該類5. 位置精修： 使用回歸器精細修正候選框位置6. 用Non-Maximum Selection 合併後選框## Faster R-CNN SSD YOLO Reference: http://zh.gluon.ai/","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://world4jason.github.io/tags/Deep-Learning/"},{"name":"R-CNN","slug":"R-CNN","permalink":"https://world4jason.github.io/tags/R-CNN/"},{"name":"CNN","slug":"CNN","permalink":"https://world4jason.github.io/tags/CNN/"},{"name":"Object Detection","slug":"Object-Detection","permalink":"https://world4jason.github.io/tags/Object-Detection/"}]},{"title":"CPP 筆記","date":"2017-10-19T04:07:48.000Z","path":"2017/10/19/CPP-note/","text":"##C++中cin、cin.get()、cin.getline()、getline()、gets()等函数的用法学C++的时候，这几个输入函数弄的有点迷糊；这里做个小结，为了自己复习，也希望对后来者能有所帮助，如果有差错的地方还请各位多多指教 1、cin2、cin.get()3、cin.getline()4、getline()5、gets()6、getchar() 附:cin.ignore();cin.get()//跳过一个字符,例如不想要的回车,空格等字符 1、cin&gt;&gt; 用法1：最基本，也是最常用的用法，输入一个数字： 12345678#include &lt;iostream&gt; using namespace std; main () &#123; int a,b; cin&gt;&gt;a&gt;&gt;b; cout&lt;&lt;a+b&lt;&lt;endl; &#125; 输入：2[回车]3[回车]输出：5 注意:&gt;&gt; 是会过滤掉不可见字符（如 空格 回车，TAB 等）cin&gt;&gt;noskipws&gt;&gt;input[j];//不想略过空白字符，那就使用 noskipws 流控制 用法2：接受一个字符串，遇“空格”、“TAB”、“回车”都结束 12345678#include &lt;iostream&gt; using namespace std; main () &#123; char a[20]; cin&gt;&gt;a; cout&lt;&lt;a&lt;&lt;endl; &#125; 输入：jkljkljkl输出：jkljkljkl 输入：jkljkl jkljkl //遇空格结束输出：jkljkl 2、cin.get() 用法1： cin.get(字符变量名)可以用来接收字符 12345678#include &lt;iostream&gt; using namespace std; main () &#123; char ch; ch=cin.get(); //或者cin.get(ch); cout&lt;&lt;ch&lt;&lt;endl; &#125; 输入：jljkljkl输出：j 用法2：cin.get(字符数组名,接收字符数目)用来接收一行字符串,可以接收空格 12345678#include &lt;iostream&gt; using namespace std; main () &#123; char a[20]; cin.get(a,20); cout&lt;&lt;a&lt;&lt;endl; &#125; 输入：jkl jkl jkl输出：jkl jkl jkl 输入：abcdeabcdeabcdeabcdeabcde （输入25个字符）输出：abcdeabcdeabcdeabcd （接收19个字符+1个’\\0’） 用法3：cin.get(无参数)没有参数主要是用于舍弃输入流中的不需要的字符,或者舍弃回车,弥补cin.get(字符数组名,接收字符数目)的不足. 这个我还不知道怎么用，知道的前辈请赐教； 3、cin.getline() // 接受一个字符串，可以接收空格并输出 12345678#include &lt;iostream&gt; using namespace std; main () &#123; char m[20]; cin.getline(m,5); cout&lt;&lt;m&lt;&lt;endl; &#125; 输入：jkljkljkl输出：jklj 接受5个字符到m中，其中最后一个为’\\0’，所以只看到4个字符输出； 如果把5改成20：输入：jkljkljkl输出：jkljkljkl 输入：jklf fjlsjf fjsdklf输出：jklf fjlsjf fjsdklf //延伸：//cin.getline()实际上有三个参数，cin.getline(接受字符串的看哦那间m,接受个数5,结束字符)//当第三个参数省略时，系统默认为’\\0’//如果将例子中cin.getline()改为cin.getline(m,5,’a’);当输入jlkjkljkl时输出jklj，输入jkaljkljkl时，输出jk 当用在多维数组中的时候，也可以用cin.getline(m[i],20)之类的用法： 123456789101112131415161718#include&lt;iostream&gt; #include&lt;string&gt; using namespace std;main () &#123; char m[3][20]; for(int i=0;i&lt;3;i++) &#123; cout&lt;&lt;\"\\n请输入第\"&lt;&lt;i+1&lt;&lt;\"个字符串：\"&lt;&lt;endl; cin.getline(m[i],20); &#125;cout&lt;&lt;endl; for(int j=0;j&lt;3;j++) cout&lt;&lt;\"输出m[\"&lt;&lt;j&lt;&lt;\"]的值:\"&lt;&lt;m[j]&lt;&lt;endl;&#125; 请输入第1个字符串：kskr1 请输入第2个字符串：kskr2 请输入第3个字符串：kskr3 输出m[0]的值:kskr1输出m[1]的值:kskr2输出m[2]的值:kskr3 4、getline() // 接受一个字符串，可以接收空格并输出，需包含“#include” 123456789#include&lt;iostream&gt; #include&lt;string&gt; using namespace std; main () &#123; string str; getline(cin,str); cout&lt;&lt;str&lt;&lt;endl; &#125; 输入：jkljkljkl输出：jkljkljkl 输入：jkl jfksldfj jklsjfl输出：jkl jfksldfj jklsjfl 和cin.getline()类似，但是cin.getline()属于istream流，而getline()属于string流，是不一样的两个函数 在寫程式的時候遇到這個問題因為atoi函式只吃char*上網GOOGLE一下找到了解決之法 123string x;int temp=atoi(x.c_str());c_str()可以轉換string成為char* 12345678910111213141516171819#include &lt;iostream&gt;using namespace std ; int main()&#123; string s ; int a; while (getline(cin,s)) &#123; int n=s.length(); for(int i=0;i&lt;n;i++) &#123; a=s.at(i); cout &lt;&lt; a &lt;&lt; \" \"; &#125; cout &lt;&lt; endl; &#125; return 0 ;&#125;","tags":[{"name":"code","slug":"code","permalink":"https://world4jason.github.io/tags/code/"},{"name":"CPP","slug":"CPP","permalink":"https://world4jason.github.io/tags/CPP/"},{"name":"C++","slug":"C","permalink":"https://world4jason.github.io/tags/C/"}]},{"title":"Hello World","date":"2017-03-28T06:57:15.000Z","path":"2017/03/28/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]}]