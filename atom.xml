<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>world4jason</title>
  
  <subtitle>菜鳥搬磚日常</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://world4jason.github.io/"/>
  <updated>2017-12-13T07:49:23.000Z</updated>
  <id>https://world4jason.github.io/</id>
  
  <author>
    <name>Jason Yeh</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Python 筆記</title>
    <link href="https://world4jason.github.io/2017/12/13/python-note/"/>
    <id>https://world4jason.github.io/2017/12/13/python-note/</id>
    <published>2017-12-13T07:35:34.000Z</published>
    <updated>2017-12-13T07:49:23.000Z</updated>
    
    <content type="html"><![CDATA[<p>Different in Py2 and Py3</p><p>Pickle module</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">if</span> sys.version_info[<span class="number">0</span>]&lt;<span class="number">3</span>:</div><div class="line">    <span class="keyword">import</span> cPickle <span class="keyword">as</span> pickle</div><div class="line"><span class="keyword">else</span></div><div class="line">    <span class="keyword">import</span> _pickle <span class="keyword">as</span> pickle</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Different in Py2 and Py3&lt;/p&gt;
&lt;p&gt;Pickle module&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>zi2zi: Master Chinese Calligraphy with Conditional Adversarial Networks</title>
    <link href="https://world4jason.github.io/2017/12/01/zi2zi/"/>
    <id>https://world4jason.github.io/2017/12/01/zi2zi/</id>
    <published>2017-11-30T19:36:19.000Z</published>
    <updated>2017-12-09T19:46:26.000Z</updated>
    
    <content type="html"><![CDATA[<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15128482902404.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>Generated samples. Related code can be found <a href="https://github.com/kaonashi-tyc/zi2zi" target="_blank" rel="external">here</a></p><h2 id="目標"><a href="#目標" class="headerlink" title="目標"></a>目標</h2><p>字體風格轉換<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15128485151045.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h2 id="動機"><a href="#動機" class="headerlink" title="動機"></a>動機</h2><p>直接用CNN進行風格轉換會有下列問題</p><ol><li>生成常常是模糊的</li><li>多數生成結果是失敗的</li><li>只能做一對一生成</li></ol><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15128485338820.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>結論：用GAN試試看</p><h2 id="用GAN秒殺一切！"><a href="#用GAN秒殺一切！" class="headerlink" title="用GAN秒殺一切！"></a>用GAN秒殺一切！</h2><p>這篇借鑒了三篇paper內容</p><p><a href="https://arxiv.org/abs/1611.07004" target="_blank" rel="external">Image-to-Image Translation with Conditional Adversarial Networks</a><br><a href="https://arxiv.org/abs/1610.09585" target="_blank" rel="external">Conditional Image Synthesis With Auxiliary Classifier GANs</a><br><a href="https://arxiv.org/abs/1611.02200" target="_blank" rel="external">Unsupervised Cross-Domain Image Generation</a></p><p>主要是由pix2pix這篇修改而來的<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15128487302209.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>其中Encoder跟Decoder還有Discriminator是直接用pix2pix的, 尤其是裡面的Unet模型</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
                    &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt;
                   
      
    
    </summary>
    
      <category term="GAN" scheme="https://world4jason.github.io/categories/GAN/"/>
    
    
      <category term="Generation" scheme="https://world4jason.github.io/tags/Generation/"/>
    
      <category term="Generative Model" scheme="https://world4jason.github.io/tags/Generative-Model/"/>
    
      <category term="GAN" scheme="https://world4jason.github.io/tags/GAN/"/>
    
      <category term="Style Transfer" scheme="https://world4jason.github.io/tags/Style-Transfer/"/>
    
  </entry>
  
  <entry>
    <title>Mask R-CNN</title>
    <link href="https://world4jason.github.io/2017/11/10/untitled-1510251239231/"/>
    <id>https://world4jason.github.io/2017/11/10/untitled-1510251239231/</id>
    <published>2017-11-09T18:13:59.000Z</published>
    <updated>2017-11-13T15:41:16.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="Segmentation" scheme="https://world4jason.github.io/categories/Segmentation/"/>
    
    
      <category term="Deep Learning" scheme="https://world4jason.github.io/tags/Deep-Learning/"/>
    
      <category term="R-CNN" scheme="https://world4jason.github.io/tags/R-CNN/"/>
    
      <category term="Segmentation" scheme="https://world4jason.github.io/tags/Segmentation/"/>
    
      <category term="Pose Estimation" scheme="https://world4jason.github.io/tags/Pose-Estimation/"/>
    
  </entry>
  
  <entry>
    <title>Conditional GAN - 條件式生成對抗網路</title>
    <link href="https://world4jason.github.io/2017/11/08/Conditional-GAN/"/>
    <id>https://world4jason.github.io/2017/11/08/Conditional-GAN/</id>
    <published>2017-11-08T05:48:12.000Z</published>
    <updated>2017-11-08T12:37:52.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="GAN" scheme="https://world4jason.github.io/categories/GAN/"/>
    
    
      <category term="Deep Learning" scheme="https://world4jason.github.io/tags/Deep-Learning/"/>
    
      <category term="Generation" scheme="https://world4jason.github.io/tags/Generation/"/>
    
      <category term="Generative Model" scheme="https://world4jason.github.io/tags/Generative-Model/"/>
    
      <category term="GAN" scheme="https://world4jason.github.io/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>GAN - Generative Adversarial Network</title>
    <link href="https://world4jason.github.io/2017/11/07/GAN-Generative-Adversarial-Network/"/>
    <id>https://world4jason.github.io/2017/11/07/GAN-Generative-Adversarial-Network/</id>
    <published>2017-11-07T15:00:02.000Z</published>
    <updated>2017-11-08T12:37:57.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Generative-Adversarial-Networks-GANs"><a href="#Generative-Adversarial-Networks-GANs" class="headerlink" title="Generative Adversarial Networks (GANs)"></a>Generative Adversarial Networks (GANs)</h2><h3 id="Lists"><a href="#Lists" class="headerlink" title="Lists"></a>Lists</h3><table><thead><tr><th style="text-align:center"><em>Name</em></th><th style="text-align:center"><em>Paper Link</em></th><th style="text-align:left"><em>Value Function</em></th></tr></thead><tbody><tr><td style="text-align:center"><strong>GAN</strong></td><td style="text-align:center"><a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="external">Arxiv</a></td><td style="text-align:left"><img src="/media/equations/GAN.png"></td></tr><tr><td style="text-align:center"><strong>LSGAN</strong></td><td style="text-align:center"><a href="https://arxiv.org/abs/1611.04076" target="_blank" rel="external">Arxiv</a></td><td style="text-align:left"><img src="/media/equations/LSGAN.png"></td></tr><tr><td style="text-align:center"><strong>WGAN</strong></td><td style="text-align:center"><a href="https://arxiv.org/abs/1701.07875" target="_blank" rel="external">Arxiv</a></td><td style="text-align:left"><img src="/media/equations/WGAN.png"></td></tr><tr><td style="text-align:center"><strong>WGAN-GP</strong></td><td style="text-align:center"><a href="https://arxiv.org/abs/1704.00028" target="_blank" rel="external">Arxiv</a></td><td style="text-align:left"><img src="/media/equations/WGAN_GP.png"></td></tr><tr><td style="text-align:center"><strong>DRAGAN</strong></td><td style="text-align:center"><a href="https://arxiv.org/abs/1705.07215" target="_blank" rel="external">Arxiv</a></td><td style="text-align:left"><img src="/media/equations/DRAGAN.png"></td></tr><tr><td style="text-align:center"><strong>CGAN</strong></td><td style="text-align:center"><a href="https://arxiv.org/abs/1411.1784" target="_blank" rel="external">Arxiv</a></td><td style="text-align:left"><img src="/media/equations/CGAN.png"></td></tr><tr><td style="text-align:center"><strong>infoGAN</strong></td><td style="text-align:center"><a href="https://arxiv.org/abs/1606.03657" target="_blank" rel="external">Arxiv</a></td><td style="text-align:left"><img src="/media/equations/infoGAN.png"></td></tr><tr><td style="text-align:center"><strong>ACGAN</strong></td><td style="text-align:center"><a href="https://arxiv.org/abs/1610.09585" target="_blank" rel="external">Arxiv</a></td><td style="text-align:left"><img src="/media/equations/ACGAN.png"></td></tr><tr><td style="text-align:center"><strong>EBGAN</strong></td><td style="text-align:center"><a href="https://arxiv.org/abs/1609.03126" target="_blank" rel="external">Arxiv</a></td><td style="text-align:left"><img src="/media/equations/EBGAN.png"></td></tr><tr><td style="text-align:center"><strong>BEGAN</strong></td><td style="text-align:center"><a href="https://arxiv.org/abs/1702.08431" target="_blank" rel="external">Arxiv</a></td><td style="text-align:left"><img src="/media/equations/BEGAN.png">  </td></tr></tbody></table><h4 id="Variants-of-GAN-structure"><a href="#Variants-of-GAN-structure" class="headerlink" title="Variants of GAN structure"></a>Variants of GAN structure</h4><p><img src="/media/etc/GAN_structure.png"></p><h3 id="Results-for-mnist"><a href="#Results-for-mnist" class="headerlink" title="Results for mnist"></a>Results for mnist</h3><p>Network architecture of generator and discriminator is the exaclty sames as in <a href="https://arxiv.org/abs/1606.03657" target="_blank" rel="external">infoGAN paper</a>.<br>For fair comparison of core ideas in all gan variants, all implementations for network architecture are kept same except EBGAN and BEGAN. Small modification is made for EBGAN/BEGAN, since those adopt auto-encoder strucutre for discriminator. But I tried to keep the capacity of discirminator.</p><h4 id="Random-generation"><a href="#Random-generation" class="headerlink" title="Random generation"></a>Random generation</h4><p>All results are randomly sampled.</p><table><thead><tr><th style="text-align:center"><em>Name</em></th><th style="text-align:center"><em>Epoch 2</em></th><th style="text-align:center"><em>Epoch 10</em></th><th style="text-align:center"><em>Epoch 25</em></th></tr></thead><tbody><tr><td style="text-align:center">GAN</td><td style="text-align:center"><img src="/media/mnist_results/random_generation/GAN_epoch001_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/GAN_epoch009_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/GAN_epoch024_test_all_classes.png"></td></tr><tr><td style="text-align:center">LSGAN</td><td style="text-align:center"><img src="/media/mnist_results/random_generation/LSGAN_epoch001_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/LSGAN_epoch009_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/LSGAN_epoch024_test_all_classes.png"></td></tr><tr><td style="text-align:center">WGAN</td><td style="text-align:center"><img src="/media/mnist_results/random_generation/WGAN_epoch001_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/WGAN_epoch009_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/WGAN_epoch024_test_all_classes.png"></td></tr><tr><td style="text-align:center">WGAN-GP</td><td style="text-align:center"><img src="/media/mnist_results/random_generation/WGAN-GP_epoch001_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/WGAN-GP_epoch009_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/WGAN-GP_epoch024_test_all_classes.png"></td></tr><tr><td style="text-align:center">DRAGAN</td><td style="text-align:center"><img src="/media/mnist_results/random_generation/DRAGAN_epoch001_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/DRAGAN_epoch009_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/DRAGAN_epoch024_test_all_classes.png"></td></tr><tr><td style="text-align:center">EBGAN</td><td style="text-align:center"><img src="/media/mnist_results/random_generation/EBGAN_epoch001_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/EBGAN_epoch009_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/EBGAN_epoch024_test_all_classes.png"></td></tr><tr><td style="text-align:center">BEGAN</td><td style="text-align:center"><img src="/media/mnist_results/random_generation/BEGAN_epoch001_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/BEGAN_epoch009_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/BEGAN_epoch024_test_all_classes.png"></td></tr></tbody></table><h4 id="Conditional-generation"><a href="#Conditional-generation" class="headerlink" title="Conditional generation"></a>Conditional generation</h4><p>Each row has the same noise vector and each column has the same label condition.</p><table><thead><tr><th style="text-align:center"><em>Name</em></th><th style="text-align:center"><em>Epoch 1</em></th><th style="text-align:center"><em>Epoch 10</em></th><th style="text-align:center"><em>Epoch 25</em></th></tr></thead><tbody><tr><td style="text-align:center">CGAN</td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/CGAN_epoch000_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/CGAN_epoch009_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/CGAN_epoch024_test_all_classes_style_by_style.png"></td></tr><tr><td style="text-align:center">ACGAN</td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/ACGAN_epoch000_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/ACGAN_epoch009_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/ACGAN_epoch024_test_all_classes_style_by_style.png"></td></tr><tr><td style="text-align:center">infoGAN</td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/infoGAN_epoch000_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/infoGAN_epoch009_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/infoGAN_epoch024_test_all_classes_style_by_style.png"></td></tr></tbody></table><h4 id="InfoGAN-Manipulating-two-continous-codes"><a href="#InfoGAN-Manipulating-two-continous-codes" class="headerlink" title="InfoGAN : Manipulating two continous codes"></a>InfoGAN : Manipulating two continous codes</h4><table align="center"><br><td><img src="/media/mnist_results/infogan/infoGAN_epoch024_test_class_c1c2_2.png"></td><br><td><img src="/media/mnist_results/infogan/infoGAN_epoch024_test_class_c1c2_5.png"></td><br><td><img src="/media/mnist_results/infogan/infoGAN_epoch024_test_class_c1c2_7.png"></td><br><td><img src="/media/mnist_results/infogan/infoGAN_epoch024_test_class_c1c2_9.png"></td><br></table><h3 id="Results-for-fashion-mnist"><a href="#Results-for-fashion-mnist" class="headerlink" title="Results for fashion-mnist"></a>Results for fashion-mnist</h3><p>Comments on network architecture in mnist are also applied to here.<br><a href="https://github.com/zalandoresearch/fashion-mnist" target="_blank" rel="external">Fashion-mnist</a> is a recently proposed dataset consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. (T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot)</p><h4 id="Random-generation-1"><a href="#Random-generation-1" class="headerlink" title="Random generation"></a>Random generation</h4><p>All results are randomly sampled.</p><table><thead><tr><th style="text-align:center"><em>Name</em></th><th style="text-align:center"><em>Epoch 1</em></th><th style="text-align:center"><em>Epoch 20</em></th><th style="text-align:center"><em>Epoch 40</em></th></tr></thead><tbody><tr><td style="text-align:center">GAN</td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/GAN_epoch000_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/GAN_epoch019_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/GAN_epoch039_test_all_classes.png"></td></tr><tr><td style="text-align:center">LSGAN</td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/LSGAN_epoch000_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/LSGAN_epoch019_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/LSGAN_epoch039_test_all_classes.png"></td></tr><tr><td style="text-align:center">WGAN</td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/WGAN_epoch000_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/WGAN_epoch019_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/WGAN_epoch039_test_all_classes.png"></td></tr><tr><td style="text-align:center">WGAN-GP</td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/WGAN-GP_epoch000_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/WGAN-GP_epoch019_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/WGAN-GP_epoch039_test_all_classes.png"></td></tr><tr><td style="text-align:center">DRAGAN</td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/DRAGAN_epoch000_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/DRAGAN_epoch019_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/DRAGAN_epoch039_test_all_classes.png"></td></tr><tr><td style="text-align:center">EBGAN</td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/EBGAN_epoch000_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/EBGAN_epoch019_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/EBGAN_epoch039_test_all_classes.png"></td></tr><tr><td style="text-align:center">BEGAN</td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/BEGAN_epoch000_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/BEGAN_epoch019_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/BEGAN_epoch039_test_all_classes.png"></td></tr></tbody></table><h4 id="Conditional-generation-1"><a href="#Conditional-generation-1" class="headerlink" title="Conditional generation"></a>Conditional generation</h4><p>Each row has the same noise vector and each column has the same label condition.</p><table><thead><tr><th style="text-align:center"><em>Name</em></th><th style="text-align:center"><em>Epoch 1</em></th><th style="text-align:center"><em>Epoch 20</em></th><th style="text-align:center"><em>Epoch 40</em></th></tr></thead><tbody><tr><td style="text-align:center">CGAN</td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/CGAN_epoch000_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/CGAN_epoch019_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/CGAN_epoch039_test_all_classes_style_by_style.png"></td></tr><tr><td style="text-align:center">ACGAN</td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/ACGAN_epoch000_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/ACGAN_epoch019_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/ACGAN_epoch039_test_all_classes_style_by_style.png"></td></tr><tr><td style="text-align:center">infoGAN</td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/infoGAN_epoch000_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/infoGAN_epoch019_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/infoGAN_epoch039_test_all_classes_style_by_style.png"></td></tr></tbody></table><p>Without hyper-parameter tuning from mnist-version, ACGAN/infoGAN does not work well as compared with CGAN.<br>ACGAN tends to fall into mode-collapse.<br>infoGAN tends to ignore noise-vector. It results in that various style within the same class can not be represented.</p><h4 id="InfoGAN-Manipulating-two-continous-codes-1"><a href="#InfoGAN-Manipulating-two-continous-codes-1" class="headerlink" title="InfoGAN : Manipulating two continous codes"></a>InfoGAN : Manipulating two continous codes</h4><table align="center"><br><td><img src="/media/fashion_mnist_results/infogan/infoGAN_epoch039_test_class_c1c2_1.png"></td><br><td><img src="/media/fashion_mnist_results/infogan/infoGAN_epoch039_test_class_c1c2_4.png"></td><br><td><img src="/media/fashion_mnist_results/infogan/infoGAN_epoch039_test_class_c1c2_5.png"></td><br><td><img src="/media/fashion_mnist_results/infogan/infoGAN_epoch039_test_class_c1c2_8.png"></td><br></table><h3 id="Some-results-for-celebA"><a href="#Some-results-for-celebA" class="headerlink" title="Some results for celebA"></a>Some results for celebA</h3><p>(to be added)</p><h2 id="Variational-Auto-Encoders-VAEs"><a href="#Variational-Auto-Encoders-VAEs" class="headerlink" title="Variational Auto-Encoders (VAEs)"></a>Variational Auto-Encoders (VAEs)</h2><h3 id="Lists-1"><a href="#Lists-1" class="headerlink" title="Lists"></a>Lists</h3><table><thead><tr><th style="text-align:center"><em>Name</em></th><th style="text-align:center"><em>Paper Link</em></th><th style="text-align:left"><em>Loss Function</em></th></tr></thead><tbody><tr><td style="text-align:center"><strong>VAE</strong></td><td style="text-align:center"><a href="https://arxiv.org/abs/1312.6114" target="_blank" rel="external">Arxiv</a></td><td style="text-align:left"><img src="/media/equations/CVAE.png"> </td></tr><tr><td style="text-align:center"><strong>CVAE</strong></td><td style="text-align:center"><a href="https://arxiv.org/abs/1406.5298" target="_blank" rel="external">Arxiv</a></td><td style="text-align:left"><img src="/media/equations/CVAE.png"></td></tr><tr><td style="text-align:center"><strong>DVAE</strong></td><td style="text-align:center"><a href="https://arxiv.org/abs/1511.06406" target="_blank" rel="external">Arxiv</a></td><td style="text-align:left">(to be added)</td></tr><tr><td style="text-align:center"><strong>AAE</strong></td><td style="text-align:center"><a href="https://arxiv.org/abs/1511.05644" target="_blank" rel="external">Arxiv</a></td><td style="text-align:left">(to be added) </td></tr></tbody></table><h4 id="Variants-of-VAE-structure"><a href="#Variants-of-VAE-structure" class="headerlink" title="Variants of VAE structure"></a>Variants of VAE structure</h4><p><img src="/media/etc/VAE_structure.png"></p><h3 id="Results-for-mnist-1"><a href="#Results-for-mnist-1" class="headerlink" title="Results for mnist"></a>Results for mnist</h3><p>Network architecture of decoder(generator) and encoder(discriminator) is the exaclty sames as in <a href="https://arxiv.org/abs/1606.0365" target="_blank" rel="external">infoGAN paper</a>. The number of output nodes in encoder is different. (2x z_dim for VAE, 1 for GAN)</p><h4 id="Random-generation-2"><a href="#Random-generation-2" class="headerlink" title="Random generation"></a>Random generation</h4><p>All results are randomly sampled.</p><table><thead><tr><th style="text-align:center"><em>Name</em></th><th style="text-align:center"><em>Epoch 1</em></th><th style="text-align:center"><em>Epoch 10</em></th><th style="text-align:center"><em>Epoch 25</em></th></tr></thead><tbody><tr><td style="text-align:center">VAE</td><td style="text-align:center"><img src="/media/mnist_results/random_generation/VAE_epoch000_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/VAE_epoch009_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/VAE_epoch024_test_all_classes.png"></td></tr><tr><td style="text-align:center">GAN</td><td style="text-align:center"><img src="/media/mnist_results/random_generation/GAN_epoch000_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/GAN_epoch009_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/GAN_epoch024_test_all_classes.png"></td></tr></tbody></table><p>Results of GAN is also given to compare images generated from VAE and GAN.<br>The main difference (VAE generates smooth and blurry images, otherwise GAN generates sharp and artifact images) is cleary observed from the results.</p><h4 id="Conditional-generation-2"><a href="#Conditional-generation-2" class="headerlink" title="Conditional generation"></a>Conditional generation</h4><p>Each row has the same noise vector and each column has the same label condition.</p><table><thead><tr><th style="text-align:center"><em>Name</em></th><th style="text-align:center"><em>Epoch 1</em></th><th style="text-align:center"><em>Epoch 10</em></th><th style="text-align:center"><em>Epoch 25</em></th></tr></thead><tbody><tr><td style="text-align:center">CVAE</td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/CVAE_epoch000_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/CVAE_epoch009_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/CVAE_epoch024_test_all_classes_style_by_style.png"></td></tr><tr><td style="text-align:center">CGAN</td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/CGAN_epoch000_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/CGAN_epoch009_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/CGAN_epoch024_test_all_classes_style_by_style.png"></td></tr></tbody></table><p>Results of CGAN is also given to compare images generated from CVAE and CGAN.</p><h4 id="Learned-manifold"><a href="#Learned-manifold" class="headerlink" title="Learned manifold"></a>Learned manifold</h4><p>The following results can be reproduced with command:<br><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="comment">python</span> <span class="comment">main</span><span class="string">.</span><span class="comment">py</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">dataset</span> <span class="comment">mnist</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">gan_type</span> <span class="comment">VAE</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">epoch</span> <span class="comment">25</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">batch_size</span> <span class="comment">64</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">dim_z</span> <span class="comment">2</span></div></pre></td></tr></table></figure></p><p>Please notice that dimension of noise-vector z is 2.</p><table><thead><tr><th style="text-align:center"><em>Name</em></th><th style="text-align:center"><em>Epoch 1</em></th><th style="text-align:center"><em>Epoch 10</em></th><th style="text-align:center"><em>Epoch 25</em></th></tr></thead><tbody><tr><td style="text-align:center">VAE</td><td style="text-align:center"><img src="/media/mnist_results/learned_manifold/VAE_epoch000_learned_manifold.png"></td><td style="text-align:center"><img src="/media/mnist_results/learned_manifold/VAE_epoch009_learned_manifold.png"></td><td style="text-align:center"><img src="/media/mnist_results/learned_manifold/VAE_epoch024_learned_manifold.png"></td></tr></tbody></table><h3 id="Results-for-fashion-mnist-1"><a href="#Results-for-fashion-mnist-1" class="headerlink" title="Results for fashion-mnist"></a>Results for fashion-mnist</h3><p>Comments on network architecture in mnist are also applied to here. </p><p>The following results can be reproduced with command:<br><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="comment">python</span> <span class="comment">main</span><span class="string">.</span><span class="comment">py</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">dataset</span> <span class="comment">fashion</span><span class="literal">-</span><span class="comment">mnist</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">gan_type</span> &lt;<span class="comment">TYPE</span>&gt; <span class="literal">-</span><span class="literal">-</span><span class="comment">epoch</span> <span class="comment">40</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">batch_size</span> <span class="comment">64</span></div></pre></td></tr></table></figure></p><h4 id="Random-generation-3"><a href="#Random-generation-3" class="headerlink" title="Random generation"></a>Random generation</h4><p>All results are randomly sampled.</p><table><thead><tr><th style="text-align:center"><em>Name</em></th><th style="text-align:center"><em>Epoch 1</em></th><th style="text-align:center"><em>Epoch 20</em></th><th style="text-align:center"><em>Epoch 40</em></th></tr></thead><tbody><tr><td style="text-align:center">VAE</td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/VAE_epoch000_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/VAE_epoch019_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/VAE_epoch039_test_all_classes.png"></td></tr><tr><td style="text-align:center">GAN</td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/GAN_epoch000_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/GAN_epoch019_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/GAN_epoch039_test_all_classes.png"></td></tr></tbody></table><p>Results of GAN is also given to compare images generated from VAE and GAN.</p><h4 id="Conditional-generation-3"><a href="#Conditional-generation-3" class="headerlink" title="Conditional generation"></a>Conditional generation</h4><p>Each row has the same noise vector and each column has the same label condition.</p><table><thead><tr><th style="text-align:center"><em>Name</em></th><th style="text-align:center"><em>Epoch 1</em></th><th style="text-align:center"><em>Epoch 20</em></th><th style="text-align:center"><em>Epoch 40</em></th></tr></thead><tbody><tr><td style="text-align:center">CVAE</td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/CVAE_epoch000_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/CVAE_epoch019_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/CVAE_epoch039_test_all_classes_style_by_style.png"></td></tr><tr><td style="text-align:center">CGAN</td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/CGAN_epoch000_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/CGAN_epoch019_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/CGAN_epoch039_test_all_classes_style_by_style.png"></td></tr></tbody></table><p>Results of CGAN is also given to compare images generated from CVAE and CGAN.</p><h4 id="Learned-manifold-1"><a href="#Learned-manifold-1" class="headerlink" title="Learned manifold"></a>Learned manifold</h4><p>The following results can be reproduced with command:<br><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="comment">python</span> <span class="comment">main</span><span class="string">.</span><span class="comment">py</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">dataset</span> <span class="comment">fashion</span><span class="literal">-</span><span class="comment">mnist</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">gan_type</span> <span class="comment">VAE</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">epoch</span> <span class="comment">25</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">batch_size</span> <span class="comment">64</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">dim_z</span> <span class="comment">2</span></div></pre></td></tr></table></figure></p><p>Please notice that dimension of noise-vector z is 2.</p><table><thead><tr><th style="text-align:center"><em>Name</em></th><th style="text-align:center"><em>Epoch 1</em></th><th style="text-align:center"><em>Epoch 10</em></th><th style="text-align:center"><em>Epoch 25</em></th></tr></thead><tbody><tr><td style="text-align:center">VAE</td><td style="text-align:center"><img src="/media/fashion_mnist_results/learned_manifold/VAE_epoch000_learned_manifold.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/learned_manifold/VAE_epoch009_learned_manifold.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/learned_manifold/VAE_epoch024_learned_manifold.png"></td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Generative-Adversarial-Networks-GANs&quot;&gt;&lt;a href=&quot;#Generative-Adversarial-Networks-GANs&quot; class=&quot;headerlink&quot; title=&quot;Generative Adversari
      
    
    </summary>
    
      <category term="GAN" scheme="https://world4jason.github.io/categories/GAN/"/>
    
    
      <category term="Deep Learning" scheme="https://world4jason.github.io/tags/Deep-Learning/"/>
    
      <category term="Generation" scheme="https://world4jason.github.io/tags/Generation/"/>
    
      <category term="Generative Model" scheme="https://world4jason.github.io/tags/Generative-Model/"/>
    
      <category term="GAN" scheme="https://world4jason.github.io/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>Self-Study Courses List</title>
    <link href="https://world4jason.github.io/2017/11/07/Self-Study-Courses-List/"/>
    <id>https://world4jason.github.io/2017/11/07/Self-Study-Courses-List/</id>
    <published>2017-11-07T03:26:18.000Z</published>
    <updated>2017-12-14T15:56:34.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="學習路程"><a href="#學習路程" class="headerlink" title="學習路程"></a>學習路程</h2><ul><li>Python<br>  政大 or MIT python都好 後者比較舊但作業比較多</li><li>ML<br>  先看莫凡 看完再看NTU基礎的 剩下就看要看進階的還是standford的</li></ul><h2 id="Basic-Concept-of-Machine-Learning"><a href="#Basic-Concept-of-Machine-Learning" class="headerlink" title="Basic Concept of Machine Learning"></a>Basic Concept of Machine Learning</h2><ul><li>Morvan莫凡<ul><li><a href="https://www.youtube.com/playlist?list=PLXO45tsB95cIFm8Y8vMkNNPPXAtYXwKin" target="_blank" rel="external">有趣的機器學習</a></li></ul></li></ul><h2 id="Machine-Learning"><a href="#Machine-Learning" class="headerlink" title="Machine Learning"></a>Machine Learning</h2><p>-<a href="https://cn.udacity.com/course/intro-to-machine-learning--ud120" target="_blank" rel="external">Udacity ud120</a></p><h2 id="Machine-Learning-Deep-Learning-Series"><a href="#Machine-Learning-Deep-Learning-Series" class="headerlink" title="Machine Learning + Deep Learning Series"></a>Machine Learning + Deep Learning Series</h2><ul><li><h3 id="National-Taiwan-University-李宏毅"><a href="#National-Taiwan-University-李宏毅" class="headerlink" title="National Taiwan University - 李宏毅"></a>National Taiwan University - 李宏毅</h3><ul><li><p>BASIC</p><ul><li><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html" target="_blank" rel="external">Machine Learning (2016,Fall)</a></li><li><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML17.html" target="_blank" rel="external">Machine Learning (2017,Spring)</a></li><li><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML17_2.html" target="_blank" rel="external">Machine Learning (2017,Spring)</a></li></ul></li><li><p>ADV</p><ul><li><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS17.html" target="_blank" rel="external">Machine Learning and having it deep and structured (2017,Spring</a></li><li><a href="speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS17.html">Machine Learning and having it deep and structured (2017,Fall)</a><br>-</li><li><h3 id="National-Taiwan-University-林軒田"><a href="#National-Taiwan-University-林軒田" class="headerlink" title="National Taiwan University - 林軒田"></a>National Taiwan University - 林軒田</h3></li></ul></li><li><a href="https://www.youtube.com/watch?v=nQvpFSMPhr0&amp;list=PLXVfgk9fNX2I7tB6oIINGBmW50rrmFTqf" target="_blank" rel="external">機器學習基石</a></li><li><a href="https://www.youtube.com/watch?v=A-GxGCCAIrg&amp;list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2" target="_blank" rel="external">機器學習技法</a></li></ul><ul><li><h3 id="Coursera"><a href="#Coursera" class="headerlink" title="Coursera"></a>Coursera</h3><ul><li><a href="https://www.coursera.org/specializations/deep-learning" target="_blank" rel="external">Deep Learning Specialization</a></li></ul></li></ul></li><li><p>blog</p><ul><li><a href="http://www.cosmosshadow.com/ml" target="_blank" rel="external">Blog</a></li></ul></li><li><p>Deep Learning Specialize</p><ul><li><a href="https://mooc.study.163.com/smartSpec/detail/1001319001.htm" target="_blank" rel="external">網易雲中文版</a>  <h2 id="Visual"><a href="#Visual" class="headerlink" title="Visual"></a>Visual</h2></li></ul></li><li><h3 id="Stanford-CS213"><a href="#Stanford-CS213" class="headerlink" title="Stanford CS213"></a>Stanford CS213</h3><ul><li><a href="http://cs231n.stanford.edu/2016/syllabus" target="_blank" rel="external">2016 Syllabus</a> </li><li><a href="https://www.youtube.com/playlist?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC" target="_blank" rel="external">2016 Winter</a></li><li><p><a href="http://study.163.com/course/introduction/1003223001.htm" target="_blank" rel="external">2016 winter 中文版</a></p></li><li><p><a href="http://cs231n.stanford.edu/2017/syllabus" target="_blank" rel="external">2017 Syllabus</a></p></li><li><a href="https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv" target="_blank" rel="external">2017 Spring</a></li><li><a href="http://www.mooc.ai/course/268" target="_blank" rel="external">2017 winter 中文版</a></li></ul></li></ul><h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><ul><li><h3 id="NCCU"><a href="#NCCU" class="headerlink" title="NCCU"></a>NCCU</h3><ul><li><a href="http://moocs.nccu.edu.tw/course/121/intro" target="_blank" rel="external">Data science with Python</a></li></ul></li><li><h3 id="MIT"><a href="#MIT" class="headerlink" title="MIT"></a>MIT</h3><ul><li><a href="http://learn.edx.org/mit-python/" target="_blank" rel="external">MIT Python</a> </li></ul></li></ul><h2 id="Deep-Learning-Framework-Tutorial"><a href="#Deep-Learning-Framework-Tutorial" class="headerlink" title="Deep Learning Framework Tutorial"></a>Deep Learning Framework Tutorial</h2><ul><li><h3 id="Tensorflow"><a href="#Tensorflow" class="headerlink" title="Tensorflow"></a>Tensorflow</h3><ul><li><a href="">Tensorflow 官方</a></li><li><a href="http://usyiyi.cn/documents/tensorflow_13/tutorials/index.html" target="_blank" rel="external">Tensorflow 中文</a> </li><li><a href="https://www.youtube.com/playlist?list=PLQ0sVbIj3URf94DQtGPJV629ctn2c1zN-" target="_blank" rel="external">Stanford CS20SI - TensorFlow</a></li><li><a href="https://www.udacity.com/course/deep-learning--ud730" target="_blank" rel="external">Udacity ud730</a></li><li><a href="https://www.youtube.com/playlist?list=PL-XeOa5hMEYxNzHM7YLRjIwE1k3VQpqEh" target="_blank" rel="external">cognitiveclass.ai</a></li><li><a href="http://learningtensorflow.com/" target="_blank" rel="external">learning Tensorflow Web</a></li></ul></li><li><h3 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h3><ul><li><a href=""></a></li></ul></li></ul><h2 id="Deep-Learning-in-practice"><a href="#Deep-Learning-in-practice" class="headerlink" title="Deep Learning in practice"></a>Deep Learning in practice</h2><ul><li><a href="http://www.fast.ai/" target="_blank" rel="external">Fast.ai</a> </li><li><h3 id="Keras"><a href="#Keras" class="headerlink" title="Keras"></a>Keras</h3><ul><li><a href="https://github.com/erhwenkuo/deep-learning-with-keras-notebooks" target="_blank" rel="external">緯創</a></li></ul></li><li><h3 id="MXNET"><a href="#MXNET" class="headerlink" title="MXNET"></a>MXNET</h3>  -<a href="https://zh.gluon.ai/" target="_blank" rel="external">Gluon</a><br>  -<a href="https://github.com/mli/gluon-tutorials-zh/" target="_blank" rel="external">Gluon-Github</a></li></ul><h2 id="Concept"><a href="#Concept" class="headerlink" title="Concept"></a>Concept</h2><p>-CapsuleNet</p><ul><li><a href="https://www.youtube.com/watch?v=UhGWH3hb3Hk" target="_blank" rel="external">李宏毅</a></li></ul><h2 id="Online-IDE"><a href="#Online-IDE" class="headerlink" title="Online IDE"></a>Online IDE</h2><ul><li><a href="https://repl.it/" target="_blank" rel="external">repl.it</a></li></ul><h2 id="Online-practice"><a href="#Online-practice" class="headerlink" title="Online practice"></a>Online practice</h2><ul><li><a href="http://www.codewars.com/" target="_blank" rel="external">codewars</a></li></ul><p><a href="http://open.163.com/special/opencourse/daishu.html" target="_blank" rel="external">http://open.163.com/special/opencourse/daishu.html</a></p><p><a href="http://www.kaierlong.me/blog/post/kaierlong/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%80%E8%AF%BE%E6%B1%87%E6%80%BB" target="_blank" rel="external">http://www.kaierlong.me/blog/post/kaierlong/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%80%E8%AF%BE%E6%B1%87%E6%80%BB</a></p><p>Cognitive Class<br><a href="https://cognitiveclass.ai/" target="_blank" rel="external">https://cognitiveclass.ai/</a><br>Cognitive Class - IBM Big Data Learn<br><a href="https://cognitiveclass.ai/learn/big-data/" target="_blank" rel="external">https://cognitiveclass.ai/learn/big-data/</a><br>Cognitive Class - IBM Hadoop Foundations Learn<br><a href="https://cognitiveclass.ai/learn/hadoop/" target="_blank" rel="external">https://cognitiveclass.ai/learn/hadoop/</a><br>Cognitive Class - IBM Data Science Foundations Learn<br><a href="https://cognitiveclass.ai/learn/data-science/" target="_blank" rel="external">https://cognitiveclass.ai/learn/data-science/</a><br>Cognitive Class - IBM Data Science for Business Learn<br><a href="https://cognitiveclass.ai/learn/data-science-business/" target="_blank" rel="external">https://cognitiveclass.ai/learn/data-science-business/</a><br>Cognitive Class - IBM Deep Learning Learn<br><a href="https://cognitiveclass.ai/learn/deep-learning/" target="_blank" rel="external">https://cognitiveclass.ai/learn/deep-learning/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;學習路程&quot;&gt;&lt;a href=&quot;#學習路程&quot; class=&quot;headerlink&quot; title=&quot;學習路程&quot;&gt;&lt;/a&gt;學習路程&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Python&lt;br&gt;  政大 or MIT python都好 後者比較舊但作業比較多&lt;/li&gt;
&lt;li&gt;ML&lt;b
      
    
    </summary>
    
    
      <category term="Deep Learning" scheme="https://world4jason.github.io/tags/Deep-Learning/"/>
    
      <category term="Courses" scheme="https://world4jason.github.io/tags/Courses/"/>
    
      <category term="Self-Study" scheme="https://world4jason.github.io/tags/Self-Study/"/>
    
  </entry>
  
  <entry>
    <title>GAN - Github List</title>
    <link href="https://world4jason.github.io/2017/11/07/GAN-Github-List/"/>
    <id>https://world4jason.github.io/2017/11/07/GAN-Github-List/</id>
    <published>2017-11-07T02:34:33.000Z</published>
    <updated>2017-12-13T14:47:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>paperList<br><a href="https://github.com/zhangqianhui/AdversarialNetsPapers" target="_blank" rel="external">https://github.com/zhangqianhui/AdversarialNetsPapers</a><br><a href="https://github.com/hindupuravinash/the-gan-zoo" target="_blank" rel="external">https://github.com/hindupuravinash/the-gan-zoo</a><br><a href="https://github.com/nightrome/really-awesome-gan" target="_blank" rel="external">https://github.com/nightrome/really-awesome-gan</a><br><a href="https://github.com/nashory/gans-awesome-applications" target="_blank" rel="external">https://github.com/nashory/gans-awesome-applications</a><br><a href="https://github.com/GKalliatakis/Delving-deep-into-GANs" target="_blank" rel="external">https://github.com/GKalliatakis/Delving-deep-into-GANs</a></p><p>====IMPLEMENTATION====<br><a href="https://github.com/YadiraF/GAN" target="_blank" rel="external">https://github.com/YadiraF/GAN</a><br><a href="https://github.com/jonbruner/generative-adversarial-networks" target="_blank" rel="external">https://github.com/jonbruner/generative-adversarial-networks</a><br><a href="https://github.com/tjwei/GANotebooks" target="_blank" rel="external">https://github.com/tjwei/GANotebooks</a><br><a href="https://github.com/AaronYALai/Generative_Adversarial_Networks_PyTorch" target="_blank" rel="external">https://github.com/AaronYALai/Generative_Adversarial_Networks_PyTorch</a></p><p>最完整的GAN實作<br><a href="https://github.com/znxlwm/pytorch-generative-model-collections" target="_blank" rel="external">https://github.com/znxlwm/pytorch-generative-model-collections</a><br><a href="https://github.com/hwalsuklee/tensorflow-generative-model-collections" target="_blank" rel="external">https://github.com/hwalsuklee/tensorflow-generative-model-collections</a><br><a href="https://github.com/eriklindernoren/Keras-GAN" target="_blank" rel="external">https://github.com/eriklindernoren/Keras-GAN</a></p><p>用貓玩GAN<br><a href="https://github.com/AlexiaJM/Deep-learning-with-cats" target="_blank" rel="external">https://github.com/AlexiaJM/Deep-learning-with-cats</a></p><p>tensorflow跟GAN都有<br><a href="https://github.com/wiseodd/generative-models" target="_blank" rel="external">https://github.com/wiseodd/generative-models</a></p><p><a href="https://github.com/jtoy/awesome-tensorflow" target="_blank" rel="external">https://github.com/jtoy/awesome-tensorflow</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;paperList&lt;br&gt;&lt;a href=&quot;https://github.com/zhangqianhui/AdversarialNetsPapers&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/zhangqianh
      
    
    </summary>
    
      <category term="Github" scheme="https://world4jason.github.io/categories/Github/"/>
    
    
      <category term="Generation" scheme="https://world4jason.github.io/tags/Generation/"/>
    
      <category term="Generative Model" scheme="https://world4jason.github.io/tags/Generative-Model/"/>
    
      <category term="GAN" scheme="https://world4jason.github.io/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>R-CNN:Region proposals+CNN</title>
    <link href="https://world4jason.github.io/2017/11/06/R-CNN/"/>
    <id>https://world4jason.github.io/2017/11/06/R-CNN/</id>
    <published>2017-11-06T07:42:19.000Z</published>
    <updated>2017-11-07T15:30:03.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="RCNN-將CNN引入目標檢測的開山之作"><a href="#RCNN-將CNN引入目標檢測的開山之作" class="headerlink" title="RCNN- 將CNN引入目標檢測的開山之作"></a>RCNN- 將CNN引入目標檢測的開山之作</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15099664021450.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p><a href="http://cs231n.stanford.edu/slides/2016/winter1516_lecture8.pdf" target="_blank" rel="external">CS231n lecture8</a></p><p>RCNN (論文：Rich feature hierarchies for accurate object detection and semantic segmentation) 是將CNN方法引入目標檢測領域， 大大提高了目標檢測效果，可以說改變了目標檢測領域的主要研究思路， 緊隨其後的系列文章： （ RCNN ）, Fast RCNN , Faster RCNN 代表該領域當前最高水準。</p><p>【論文主要特點】（相對傳統方法的改進）</p><p>速度： 經典的目標檢測算法使用滑動窗法依次判斷所有可能的區域。 本文則(採用Selective Search方法)預先提取一系列較可能是物體的候選區域，之後僅在這些候選區域上(採用CNN)提取特徵，進行判斷。<br>訓練集： 經典的目標檢測算法在區域中提取人工設定的特徵。 本文則採用深度網絡進行特徵提取。 使用兩個數據庫： 一個較大的識​​別庫（ImageNet ILSVC 2012）：標定每張圖片中物體的類別。 一千萬圖像，1000類。 一個較小的檢測庫（PASCAL VOC 2007）：標定每張圖片中，物體的類別和位置，一萬圖像，20類。 本文使用識別庫進行預訓練得到CNN（有監督預訓練），而後用檢測庫調優參數，最後在檢測庫上評測。<br>看到這裡也許你已經對很多名詞很困惑，下面會解釋。 先來看看它的基本流程：</p><h1 id="基本流程"><a href="#基本流程" class="headerlink" title="基本流程"></a>基本流程</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15099664799300.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15099707654238.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>RCNN算法分為4個步驟 </p><p>候選區域生成： 一張圖像生成1K~2K個候選區域（採用Selective Search 方法）<br>特徵提取： 對每個候選區域，使用深度卷積網絡提取特徵（CNN）<br>類別判斷： 特徵送入每一類的SVM 分類器，判別是否屬於該類<br>位置精修： 使用回歸器精細修正候選框位置 </p><p>【基礎知識===================================】</p><h3 id="Selective-Search"><a href="#Selective-Search" class="headerlink" title="Selective Search"></a>Selective Search</h3><p>主要思想:<br>使用一種過分割手段，將圖像分割成小區域(1k~2k 個)<br>查看現有小區域，按照合併規則合併可能性最高的相鄰兩個區域。 重複直到整張圖像合併成一個區域位置<br>輸出所有曾經存在過的區域，所謂候選區域<br>其中合併規則如下： 優先合併以下四種區域：</p><p>顏色（顏色直方圖）相近的<br>紋理（梯度直方圖）相近的<br>合併後總面積小的： 保證合併操作的尺度較為均勻，避免一個大區域陸續“吃掉”其他小區域（例：設有區域abcdefgh。較好的合併方式是：ab-cd-ef-gh - &gt; abcd-efgh -&gt; abcdefgh。 不好的合併方法是：ab-cdefgh -&gt;abcd-efgh -&gt;abcdef-gh -&gt; abcdefgh）<br>合併後，總面積在其BBOX中所佔比例大的： 保證合併後形狀規則。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15099665196976.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>上述四條規則只涉及區域的顏色直方圖、梯度直方圖、面積和位置。 合併後的區域特徵可以直接由子區域特徵計算而來，速度較快。</p><p>有監督預訓練與無監督預訓練: </p><p>(1)無監督預訓練(Unsupervised pre-training)</p><p>預訓練階段的樣本不需要人工標註數據，所以就叫做無監督預訓練。</p><p>(2)有監督預訓練(Supervised pre-training)</p><p>所謂的有監督預訓練也可以把它稱之為遷移學習。 比如你已經有一大堆標註好的人臉年齡分類的圖片數據，訓練了一個CNN，用於人臉的年齡識別。 然後當你遇到新的項目任務時：人臉性別識別，那麼這個時候你可以利用已經訓練好的年齡識別CNN模型，去掉最後一層，然後其它的網絡層參數就直接複製過來，繼續進行訓練，讓它輸出性別。 這就是所謂的遷移學習，說的簡單一點就是把一個任務訓練好的參數，拿到另外一個任務，作為神經網絡的初始參數值,這樣相比於你直接採用隨機初始化的方法，精度可以有很大的提高。 </p><p>對於目標檢測問題： 圖片分類標註好的訓練數據非常多，但是物體檢測的標註數據卻很少，如何用少量的標註數據，訓練高質量的模型，這就是文獻最大的特點，這篇論文采用了遷移學習的思想：先用了ILSVRC2012這個訓練數據庫（這是一個圖片分類訓練數據庫），先進行網絡圖片分類訓練。 這個數據庫有大量的標註數據，共包含了1000種類別物體，因此預訓練階段CNN模型的輸出是1000個神經元（當然也直接可以採用Alexnet訓練好的模型參數）。</p><h3 id="重疊度（IOU）"><a href="#重疊度（IOU）" class="headerlink" title="重疊度（IOU）:"></a>重疊度（IOU）:</h3><p>物體檢測需要定位出物體的bounding box，就像下面的圖片一樣，我們不僅要定位出車輛的bounding box 我們還要識別出bounding box 裡面的物體就是車輛。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15099665384173.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>對於bounding box的定位精度，有一個很重要的概念： 因為我們算法不可能百分百跟人工標註的數據完全匹配，因此就存在一個定位精度評價公式：IOU。 它定義了兩個bounding box的重疊度，如下圖所示 </p><p><img src="/media/15099665471055.jpg" alt=""></p><p>就是矩形框A、B的重疊面積佔A、B並集的面積比例。</p><h3 id="非極大值抑制（-NMS-）："><a href="#非極大值抑制（-NMS-）：" class="headerlink" title="非極大值抑制（ NMS ）："></a>非極大值抑制（ NMS ）：</h3><p>RCNN會從一張圖片中找出n個可能是物體的矩形框，然後為每個矩形框為做類別分類概率：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15099665602965.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>就像上面的圖片一樣，定位一個車輛，最後算法就找出了一堆的方框，我們需要判別哪些矩形框是沒用的。 非極大值抑制的方法是：先假設有6個矩形框，根據分類器的類別分類概率做排序，假設從小到大屬於車輛的概率分別為A、B、C、D、E、F。</p><p>(1)從最大概率矩形框F開始，分別判斷A~E與F的重疊度IOU是否大於某個設定的閾值;</p><p>(2)假設B、D與F的重疊度超過閾值，那麼就扔掉B、D；並標記第一個矩形框F，是我們保留下來的。</p><p>(3)從剩下的矩形框A、C、E中，選擇概率最大的E，然後判斷E與A、C的重疊度，重疊度大於一定的閾值，那麼就扔掉；並標記E是我們保留下來的第二個矩形框。</p><p>就這樣一直重複，找到所有被保留下來的矩形框。</p><p>非極大值抑制（NMS）顧名思義就是抑制不是極大值的元素，搜索局部的極大值。 這個局部代表的是一個鄰域，鄰域有兩個參數可變，一是鄰域的維數，二是鄰域的大小。 這裡不討論通用的NMS算法，而是用於在目標檢測中用於提取分數最高的窗口的。 例如在行人檢測中，滑動窗口經提取特徵，經分類器分類識別後，每個窗口都會得到一個分數。 但是滑動窗口會導致很多窗口與其他窗口存在包含或者大部分交叉的情況。 這時就需要用到NMS來選取那些鄰域里分數最高（是行人的概率最大），並且抑制那些分數低的窗口。</p><p>###VOC物體檢測任務:</p><p>相當於一個競賽，裡麵包含了20個物體類別： PASCAL VOC2011 Example Images 還有一個背景，總共就相當於21個類別，因此一會設計fine-tuning CNN的時候，我們softmax分類輸出層為21個神經元。</p><p>【各個階段詳解===================================】 </p><p>總體思路再回顧：</p><p>首先對每一個輸入的圖片產生近2000個不分種類的候選區域（region proposals），然後使用CNNs從每個候選框中提取一個固定長度的特徵向量（4096維度），接著對每個取出的特徵向量使用特定種類的線性SVM進行分類。 也就是總個過程分為三個程序：a、找出候選框；b、利用CNN提取特徵向量；c、利用SVM進行特徵向量分類。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15099666382907.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h4 id="候選框搜索階段："><a href="#候選框搜索階段：" class="headerlink" title="候選框搜索階段："></a>候選框搜索階段：</h4><p>當我們輸入一張圖片時，我們要搜索出所有可能是物體的區域，這裡採用的就是前面提到的Selective Search方法，通過這個算法我們搜索出2000個候選框。 然後從上面的總流程圖中可以看到，搜出的候選框是矩形的，而且是大小各不相同。 然而CNN對輸入圖片的大小是有固定的，如果把搜索到的矩形選框不做處理，就扔進CNN中，肯定不行。 因此對於每個輸入的候選框都需要縮放到固定的大小。 下面我們講解要怎麼進行縮放處理，為了簡單起見我們假設下一階段CNN所需要的輸入圖片大小是個正方形圖片227*227。 因為我們經過selective search 得到的是矩形框，paper試驗了兩種不同的處理方法：</p><p>(1)各向異性縮放</p><p>這種方法很簡單，就是不管圖片的長寬比例，管它是否扭曲，進行縮放就是了，全部縮放到CNN輸入的大小227*227，如下圖(D)所示；<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15099666636066.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>(2)各向同性縮放</p><p>因為圖片扭曲後，估計會對後續CNN的訓練精度有影響，於是作者也測試了“各向同性縮放”方案。 有兩種辦法</p><p>A、先擴充後裁剪： 直接在原始圖片中，把bounding box的邊界進行擴展延伸成正方形，然後再進行裁剪；如果已經延伸到了原始圖片的外邊界，那麼就用bounding box中的顏色均值填充；如上圖(B)所示;</p><p>B、先裁剪後擴充：先把bounding box圖片裁剪出來，然後用固定的背景顏色填充成正方形圖片(背景顏色也是採用bounding box的像素顏色均值),如上圖(C)所示;</p><p>對於上面的異性、同性縮放，文獻還有個padding處理，上面的示意圖中第1、3行就是結合了padding=0,第2、4行結果圖採用padding=16的結果。 經過最後的試驗，作者發現採用各向異性縮放、padding=16的精度最高。</p><p>（備註：候選框的搜索策略作者也考慮過使用一個滑動窗口的方法，然而由於更深的網絡，更大的輸入圖片和滑動步長，使得使用滑動窗口來定位的方法充滿了挑戰。） </p><p>CNN特徵提取階段： </p><p>1、算法實現</p><p>a、網絡結構設計階段<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15099666918238.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15099666873410.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>網絡架構兩個可選方案：第一選擇經典的Alexnet；第二選擇VGG16。 經過測試Alexnet精度為58.5%，VGG16精度為66%。 VGG這個模型的特點是選擇比較小的捲積核、選擇較小的跨步，這個網絡的精度高，不過計算量是Alexnet的7倍。 後面為了簡單起見，我們就直接選用Alexnet，並進行講解；Alexnet特徵提取部分包含了5個卷積層、2個全連接層，在Alexnet中p5層神經元個數為9216、 f6、f7的神經元個數都是4096，通過這個網絡訓練完畢後，最後提取特徵每個輸入候選框圖片都能得到一個4096維的特徵向量。</p><p>b、網絡有監督預訓練階段（圖片數據庫：ImageNet ILSVC ）<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15099667042004.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>參數初始化部分：物體檢測的一個難點在於，物體標籤訓練數據少，如果要直接採用隨機初始化CNN參數的方法，那麼目前的訓練數據量是遠遠不夠的。 這種情況下，最好的是採用某些方法，把參數初始化了，然後在進行有監督的參數微調，這里文獻採用的是有監督的預訓練。 所以paper在設計網絡結構的時候，是直接用Alexnet的網絡，然後連參數也是直接採用它的參數，作為初始的參數值，然後再fine-tuning訓練。 網絡優化求解時採用隨機梯度下降法，學習率大小為0.001；</p><p>c、fine-tuning階段（圖片數據庫： PASCAL VOC）</p><p>我們接著採用selective search 搜索出來的候選框（PASCAL VOC 數據庫中的圖片） 繼續對上面預訓練的CNN模型進行fine-tuning訓練。 假設要檢測的物體類別有N類，那麼我們就需要把上面預訓練階段的CNN模型的最後一層給替換掉，替換成N+1個輸出的神經元(加1，表示還有一個背景) (20 + 1bg = 21)，然後這一層直接採用參數隨機初始化的方法，其它網絡層的參數不變；接著就可以開始繼續SGD訓練了。 開始的時候，SGD學習率選擇0.001，在每次訓練的時候，我們batch size大小選擇128，其中32個事正樣本、96個事負樣本。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15099667194897.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>關於正負樣本問題：</p><p>一張照片我們得到了2000個候選框。 然而人工標註的數據一張圖片中就只標註了正確的bounding box，我們搜索出來的2000個矩形框也不可能會出現一個與人工標註完全匹配的候選框。 因此在CNN階段我們需要用IOU為2000個bounding box打標籤。 如果用selective search挑選出來的候選框與物體的人工標註矩形框（PASCAL VOC的圖片都有人工標註）的重疊區域IoU大於0.5，那麼我們就把這個候選框標註成物體類別（正樣本），否則我們就把它當做背景類別（負樣本）。</p><p>（備註： 如果不針對特定任務進行fine-tuning，而是把CNN當做特徵提取器，卷積層所學到的特徵其實就是基礎的共享特徵提取層，就類似於SIFT算法一樣，可以用於提取各種圖片的特徵，而f6、f7所學習到的特徵是用於針對特定任務的特徵。打個比方：對於人臉性別識別來說，一個CNN模型前面的捲積層所學習到的特徵就類似於學習人臉共性特徵，然後全連接層所學習的特徵就是針對性別分類的特徵了）</p><p>2.疑惑點 ： CNN訓練的時候，本來就是對bounding box的物體進行識別分類訓練，在訓練的時候最後一層softmax就是分類層。 那麼為什麼作者閒著沒事幹要先用CNN做特徵提取（提取fc7層數據），然後再把提取的特徵用於訓練svm分類器？ </p><p>這個是因為svm訓練和cnn訓練過程的正負樣本定義方式各有不同，導致最後採用CNN softmax輸出比採用svm精度還低。 事情是這樣的，cnn在訓練的時候，對訓練數據做了比較寬鬆的標註，比如一個bounding box可能只包含物體的一部分，那麼我也把它標註為正樣本，用於訓練cnn；採用這個方法的主要原因在於因為CNN容易過擬合，所以需要大量的訓練數據，所以在CNN訓練階段我們是對Bounding box的位置限制條件限制的比較鬆(IOU只要大於0.5都被標註為正樣本了)；然而svm訓練的時候，因為svm適用於少樣本訓練，所以對於訓練樣本數據的IOU要求比較嚴格，我們只有當bounding box把整個物體都包含進去了，我們才把它標註為物體類別，然後訓練svm ，具體請看下文。</p><p>SVM訓練、測試階段</p><p>訓練階段 ：</p><p>這是一個二分類問題，我麼假設我們要檢測車輛。 我們知道只有當bounding box把整量車都包含在內，那才叫正樣本；如果bounding box 沒有包含到車輛，那麼我們就可以把它當做負樣本。 但問題是當我們的檢測窗口只有部分包含物體，那該怎麼定義正負樣本呢？ 作者測試了IOU閾值各種方案數值0,0.1,0.2,0.3,0.4,0.5。 最後通過訓練發現，如果選擇IOU閾值為0.3 效果最好 （選擇為0精度下降了4個百分點，選擇0.5精度下降了5個百分點）,即當重疊度小於0.3的時候，我們就把它標註為負樣本。 一旦CNN f7層特徵被提取出來，那麼我們將為每個物體類訓練一個svm分類器。 當我們用CNN提取2000個候選框，可以得到2000x4096這樣的特徵向量矩陣，然後我們只需要把這樣的一個矩陣與svm權值矩陣4096xN點乘(N為分類類別數目，因為我們訓練的N個svm，每個svm包含了4096個權值w)，就可以得到結果了。 </p><p><img src="/media/15099667603349.jpg" alt=""></p><p>得到的特徵輸入到SVM進行分類看看這個feature vector所對應的region proposal是需要的物體還是無關的實物(background) 。 排序，canny邊界檢測之後就得到了我們需要的bounding-box。 </p><p>再回顧總結一下：整個系統分為三個部分：1.產生不依賴與特定類別的region proposals，這些region proposals定義了一個整個檢測器可以獲得的候選目標2.一個大的捲積神經網絡，對每個region產生一個固定長度的特徵向量3.一系列特定類別的線性SVM分類器。</p><p>位置精修： 目標檢測問題的衡量標準是重疊面積：許多看似準確的檢測結果，往往因為候選框不夠準確，重疊面積很小。 故需要一個位置精修步驟。 回歸器：對每一類目標，使用一個線性脊回歸器進行精修。 正則項λ=10000。 輸入為深度網絡pool5層的4096維特徵，輸出為xy方向的縮放和平移。 訓練樣本：判定為本類的候選框中和真值重疊面積大於0.6的候選框。 </p><p>測試階段 ：</p><p>使用selective search的方法在測試圖片上提取2000個region propasals ，將每個region proposals歸一化到227x227，然後再CNN中正向傳播，將最後一層得到的特徵提取出來。 然後對於每一個類別，使用為這一類訓練的SVM分類器對提取的特徵向量進行打分，得到測試圖片中對於所有region proposals的對於這一類的分數，再使用貪心的非極大值抑制（ NMS）去除相交的多餘的框。 再對這些框進行canny邊緣檢測，就可以得到bounding-box(then B-BoxRegression)。</p><p>（非極大值抑制（NMS）先計算出每一個bounding box的面積，然後根據score進行排序，把score最大的bounding box作為選定的框，計算其餘bounding box與當前最大score與box的IoU，去除IoU大於設定的閾值的bounding box。然後重複上面的過程，直至候選bounding box為空，然後再將score小於一定閾值的選定框刪除得到這一類的結果（然後繼續進行下一個分類） 。作者提到花費在region propasals和提取特徵的時間是13s/張-GPU和53s/張-CPU，可以看出時間還是很長的，不能夠達到及時性。 </p><p>完。</p><p>本文主要整理自以下文章：</p><p>RCNN學習筆記(0):rcnn簡介<br>RCNN學習筆記(1):Rich feature hierarchies for accurate object detection and semantic segmentation<br>RCNN學習筆記(2):Rich feature hierarchies for accurate object detection and semantic segmentation<br>《Rich feature hierarchies for Accurate Object Detection and Segmentation》<br>《Spatial 《Pyramid Pooling in Deep Convolutional Networks for Visual Recognition》</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;RCNN-將CNN引入目標檢測的開山之作&quot;&gt;&lt;a href=&quot;#RCNN-將CNN引入目標檢測的開山之作&quot; class=&quot;headerlink&quot; title=&quot;RCNN- 將CNN引入目標檢測的開山之作&quot;&gt;&lt;/a&gt;RCNN- 將CNN引入目標檢測的開山之作&lt;/h1
      
    
    </summary>
    
      <category term="Detection" scheme="https://world4jason.github.io/categories/Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://world4jason.github.io/tags/Deep-Learning/"/>
    
      <category term="R-CNN" scheme="https://world4jason.github.io/tags/R-CNN/"/>
    
      <category term="CNN" scheme="https://world4jason.github.io/tags/CNN/"/>
    
      <category term="Object Detection" scheme="https://world4jason.github.io/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>Object Detection with Convolution Neural Network Series</title>
    <link href="https://world4jason.github.io/2017/11/06/Object-Detection-with-Convolution-Neural-Network/"/>
    <id>https://world4jason.github.io/2017/11/06/Object-Detection-with-Convolution-Neural-Network/</id>
    <published>2017-11-05T18:51:33.000Z</published>
    <updated>2017-11-08T14:51:01.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Object-Detection-with-Convolution-Neural-Network-Series"><a href="#Object-Detection-with-Convolution-Neural-Network-Series" class="headerlink" title="Object Detection with Convolution Neural Network Series"></a>Object Detection with Convolution Neural Network Series</h1><p>Outline</p><ul><li>檢測任務?</li><li>R-CNN</li><li>Fast R-CNN</li><li>Faster R-CNN</li><li>SSD</li><li>YOLO</li></ul><h2 id="檢測任務"><a href="#檢測任務" class="headerlink" title="檢測任務"></a>檢測任務</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15100388874028.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h2 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h2><center><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15099707654238.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></center><p>步驟 </p><ol><li>候選區域生成： 一張圖像生成1K~2K個候選區域（採用Selective Search 方法） </li><li>特徵提取： 對每個候選區域，使用深度卷積網絡提取特徵（CNN） </li><li>類別判斷： 特徵送入每一類的SVM 分類器，判別是否屬於該類 </li><li>位置精修： 使用回歸器精細修正候選框位置 </li><li>用Non-Maximum Selection 合併後選框</li></ol><h2 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h2><center><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15100371122830.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></center><br>步驟<br>1. 候選區域生成： 一張圖像生成1K~2K個候選區域（採用Selective Search 方法）<br>2. 特徵提取： 該張圖片，使用深度卷積網絡提取特徵（CNN）<br>3. ROI Pooling：<br>4. 類別判斷： 特徵送入每一類的SVM 分類器，判別是否屬於該類<br>5. 位置精修： 使用回歸器精細修正候選框位置<br>6. 用Non-Maximum Selection 合併後選框<br><br>## Faster R-CNN<br><center><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15100371249624.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></center><h2 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h2><center><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15100374296902.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br></center><p>Reference:</p><ol><li><a href="http://zh.gluon.ai/" target="_blank" rel="external">http://zh.gluon.ai/</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Object-Detection-with-Convolution-Neural-Network-Series&quot;&gt;&lt;a href=&quot;#Object-Detection-with-Convolution-Neural-Network-Series&quot; class=&quot;h
      
    
    </summary>
    
      <category term="Detection" scheme="https://world4jason.github.io/categories/Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://world4jason.github.io/tags/Deep-Learning/"/>
    
      <category term="R-CNN" scheme="https://world4jason.github.io/tags/R-CNN/"/>
    
      <category term="CNN" scheme="https://world4jason.github.io/tags/CNN/"/>
    
      <category term="Object Detection" scheme="https://world4jason.github.io/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>CPP 筆記</title>
    <link href="https://world4jason.github.io/2017/10/19/CPP-note/"/>
    <id>https://world4jason.github.io/2017/10/19/CPP-note/</id>
    <published>2017-10-19T04:07:48.000Z</published>
    <updated>2017-12-19T04:11:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>##C++中cin、cin.get()、cin.getline()、getline()、gets()等函数的用法<br>学C++的时候，这几个输入函数弄的有点迷糊；这里做个小结，为了自己复习，也希望对后来者能有所帮助，如果有差错的地方还请各位多多指教（本文所有程序均通过VC 6.0运行）</p><p>1、cin<br>2、cin.get()<br>3、cin.getline()<br>4、getline()<br>5、gets()<br>6、getchar()</p><p>附:cin.ignore();cin.get()//跳过一个字符,例如不想要的回车,空格等字符</p><p>1、cin&gt;&gt;         </p><p>用法1：最基本，也是最常用的用法，输入一个数字：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt; </span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>; </div><div class="line">main () </div><div class="line">&#123; </div><div class="line"><span class="keyword">int</span> a,b; </div><div class="line"><span class="built_in">cin</span>&gt;&gt;a&gt;&gt;b; </div><div class="line"><span class="built_in">cout</span>&lt;&lt;a+b&lt;&lt;<span class="built_in">endl</span>; </div><div class="line">&#125;</div></pre></td></tr></table></figure><p>输入：2[回车]3[回车]<br>输出：5</p><p>注意:&gt;&gt; 是会过滤掉不可见字符（如 空格 回车，TAB 等）<br>cin&gt;&gt;noskipws&gt;&gt;input[j];//不想略过空白字符，那就使用 noskipws 流控制</p><p>用法2：接受一个字符串，遇“空格”、“TAB”、“回车”都结束</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt; </span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>; </div><div class="line">main () </div><div class="line">&#123; </div><div class="line"><span class="keyword">char</span> a[<span class="number">20</span>]; </div><div class="line"><span class="built_in">cin</span>&gt;&gt;a; </div><div class="line"><span class="built_in">cout</span>&lt;&lt;a&lt;&lt;<span class="built_in">endl</span>; </div><div class="line">&#125;</div></pre></td></tr></table></figure><p>输入：jkljkljkl<br>输出：jkljkljkl</p><p>输入：jkljkl jkljkl       //遇空格结束<br>输出：jkljkl</p><p>2、cin.get()</p><p>用法1： cin.get(字符变量名)可以用来接收字符</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt; </span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>; </div><div class="line">main () </div><div class="line">&#123; </div><div class="line"><span class="keyword">char</span> ch; </div><div class="line">ch=<span class="built_in">cin</span>.get();               <span class="comment">//或者cin.get(ch); </span></div><div class="line"><span class="built_in">cout</span>&lt;&lt;ch&lt;&lt;<span class="built_in">endl</span>; </div><div class="line">&#125;</div></pre></td></tr></table></figure><p>输入：jljkljkl<br>输出：j</p><p>用法2：cin.get(字符数组名,接收字符数目)用来接收一行字符串,可以接收空格</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt; </span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>; </div><div class="line">main () </div><div class="line">&#123; </div><div class="line"><span class="keyword">char</span> a[<span class="number">20</span>]; </div><div class="line"><span class="built_in">cin</span>.get(a,<span class="number">20</span>); </div><div class="line"><span class="built_in">cout</span>&lt;&lt;a&lt;&lt;<span class="built_in">endl</span>; </div><div class="line">&#125;</div></pre></td></tr></table></figure><p>输入：jkl jkl jkl<br>输出：jkl jkl jkl</p><p>输入：abcdeabcdeabcdeabcdeabcde （输入25个字符）<br>输出：abcdeabcdeabcdeabcd              （接收19个字符+1个’\0’）</p><p>用法3：cin.get(无参数)没有参数主要是用于舍弃输入流中的不需要的字符,或者舍弃回车,弥补cin.get(字符数组名,接收字符数目)的不足.</p><p>这个我还不知道怎么用，知道的前辈请赐教；</p><p>3、cin.getline()   // 接受一个字符串，可以接收空格并输出</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt; </span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>; </div><div class="line">main () </div><div class="line">&#123; </div><div class="line"><span class="keyword">char</span> m[<span class="number">20</span>]; </div><div class="line"><span class="built_in">cin</span>.getline(m,<span class="number">5</span>); </div><div class="line"><span class="built_in">cout</span>&lt;&lt;m&lt;&lt;<span class="built_in">endl</span>; </div><div class="line">&#125;</div></pre></td></tr></table></figure><p>输入：jkljkljkl<br>输出：jklj</p><p>接受5个字符到m中，其中最后一个为’\0’，所以只看到4个字符输出；</p><p>如果把5改成20：<br>输入：jkljkljkl<br>输出：jkljkljkl</p><p>输入：jklf fjlsjf fjsdklf<br>输出：jklf fjlsjf fjsdklf</p><p>//延伸：<br>//cin.getline()实际上有三个参数，cin.getline(接受字符串的看哦那间m,接受个数5,结束字符)<br>//当第三个参数省略时，系统默认为’\0’<br>//如果将例子中cin.getline()改为cin.getline(m,5,’a’);当输入jlkjkljkl时输出jklj，输入jkaljkljkl时，输出jk</p><p>当用在多维数组中的时候，也可以用cin.getline(m[i],20)之类的用法：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt; </span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt; </span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"></div><div class="line">main () </div><div class="line">&#123; </div><div class="line"><span class="keyword">char</span> m[<span class="number">3</span>][<span class="number">20</span>]; </div><div class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="number">3</span>;i++) </div><div class="line">&#123; </div><div class="line"><span class="built_in">cout</span>&lt;&lt;<span class="string">"\n请输入第"</span>&lt;&lt;i+<span class="number">1</span>&lt;&lt;<span class="string">"个字符串："</span>&lt;&lt;<span class="built_in">endl</span>; </div><div class="line"><span class="built_in">cin</span>.getline(m[i],<span class="number">20</span>); </div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="built_in">cout</span>&lt;&lt;<span class="built_in">endl</span>; </div><div class="line"><span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;<span class="number">3</span>;j++) </div><div class="line"><span class="built_in">cout</span>&lt;&lt;<span class="string">"输出m["</span>&lt;&lt;j&lt;&lt;<span class="string">"]的值:"</span>&lt;&lt;m[j]&lt;&lt;<span class="built_in">endl</span>;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure><p>请输入第1个字符串：<br>kskr1</p><p>请输入第2个字符串：<br>kskr2</p><p>请输入第3个字符串：<br>kskr3</p><p>输出m[0]的值:kskr1<br>输出m[1]的值:kskr2<br>输出m[2]的值:kskr3</p><p>4、getline()     // 接受一个字符串，可以接收空格并输出，需包含“#include<string>”</string></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt; </span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt; </span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>; </div><div class="line">main () </div><div class="line">&#123; </div><div class="line"><span class="built_in">string</span> str; </div><div class="line">getline(<span class="built_in">cin</span>,str); </div><div class="line"><span class="built_in">cout</span>&lt;&lt;str&lt;&lt;<span class="built_in">endl</span>; </div><div class="line">&#125;</div></pre></td></tr></table></figure><p>输入：jkljkljkl<br>输出：jkljkljkl</p><p>输入：jkl jfksldfj jklsjfl<br>输出：jkl jfksldfj jklsjfl</p><p>和cin.getline()类似，但是cin.getline()属于istream流，而getline()属于string流，是不一样的两个函数</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;##C++中cin、cin.get()、cin.getline()、getline()、gets()等函数的用法&lt;br&gt;学C++的时候，这几个输入函数弄的有点迷糊；这里做个小结，为了自己复习，也希望对后来者能有所帮助，如果有差错的地方还请各位多多指教（本文所有程序均通过VC
      
    
    </summary>
    
      <category term="Code" scheme="https://world4jason.github.io/categories/Code/"/>
    
    
      <category term="code" scheme="https://world4jason.github.io/tags/code/"/>
    
      <category term="CPP" scheme="https://world4jason.github.io/tags/CPP/"/>
    
      <category term="C++" scheme="https://world4jason.github.io/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://world4jason.github.io/2017/03/28/hello-world/"/>
    <id>https://world4jason.github.io/2017/03/28/hello-world/</id>
    <published>2017-03-28T06:57:15.000Z</published>
    <updated>2017-03-28T06:57:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
