<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>world4jason</title>
  
  <subtitle>菜鳥搬磚日常</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://world4jason.github.io/"/>
  <updated>2018-01-31T04:13:11.000Z</updated>
  <id>https://world4jason.github.io/</id>
  
  <author>
    <name>Jason Yeh</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ICNET for Real-Time Semantic Segmentation on High-Resolution Images</title>
    <link href="https://world4jason.github.io/2018/01/28/ICNET-for-Real-Time-Semantic-Segmentation-on-High-Resolution-Images/"/>
    <id>https://world4jason.github.io/2018/01/28/ICNET-for-Real-Time-Semantic-Segmentation-on-High-Resolution-Images/</id>
    <published>2018-01-28T08:50:31.000Z</published>
    <updated>2018-01-31T04:13:11.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Recap"><a href="#Recap" class="headerlink" title="Recap"></a>Recap</h2><p>Quote:<br>其實很討厭這作者的paper<br>效果都很好, 但是每次都是用matlab, 而且PSPNet作者還說training code因為公司問題不能發布, 傻眼<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15172367977860.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><a href="https://www.zhihu.com/question/53356671" target="_blank" rel="external">https://www.zhihu.com/question/53356671</a></p><p>Paper<br><a href="https://arxiv.org/pdf/1704.08545.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1704.08545.pdf</a><br>Code<br><a href="https://github.com/hszhao/ICNet" target="_blank" rel="external">https://github.com/hszhao/ICNet</a><br><a href="https://github.com/aitorzip/Keras-ICNet" target="_blank" rel="external">https://github.com/aitorzip/Keras-ICNet</a><br><a href="https://github.com/hellochick/ICNet-tensorflow" target="_blank" rel="external">https://github.com/hellochick/ICNet-tensorflow</a></p><p>Key Difference<br>之前的那些方法，如FCN、SegNet、UNet、RefineNet等，用高解析度圖片當input以後，強調Single scale或是Multi Scale在不同層之間的特徵融合，所有的Data需要在整個網絡中運行，因為高解析度的輸入而導致了昂貴的計算費用.而本文的方法，使用低解析度圖片作為主要輸入，採用高解析度圖片進行refine，保留細節的同時減少了開銷.</p><p>##Abstract</p><figure class="highlight armasm"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="symbol">We</span> propose an compressed-PSPNet-<span class="keyword">based </span>image cascade network (ICNet) that incorporates <span class="keyword">multi-resolution </span><span class="keyword">branches </span>under proper label guidance to <span class="keyword">address </span>this challenge.</div></pre></td></tr></table></figure><p>ICNet是一個基於PSPNet的real-time semantic segmentation network，論文內對PSPNet做深入的分析，並且找出影響inference speed*的缺點。並且用搭配multi-resolution cascade combination。</p><p>*註:inference speed是單指DeConv的階段。</p><p>##Introduction</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15172162210529.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>在論文發表的時刻(2017 March), CityScapes上所有的Model表現基本上分成兩種類型, 一種是擁有高精準度但速度不行, 另一種是速度快但精准度不行。此論文在PSPNet的基礎上來增進速度，並找一個速度跟精準度的平衡點。</p><p>論文貢獻:</p><ul><li>可以在1024x2048的解析度下保持30 fps的計算速度(Tensorflow版本實測可行, 但要去掉preproccess部分)</li><li>相對PSPNet來說, 可疑提高5倍速度並可以減少五倍RAM消耗</li><li>低解析的速度+高解析的細節做cascade的整合</li></ul><h2 id="Speed-Analysis"><a href="#Speed-Analysis" class="headerlink" title="Speed Analysis"></a>Speed Analysis</h2><p>從PSPNet做解析<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15172351270369.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>藍色是1024x2048, 綠色是512x1024 (1/4大小)<br>從上圖可知</p><ul><li>圖越大速度越慢</li><li>網路寬度越大速度越慢</li><li>Kernel越多速度越慢, 以圖中例子來說stage4跟stage5在解析同樣的input時, inference speed差距十分驚人, 因為這部分的kernel number差距了一倍。</li></ul><h2 id="Intuitive-Speedup"><a href="#Intuitive-Speedup" class="headerlink" title="Intuitive Speedup"></a>Intuitive Speedup</h2><h4 id="加速方法-1-輸入向下採樣-Downsampling-Input"><a href="#加速方法-1-輸入向下採樣-Downsampling-Input" class="headerlink" title="加速方法 1: 輸入向下採樣(Downsampling Input)"></a>加速方法 1: 輸入向下採樣(Downsampling Input)</h4><p>在resolution只有原本的0.5跟0.25的狀況下雖然速度變快但精准度如上圖所示可以看出效果很差。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15172353341447.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h4 id="加速方法-2-利用較小的feature-map來做inference-Downsampling-Feature"><a href="#加速方法-2-利用較小的feature-map來做inference-Downsampling-Feature" class="headerlink" title="加速方法 2 : 利用較小的feature map來做inference(Downsampling Feature)"></a>加速方法 2 : 利用較小的feature map來做inference(Downsampling Feature)</h4><p>FCN Downsampling到32倍, Deep Lab到 8倍, 而下方是用作者之前的PSPNet50, 縮小到了1:8, 1:16, 1:32整理的Table, 但可以看到最快的速度也只有132ms, 不太能符合real-time的標準。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15172758974357.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h4 id="加速方法-3-減少模型複雜度-Model-Compression"><a href="#加速方法-3-減少模型複雜度-Model-Compression" class="headerlink" title="加速方法 3 : 減少模型複雜度(Model Compression)"></a>加速方法 3 : 減少模型複雜度(Model Compression)</h4><p>採用了其他篇paper(Pruning filters for efficient convnets)，作法就是減少Filter數量, 但一樣差強人意<br> <figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15172822508697.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h4 id="FCN-Fully-Convolutional-Networks-for-Fully-Convolutional-Networks"><a href="#FCN-Fully-Convolutional-Networks-for-Fully-Convolutional-Networks" class="headerlink" title="FCN:Fully Convolutional Networks for Fully Convolutional Networks"></a>FCN:Fully Convolutional Networks for Fully Convolutional Networks</h4><p>這裡額外多講一下FCN，算是CNN做semantic segmentation的始祖，本質上的區別大概就是…FCN是沒有全連結層的CNN，好處是可以接受任意大小輸入。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15172765679435.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>CNN要如何轉FCN? 以此篇paper為例，input是一個224x224x3的圖，經過一系列Conv跟Downsampling之後是7x7x512。<br>AlexNet使用了兩個4096的全連接層，最後一個有1000個神經元的全連接層用於計算分類評分。我們可以將這3個全連接層轉化為Convolution層。</p><p>任一全連結層轉化為Conv的方式以以下為例：</p><h6 id="例如-K-4096-的全連接層，輸入是7x7x512，這個全連接層可以被等效地看做一個F-7-Padding-0-Stride-1-Filter-Number-4096-的Conv層。換句話說，就是將Filter-Size設置的和Input-Data-Size一致了。輸出將變成-1x1x4096，這個結果就和使用初始的那個全連接層一樣了。"><a href="#例如-K-4096-的全連接層，輸入是7x7x512，這個全連接層可以被等效地看做一個F-7-Padding-0-Stride-1-Filter-Number-4096-的Conv層。換句話說，就是將Filter-Size設置的和Input-Data-Size一致了。輸出將變成-1x1x4096，這個結果就和使用初始的那個全連接層一樣了。" class="headerlink" title="例如 K=4096 的全連接層，輸入是7x7x512，這個全連接層可以被等效地看做一個F=7,Padding=0,Stride=1,Filter Number=4096 的Conv層。換句話說，就是將Filter Size設置的和Input Data Size一致了。輸出將變成 1x1x4096，這個結果就和使用初始的那個全連接層一樣了。"></a>例如 K=4096 的全連接層，輸入是7x7x512，這個全連接層可以被等效地看做一個F=7,Padding=0,Stride=1,Filter Number=4096 的Conv層。換句話說，就是將Filter Size設置的和Input Data Size一致了。輸出將變成 1x1x4096，這個結果就和使用初始的那個全連接層一樣了。</h6><p>針對第一個連接區域是[7x7x512]的全連接層，令其Filter Size為F=7<strong>（Filter Size為7x7）</strong>，這樣輸出為[1x1x4096]。<br>針對第二個全連接層，令其Filter Size為F=1<strong>（Filter Size為1x1）</strong>，這樣輸出為[1x1x4096]。<br>對最後一個全連接層也做類似的，令其F=1<strong>（Filter Size為1x1）</strong>，最終輸出為[1x1x1000]</p><h5 id="Step-1"><a href="#Step-1" class="headerlink" title="Step 1:"></a>Step 1:</h5><p>下圖是是原始CNN結構，CNN中輸入的圖像大小是統一固定resize成227x227大小的圖像，第一層pooling後為55x55，第二層pooling後為27x27，第五層pooling後的圖像大小為13*13。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15172417899356.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h5 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2:"></a>Step 2:</h5><p>FCN輸入的圖像是假設是H*W，第一層pooling後變為原圖大小的1/4，第二層變為的1/8，第五層變為1/ 16，第八層變為1/32<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15172417973947.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h5 id="Step-3"><a href="#Step-3" class="headerlink" title="Step 3:"></a>Step 3:</h5><p>Convolution本質上就是DownSampling（下採樣）。經過多次Convolution和pooling以後，得到的圖像越來越小，解析度越來越低。其中圖像到H/32∗W/32 的時候圖片是最小的一層時，所產生圖叫做heatmap，heatmap就是我們最重要的高維特徵圖，得到高維特徵的heatmap之後就是最重要的一步也是最後的一步，就是對此heatmap進行UpSampling(Deconvolution)，把圖像進行放大到原圖像的大小。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15172418178955.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h5 id="Step-4"><a href="#Step-4" class="headerlink" title="Step 4:"></a>Step 4:</h5><p>最後的輸出是1000張heatmap經過UpSampling變為原圖大小的圖片。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15172418074043.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h5 id="Upsampling"><a href="#Upsampling" class="headerlink" title="Upsampling"></a>Upsampling</h5><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/20161024115403020.gif" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>其實這篇paper內雖然叫做Deconvolution，但之前CS231n課程內的大神也有說到，叫做Transposed Convolution比較適合。<br>舉個例子來說：</p><p>4x4的圖片輸入，Filter Size為3x3, 沒有Padding / Stride, 則輸出為2x2。</p><p>輸入矩陣可展開為16維向量，記作<em>x</em><br>輸出矩陣可展開為4維向量，記作<em>y</em><br>Convolution運算可表示為<em>y</em>=<em>Cx</em><br>C其實就是如下的稀疏陣，而Forwarding就改成了的矩陣運算<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15172789125366.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>BackPropagation的話，假若已經從更深的網路得到了</p><p><center><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15172790446136.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></center><br>那麼就可以導出以下公式:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15172790496567.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>Deconvolution其實就是Forwarding時乘CT，而BackPropagation時乘(CT)T，即C。總結來說，Deconvolution等於Convolution在神經網絡結構的正向和反向傳播中的計算，做相反的計算。</p><h5 id="Skip-Architecture"><a href="#Skip-Architecture" class="headerlink" title="Skip Architecture"></a>Skip Architecture</h5><p>由於縮小32倍結果超糟糕，所以FCN在前面的Pooling Layer進行Upsampling，然後結合這些結果來優化輸出。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15172794424801.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p>總結一下前面速度分析的結果，一系列的優化方法：</p><ul><li>Downsampling Input：降低輸入解析度能都大幅度的加速，但同時會讓預測非常模糊</li><li>Downsampling Feature：可以加速但同時會降低準確率</li><li>Model Compression：壓縮訓練好的模型，通過減輕模型達到加速效果，可惜實驗效果不佳</li></ul><p>針對以上的分析，發現，低解析度的圖片能夠有效降低運行時間，但是失去很多細節，而且邊界模糊；但是高解析度的計算時間難以忍受，ICNet總結了上述幾個問題，提出了一個綜合性的方法：使用低解析度加速捕捉語義，使用高解析度獲取細節，使用特徵融合(CFF)結合，同時使用guide label來監督，在限制的時間內獲得有效的結果。</p><p><img src="/media/15172827737568.png" alt=""></p><h4 id="Branch-Analysis"><a href="#Branch-Analysis" class="headerlink" title="Branch Analysis"></a>Branch Analysis</h4><p>圖中用了原尺寸,1/2,1/4當input，低解析度分枝超過50層Convolution，來提取更多的語義信息(inference 18 ms)，中解析度分枝有17層Convolution，但是由於權重共享，只有inference 6ms，而高解析度分枝是3 Convolution，有inference 9ms.</p><table><thead><tr><th style="text-align:center">分枝</th><th>過程</th><th>耗時</th></tr></thead><tbody><tr><td style="text-align:center">低解析</td><td>低解析是FCN-based PSPNet的架構，總和有超過50層的Convolution，在中解析度的1/16輸出的基礎上，再縮放到1/32.經過Convolution後，然後使用幾個dilated convolution擴展接受野但不縮小尺寸，最終以原圖的1/32大小輸出feature map。</td><td>雖然層數較多，但是解析度低，速度快，且與分枝二共享一部分權重，耗時為18 ms</td></tr><tr><td style="text-align:center">中解析</td><td>以原圖的1/2的解析度作為輸入，經過17層Convolution後以1/8縮放，得到原圖的1/16大小feature map，再將低解析度分枝的輸出feature map通過CFF(cascade feature fusion )單元相融合得到最終輸出。值得注意的是：低解析度和中解析度的捲積參數是共享的。</td><td>有17個Convolution層，與分枝一共享一部分權重，與分枝一一起一共耗時6ms</td></tr><tr><td style="text-align:center">高解析</td><td>原圖輸入，經過三層的Convolution(Stride=2,Size=3x3)得到原圖的1/8大小的feature map，再將中解析度處理後的輸出通過CFF單元融合</td><td>有3個卷積層，雖然解析度高，因為少，耗時為9ms</td></tr></tbody></table><p>對於每個分枝的輸出特徵，首先會上採樣2倍做輸出，在訓練的時候，會以Ground truth的1/16、1/8/、1/4來指導各個分枝的訓練，這樣的輔助訓練使得梯度優化更為平滑，便於訓練收斂，隨著每個分枝學習能力的增強，預測沒有被任何一個分枝主導。利用這樣的漸變的特徵融合和級聯引導結構可以產生合理的預測結果。</p><p>ICNet使用低解析度完成語義分割，使用高解析度幫助細化結果。在結構上，產生的feature大大減少，同時仍然保持必要的細節。</p><h4 id="Cascade-Label-Guidance"><a href="#Cascade-Label-Guidance" class="headerlink" title="Cascade Label Guidance"></a>Cascade Label Guidance</h4><h4 id="Branch-Output"><a href="#Branch-Output" class="headerlink" title="Branch Output"></a>Branch Output</h4><p>不同分枝的預測效果如下:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15172838466580.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>可以看到第三個分枝輸出效果無疑是最好的。在測試時，只保留第三分枝的結果。</p><h2 id="Cascade-Feature-Fusion"><a href="#Cascade-Feature-Fusion" class="headerlink" title="Cascade Feature Fusion"></a>Cascade Feature Fusion</h2><p><img src="/media/15172845289475.jpg" alt=""><br>圖中的Loss是輔助Loss,F1是較低解析的分枝, F2是較高解析的分枝，</p><h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><p>L=λ1L1+λ2L2+λ3L3<br>Loss是對應到每個downsampled score maps使用cross-entropy loss</p><p>依據CFF的設置，下分枝的lossL3的佔比λ3設置為1的話,則中分枝的lossL2的佔比λ2設置為0.4，上分枝的lossL1的佔比λ1設置為0.16</p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><table><thead><tr><th style="text-align:center">項目</th><th style="text-align:center">設置</th></tr></thead><tbody><tr><td style="text-align:center">平台</td><td style="text-align:center">Caffe，CUDA7.5 cudnnV5，TitanX一張</td></tr><tr><td style="text-align:center">測量時間</td><td style="text-align:center">Caffe Time 100次取平均</td></tr><tr><td style="text-align:center">Batch Size</td><td style="text-align:center">16</td></tr><tr><td style="text-align:center">學習速率</td><td style="text-align:center">Poly, Learning Rate 0.01, Momentum 0.9</td></tr><tr><td style="text-align:center">迭代次數</td><td style="text-align:center">30K</td></tr><tr><td style="text-align:center">權重衰減</td><td style="text-align:center">0.0001</td></tr><tr><td style="text-align:center">數據增強</td><td style="text-align:center">Random flip, 0.5 to 2 random scale</td></tr><tr><td style="text-align:center">資料集</td><td style="text-align:center">Cityscapes</td></tr></tbody></table><h4 id="model-Compression"><a href="#model-Compression" class="headerlink" title="model Compression"></a>model Compression</h4><p>以PSPNet50為例，直接壓縮結果如下表Baseline：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15173712115585.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>mIoU降低了，但時間170ms達不到realtime。這表明只有模型壓縮是達不到有良好分割結果的實時性能。對比ICNet，有類似的分割結果，但速度提升了5倍多。</p><h4 id="Cascade-Structure-Experiment"><a href="#Cascade-Structure-Experiment" class="headerlink" title="Cascade Structure Experiment"></a>Cascade Structure Experiment</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15173713226717.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>sub4代表只有低解析度輸入的結果，sub24代表前兩個分枝，sub124全部分枝。注意到全部分枝的速度很快，並且性能接近PSPNet了，且能保持30fps。而且Ram消耗也明顯減少了。</p><h4 id="Visualization"><a href="#Visualization" class="headerlink" title="Visualization"></a>Visualization</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15173715003744.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h4 id="Cityscape-Comparison"><a href="#Cityscape-Comparison" class="headerlink" title="Cityscape Comparison"></a>Cityscape Comparison</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15173715775469.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Recap&quot;&gt;&lt;a href=&quot;#Recap&quot; class=&quot;headerlink&quot; title=&quot;Recap&quot;&gt;&lt;/a&gt;Recap&lt;/h2&gt;&lt;p&gt;Quote:&lt;br&gt;其實很討厭這作者的paper&lt;br&gt;效果都很好, 但是每次都是用matlab, 而且PSPNet
      
    
    </summary>
    
      <category term="Segmentation" scheme="https://world4jason.github.io/categories/Segmentation/"/>
    
    
      <category term="Deep Learning" scheme="https://world4jason.github.io/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="https://world4jason.github.io/tags/Computer-Vision/"/>
    
      <category term="Segmentation" scheme="https://world4jason.github.io/tags/Segmentation/"/>
    
      <category term="real-time" scheme="https://world4jason.github.io/tags/real-time/"/>
    
  </entry>
  
  <entry>
    <title>機器學習中的相似性度量</title>
    <link href="https://world4jason.github.io/2018/01/14/Distance/"/>
    <id>https://world4jason.github.io/2018/01/14/Distance/</id>
    <published>2018-01-13T18:12:19.000Z</published>
    <updated>2018-01-17T12:23:55.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Euclidean-Distance-歐幾里和距離"><a href="#Euclidean-Distance-歐幾里和距離" class="headerlink" title="Euclidean Distance(歐幾里和距離)"></a>Euclidean Distance(歐幾里和距離)</h2><p>二維與三維中就是兩點之間的距離。</p><p><img src="/media/15158672262601.jpg" alt=""></p><p>上圖中的綠線為歐氏距離，又稱歐幾里和距離(Euclidean Distance)，其餘的藍色與黃色還有紅色皆為曼哈頓距離。</p><h2 id="Manhattan-distance-曼哈頓距離"><a href="#Manhattan-distance-曼哈頓距離" class="headerlink" title="Manhattan distance(曼哈頓距離)"></a>Manhattan distance(曼哈頓距離)</h2><p>平面上，坐標（x1, y1）的點P1與坐標（x2, y2）的點P2的曼哈頓距離為：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15158673215658.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>L1-距離</p><h2 id="Mahalanobis-distance-馬氏距離"><a href="#Mahalanobis-distance-馬氏距離" class="headerlink" title="Mahalanobis distance(馬氏距離)"></a>Mahalanobis distance(馬氏距離)</h2><p><a href="https://zh.wikipedia.org/wiki/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5" target="_blank" rel="external">Covariance Matrix</a></p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Mahalanobis distance can be defined <span class="keyword">as</span> <span class="keyword">a</span> dissimilarity measure between <span class="literal">two</span> <span class="built_in">random</span> vectors x <span class="keyword">and</span> y <span class="keyword">of</span> <span class="keyword">the</span> same distribution <span class="keyword">with</span> <span class="keyword">the</span> covariance matrix S:</div></pre></td></tr></table></figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15158695072163.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15158699156018.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h2 id="Chebyshev-distance-切比雪夫距離"><a href="#Chebyshev-distance-切比雪夫距離" class="headerlink" title="Chebyshev distance(切比雪夫距離)"></a>Chebyshev distance(切比雪夫距離)</h2><h2 id="Minkowski-distance-明可夫斯基距離"><a href="#Minkowski-distance-明可夫斯基距離" class="headerlink" title="Minkowski distance (明可夫斯基距離)"></a>Minkowski distance (明可夫斯基距離)</h2><p><a href="http://www.cnblogs.com/heaad/archive/2011/03/08/1977733.html" target="_blank" rel="external">http://www.cnblogs.com/heaad/archive/2011/03/08/1977733.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Euclidean-Distance-歐幾里和距離&quot;&gt;&lt;a href=&quot;#Euclidean-Distance-歐幾里和距離&quot; class=&quot;headerlink&quot; title=&quot;Euclidean Distance(歐幾里和距離)&quot;&gt;&lt;/a&gt;Euclidean 
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Deep Sort: Simple Online and Realtime Tracking with a Deep Association Metric</title>
    <link href="https://world4jason.github.io/2018/01/14/Deep-Sort/"/>
    <id>https://world4jason.github.io/2018/01/14/Deep-Sort/</id>
    <published>2018-01-13T17:38:43.000Z</published>
    <updated>2018-01-13T18:12:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>此篇文章是基於SORT的改進<br>具體來說就是一句話</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">We adopt <span class="keyword">a</span> conventional single hypothesis tracking methodology <span class="keyword">with</span> recursive kalman filtering <span class="keyword">and</span> frame-<span class="keyword">by</span>-frame data association.</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;此篇文章是基於SORT的改進&lt;br&gt;具體來說就是一句話&lt;/p&gt;
&lt;figure class=&quot;highlight livecodeserver&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pr
      
    
    </summary>
    
      <category term="MOT" scheme="https://world4jason.github.io/categories/MOT/"/>
    
    
      <category term="Deep Learning" scheme="https://world4jason.github.io/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="https://world4jason.github.io/tags/Computer-Vision/"/>
    
      <category term="Tracking" scheme="https://world4jason.github.io/tags/Tracking/"/>
    
      <category term="MOT" scheme="https://world4jason.github.io/tags/MOT/"/>
    
  </entry>
  
  <entry>
    <title>Multiple Object Tracking Summary</title>
    <link href="https://world4jason.github.io/2018/01/10/Multiple-Object-Tracking-Summary/"/>
    <id>https://world4jason.github.io/2018/01/10/Multiple-Object-Tracking-Summary/</id>
    <published>2018-01-10T12:03:49.000Z</published>
    <updated>2018-01-13T17:43:34.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://perception.yale.edu/Brian/refGuides/MOT.html" target="_blank" rel="external">http://perception.yale.edu/Brian/refGuides/MOT.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;http://perception.yale.edu/Brian/refGuides/MOT.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://perception.yale.edu/Brian/refGuides/M
      
    
    </summary>
    
      <category term="MOT" scheme="https://world4jason.github.io/categories/MOT/"/>
    
    
      <category term="Computer Vision" scheme="https://world4jason.github.io/tags/Computer-Vision/"/>
    
      <category term="Tracking" scheme="https://world4jason.github.io/tags/Tracking/"/>
    
      <category term="MOT" scheme="https://world4jason.github.io/tags/MOT/"/>
    
      <category term="Summary" scheme="https://world4jason.github.io/tags/Summary/"/>
    
  </entry>
  
  <entry>
    <title>Deep Learning and Computer Vision Recommended Paper</title>
    <link href="https://world4jason.github.io/2018/01/09/Deep-Learning-Recommended-Papers/"/>
    <id>https://world4jason.github.io/2018/01/09/Deep-Learning-Recommended-Papers/</id>
    <published>2018-01-09T13:50:49.000Z</published>
    <updated>2018-01-13T18:39:45.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Image-Classification"><a href="#Image-Classification" class="headerlink" title="Image Classification"></a>Image Classification</h2><h4 id="Must-Read-LeNet-AlexNet-VGG-16-GoogleNet-ResNet"><a href="#Must-Read-LeNet-AlexNet-VGG-16-GoogleNet-ResNet" class="headerlink" title="Must Read : LeNet, AlexNet, VGG-16, GoogleNet, ResNet"></a>Must Read : LeNet, AlexNet, VGG-16, GoogleNet, ResNet</h4><table><thead><tr><th style="text-align:center">Title</th><th style="text-align:center">Authors</th><th style="text-align:center">Pub.</th><th style="text-align:center">Links</th><th style="text-align:center">Figure</th></tr></thead><tbody><tr><td style="text-align:center">LeNet-5, convolutional neural networks</td><td style="text-align:center">Y. LeCun</td><td style="text-align:center">??? 199X</td><td style="text-align:center"><a href="http://yann.lecun.com/exdb/lenet/" target="_blank" rel="external">Web</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/LeNet.png" alt="LeNet" title="">                </div>                <div class="image-caption">LeNet</div>            </figure></td></tr><tr><td style="text-align:center">ImageNet Classification with Deep Convolutional Neural Networks</td><td style="text-align:center">Alex Krizhevsky,Ilya Sutskever,Geoffrey E. Hinton</td><td style="text-align:center">NIPS 2014</td><td style="text-align:center"><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="external">paper</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/AlexNet.jpg" alt="AlexNet" title="">                </div>                <div class="image-caption">AlexNet</div>            </figure></td></tr><tr><td style="text-align:center">Very Deep Convolutional Networks for Large-Scale Image Recognition</td><td style="text-align:center">Karen Simonyan, Andrew Zisserman</td><td style="text-align:center">ICLR 2014</td><td style="text-align:center"><a href="https://arxiv.org/pdf/1409.1556.pdf" target="_blank" rel="external">paper</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/VGG16.png" alt="VGG16" title="">                </div>                <div class="image-caption">VGG16</div>            </figure></td></tr><tr><td style="text-align:center">Going Deeper with Convolutions</td><td style="text-align:center">Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed</td><td style="text-align:center">CVPR 2015</td><td style="text-align:center"><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html" target="_blank" rel="external">paper</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/GoogLeNet.png" alt="GoogLeNet" title="">                </div>                <div class="image-caption">GoogLeNet</div>            </figure></td></tr><tr><td style="text-align:center">Deep Residual Learning for Image Recognition</td><td style="text-align:center">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun</td><td style="text-align:center">CVPR 2016 <em><code>best</code></em></td><td style="text-align:center"><a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="external">paper</a> <a href="https://github.com/KaimingHe/deep-residual-networks" target="_blank" rel="external">github</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/ResNet.png" alt="ResNet" title="">                </div>                <div class="image-caption">ResNet</div>            </figure></td></tr><tr><td style="text-align:center">Residual Attention Network for Image Classification</td><td style="text-align:center">Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, Xiaoou Tang</td><td style="text-align:center">CVPR 2017</td><td style="text-align:center"><a href="https://arxiv.org/abs/1704.06904" target="_blank" rel="external">paper</a> <a href="https://github.com/buptwangfei/residual-attention-network" target="_blank" rel="external">github</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/Res-Attention-Network.png" alt="Res-Attention-Network" title="">                </div>                <div class="image-caption">Res-Attention-Network</div>            </figure></td></tr><tr><td style="text-align:center">Aggregated Residual Transformations for Deep Neural Networks</td><td style="text-align:center">Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He</td><td style="text-align:center">CVPR 2017</td><td style="text-align:center"><a href="https://arxiv.org/abs/1611.05431" target="_blank" rel="external">paper</a> <a href="https://github.com/facebookresearch/ResNeXt" target="_blank" rel="external">github</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/ResNeXt.png" alt="ResNeXt" title="">                </div>                <div class="image-caption">ResNeXt</div>            </figure></td></tr><tr><td style="text-align:center">Densely Connected Convolutional Networks</td><td style="text-align:center">Gao Huang, Zhuang Liu, Kilian Q. Weinberger</td><td style="text-align:center">CVPR 2017 <em><code>best</code></em></td><td style="text-align:center"><a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="external">paper</a> <a href="https://github.com/liuzhuang13/DenseNet" target="_blank" rel="external">github</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/DenseNet.png" alt="DenseNet" title="">                </div>                <div class="image-caption">DenseNet</div>            </figure></td></tr><tr><td style="text-align:center">Deep Pyramidal Residual Networks</td><td style="text-align:center">Dongyoon Han, Jiwhan Kim, Junmo Kim</td><td style="text-align:center">CVPR 2017</td><td style="text-align:center"><a href="https://arxiv.org/pdf/1610.02915.pdf" target="_blank" rel="external">paper</a> <a href="https://github.com/jhkim89/PyramidNet" target="_blank" rel="external">github</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/PyramidNet.png" alt="PyramidNet" title="">                </div>                <div class="image-caption">PyramidNet</div>            </figure></td></tr></tbody></table><h2 id="Object-Detection"><a href="#Object-Detection" class="headerlink" title="Object Detection"></a>Object Detection</h2><h4 id="Must-Read-R-CNN-Fast-R-CNN-Faster-R-CNN-YOLO-SSD"><a href="#Must-Read-R-CNN-Fast-R-CNN-Faster-R-CNN-YOLO-SSD" class="headerlink" title="Must Read : R-CNN, Fast R-CNN, Faster R-CNN, YOLO, SSD"></a>Must Read : R-CNN, Fast R-CNN, Faster R-CNN, YOLO, SSD</h4><table><thead><tr><th style="text-align:center">Title</th><th style="text-align:center">Authors</th><th style="text-align:center">Pub.</th><th style="text-align:center">Links</th><th style="text-align:center">Figure</th></tr></thead><tbody><tr><td style="text-align:center">Rich feature hierarchies for accurate object detection and semantic segmentation</td><td style="text-align:center">Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik</td><td style="text-align:center">CVPR 2014</td><td style="text-align:center"><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf" target="_blank" rel="external">paper</a> <a href="https://github.com/rbgirshick/rcnn" target="_blank" rel="external">github</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/R-CNN.png" alt="R-CNN" title="">                </div>                <div class="image-caption">R-CNN</div>            </figure></td></tr><tr><td style="text-align:center">Fast R-CNN</td><td style="text-align:center">Ross Girshick</td><td style="text-align:center">ICCV 2015</td><td style="text-align:center"><a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf" target="_blank" rel="external">paper</a> <a href="https://github.com/rbgirshick/fast-rcnn" target="_blank" rel="external">github</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/Fast-R-CNN.png" alt="Fast-R-CNN" title="">                </div>                <div class="image-caption">Fast-R-CNN</div>            </figure></td></tr><tr><td style="text-align:center">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</td><td style="text-align:center">Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun</td><td style="text-align:center">TPAMI 2015</td><td style="text-align:center"><a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf" target="_blank" rel="external">paper</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/SPP-Net.png" alt="SPP Net" title="">                </div>                <div class="image-caption">SPP Net</div>            </figure></td></tr><tr><td style="text-align:center">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</td><td style="text-align:center">Shaoqing Ren, <a href="http://kaiminghe.com/" target="_blank" rel="external">Kaiming He</a>, Ross Girshick, Jian Sun</td><td style="text-align:center">NIPS 2015</td><td style="text-align:center"><a href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf" target="_blank" rel="external">paper</a> <a href="https://github.com/ShaoqingRen/faster_rcnn" target="_blank" rel="external"><code>matlab</code></a> <a href="https://github.com/rbgirshick/py-faster-rcnn" target="_blank" rel="external"><code>python</code></a> <a href="https://github.com/longcw/faster_rcnn_pytorch" target="_blank" rel="external"><code>pytorch</code></a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/Faster-R-CNN.png" alt="Faster-R-CNN" title="">                </div>                <div class="image-caption">Faster-R-CNN</div>            </figure></td></tr><tr><td style="text-align:center">You Only Look Once: Unified, Real-Time Object Detection</td><td style="text-align:center">Joseph Redmon,Santosh Divvala,Ross Girshick, Ali Farhadi</td><td style="text-align:center">CVPR 2016</td><td style="text-align:center"><a href="http://arxiv.org/pdf/1506.02640.pdf" target="_blank" rel="external">paper</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/YOLO.jpg" alt="YOLO" title="">                </div>                <div class="image-caption">YOLO</div>            </figure></td></tr><tr><td style="text-align:center">SSD: Single Shot MultiBox Detector</td><td style="text-align:center">Wei Liu1, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg</td><td style="text-align:center">CVPR 2016</td><td style="text-align:center"><a href="http://arxiv.org/pdf/1512.02325.pdf" target="_blank" rel="external">paper</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/SSD.png" alt="SSD" title="">                </div>                <div class="image-caption">SSD</div>            </figure></td></tr><tr><td style="text-align:center">Convolutional Feature Masking for Joint Object and Stuff Segmentation</td><td style="text-align:center">Jifeng Dai, <a href="http://kaiminghe.com/" target="_blank" rel="external">Kaiming He</a>, Jian Sun</td><td style="text-align:center">CVPR 2015</td><td style="text-align:center"><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Dai_Convolutional_Feature_Masking_2015_CVPR_paper.pdf" target="_blank" rel="external">paper</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/CFM.png" alt="CFM" title="">                </div>                <div class="image-caption">CFM</div>            </figure></td></tr><tr><td style="text-align:center">Instance-aware Semantic Segmentation via Multi-task Network Cascades</td><td style="text-align:center">Jifeng Dai, <a href="http://kaiminghe.com/" target="_blank" rel="external">Kaiming He</a>, Jian Sun</td><td style="text-align:center">CVPR 2016</td><td style="text-align:center"><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Dai_Instance-Aware_Semantic_Segmentation_CVPR_2016_paper.pdf" target="_blank" rel="external">paper</a> <a href="https://github.com/daijifeng001/MNC" target="_blank" rel="external">github</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/MNC.png" alt="MNC" title="">                </div>                <div class="image-caption">MNC</div>            </figure></td></tr><tr><td style="text-align:center">R-FCN: Object Detection via Region-based Fully Convolutional Networks</td><td style="text-align:center">Jifeng Dai, Yi Li, <a href="http://kaiminghe.com/" target="_blank" rel="external">Kaiming He</a>, Jian Sun</td><td style="text-align:center">NIPS 2016</td><td style="text-align:center"><a href="https://arxiv.org/abs/1605.06409" target="_blank" rel="external">paper</a> <a href="https://github.com/daijifeng001/R-FCN" target="_blank" rel="external">github</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/Region-FCN.png" alt="Region-FCN" title="">                </div>                <div class="image-caption">Region-FCN</div>            </figure></td></tr><tr><td style="text-align:center">Feature Pyramid Networks for Object Detection</td><td style="text-align:center">Tsung-Yi Lin, Piotr Dollár, Ross Girshick, <a href="http://kaiminghe.com/" target="_blank" rel="external">Kaiming He</a>, Bharath Hariharan, and Serge Belongie</td><td style="text-align:center">CVPR 2017</td><td style="text-align:center"><a href="https://arxiv.org/pdf/1612.03144.pdf" target="_blank" rel="external">paper</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/FPN.png" alt="FPN" title="">                </div>                <div class="image-caption">FPN</div>            </figure></td></tr><tr><td style="text-align:center">Mask R-CNN</td><td style="text-align:center">Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick</td><td style="text-align:center">ICCV 2017</td><td style="text-align:center"><a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="external">paper</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/Mask-R-CNN.png" alt="Mask-R-CNN" title="">                </div>                <div class="image-caption">Mask-R-CNN</div>            </figure></td></tr><tr><td style="text-align:center">A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection</td><td style="text-align:center">Xiaolong Wang, Abhinav Shrivastava, Abhinav Gupta</td><td style="text-align:center">CVPR 2017</td><td style="text-align:center"><a href="https://arxiv.org/abs/1704.03414" target="_blank" rel="external">paper</a>  <a href="https://github.com/xiaolonw/adversarial-frcnn" target="_blank" rel="external">github</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/A-Fast-R-CNN.png" alt="A-Fast-R-CNN" title="">                </div>                <div class="image-caption">A-Fast-R-CNN</div>            </figure></td></tr><tr><td style="text-align:center">Multiple Instance Detection Network with Online Instance Classifier Refinement</td><td style="text-align:center">Peng Tang, Xinggang Wang, Xiang Bai, Wenyu Liu</td><td style="text-align:center">CVPR 2017</td><td style="text-align:center"><a href="https://arxiv.org/abs/1704.00138" target="_blank" rel="external">paper</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/MIDN.png" alt="MIDN" title="">                </div>                <div class="image-caption">MIDN</div>            </figure></td></tr><tr><td style="text-align:center">R-FCN-3000 at 30fps: Decoupling Detection and Classification</td><td style="text-align:center">Bharat Singh, Hengdou Li, Abhishek Sharma and Larry S. Davis</td><td style="text-align:center">Tech Report</td><td style="text-align:center"><a href="https://arxiv.org/abs/1712.01802" target="_blank" rel="external">paper</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/R-FCN-3000.png" alt="R-FCN-3000" title="">                </div>                <div class="image-caption">R-FCN-3000</div>            </figure></td></tr></tbody></table><h2 id="Semantic-Segmentation-and-Scene-Parsing"><a href="#Semantic-Segmentation-and-Scene-Parsing" class="headerlink" title="Semantic Segmentation and Scene Parsing"></a>Semantic Segmentation and Scene Parsing</h2><h4 id="Must-Read-FCN-Learning-Deconvolution-Network-for-Semantic-Segmentation-U-Net"><a href="#Must-Read-FCN-Learning-Deconvolution-Network-for-Semantic-Segmentation-U-Net" class="headerlink" title="Must Read : FCN, Learning Deconvolution Network for Semantic Segmentation, U-Net"></a>Must Read : FCN, Learning Deconvolution Network for Semantic Segmentation, U-Net</h4><table><thead><tr><th>Title</th><th style="text-align:center">Authors</th><th style="text-align:center">Pub.</th><th style="text-align:center">Links</th><th style="text-align:center">Figure</th></tr></thead><tbody><tr><td>Fully Convolutional Networks for Semantic Segmentation</td><td style="text-align:center">Jonathan Long, Evan Shelhamer, Trevor Darrell</td><td style="text-align:center">CVPR 2015</td><td style="text-align:center"><a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf" target="_blank" rel="external">paper</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/FCN.png" alt="FCN" title="">                </div>                <div class="image-caption">FCN</div>            </figure></td></tr><tr><td>Learning to Segment Object Candidates</td><td style="text-align:center">Pedro O. Pinheiro, Ronan Collobert, Piotr Dollar</td><td style="text-align:center">NIPS 2015</td><td style="text-align:center"><a href="http://papers.nips.cc/paper/5852-learning-to-segment-object-candidates.pdf" target="_blank" rel="external">paper</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/LSOC.png" alt="LSOC" title="">                </div>                <div class="image-caption">LSOC</div>            </figure></td></tr><tr><td>Learning to Refine Object Segments</td><td style="text-align:center">Pedro O. Pinheiro , Tsung-Yi Lin , Ronan Collobert, Piotr Doll ́ar</td><td style="text-align:center">arXiv 1603.08695</td><td style="text-align:center"><a href="https://arxiv.org/pdf/1603.08695.pdf" target="_blank" rel="external">paper</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/LROS.png" alt="LROS" title="">                </div>                <div class="image-caption">LROS</div>            </figure></td></tr><tr><td>Conditional Random Fields as Recurrent Neural Networks</td><td style="text-align:center">Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, ZhiZhong Su, Dalong Du, Chang Huang, and Philip H. S. Torr</td><td style="text-align:center">ICCV 2015</td><td style="text-align:center"><a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Zheng_Conditional_Random_Fields_ICCV_2015_paper.html" target="_blank" rel="external">paper</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/CRFRNN.png" alt="CRFRNN" title="">                </div>                <div class="image-caption">CRFRNN</div>            </figure></td></tr><tr><td>Learning Deconvolution Network for Semantic Segmentation</td><td style="text-align:center">Heonwoo Noh, Seunghoon Hong, Bohyung Han</td><td style="text-align:center">ICCV 2015</td><td style="text-align:center"><a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Noh_Learning_Deconvolution_Network_ICCV_2015_paper.html" target="_blank" rel="external">paper</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/LDN.png" alt="LDN" title="">                </div>                <div class="image-caption">LDN</div>            </figure></td></tr><tr><td>U-Net: Convolutional Networks for Biomedical Image Segmentation</td><td style="text-align:center">Olaf Ronneberger, Philipp Fischer, Thomas Brox</td><td style="text-align:center">MICCAI 2015</td><td style="text-align:center"><a href="https://arxiv.org/pdf/1505.04597.pdf" target="_blank" rel="external">paper</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/U-Net.png" alt="U-Net" title="">                </div>                <div class="image-caption">U-Net</div>            </figure></td></tr><tr><td>Instance-sensitive Fully Convolutional Networks</td><td style="text-align:center">Jifeng Dai, Kaiming He, Yi Li, Shaoqing Ren, Jian Sun</td><td style="text-align:center">ECCV 2016</td><td style="text-align:center"><a href="https://arxiv.org/abs/1603.08678" target="_blank" rel="external">paper</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/ISFCN.png" alt="ISFCN" title="">                </div>                <div class="image-caption">ISFCN</div>            </figure></td></tr><tr><td>Laplacian Pyramid Reconstruction and Refinement for Semantic Segmentation</td><td style="text-align:center">Golnaz Ghiasi, Charless C. Fowlkes</td><td style="text-align:center">ECCV 2016</td><td style="text-align:center"><a href="https://link.springer.com/chapter/10.1007/978-3-319-46487-9_32" target="_blank" rel="external">paper</a>  <a href="https://github.com/golnazghiasi/LRR" target="_blank" rel="external">github</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/LPRR.png" alt="LPRR" title="">                </div>                <div class="image-caption">LPRR</div>            </figure></td></tr><tr><td>Attention to Scale: Scale-aware Semantic Image Segmentation</td><td style="text-align:center">Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu</td><td style="text-align:center">CVPR 2016</td><td style="text-align:center"><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Chen_Attention_to_Scale_CVPR_2016_paper.html" target="_blank" rel="external">paper</a> <a href="http://liangchiehchen.com/projects/DeepLab.html" target="_blank" rel="external"><code>DeepLab</code></a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/Attention-to-scale.png" alt="Attention-to-scale" title="">                </div>                <div class="image-caption">Attention-to-scale</div>            </figure></td></tr><tr><td>RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation</td><td style="text-align:center">Guosheng Lin, Anton Milan, Chunhua Shen, Ian Reid</td><td style="text-align:center">CVPR 2017</td><td style="text-align:center"><a href="https://arxiv.org/abs/1611.06612" target="_blank" rel="external">paper</a>  <a href="https://github.com/guosheng/refinenet" target="_blank" rel="external">github</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/RefineNet.png" alt="RefineNet" title="">                </div>                <div class="image-caption">RefineNet</div>            </figure></td></tr><tr><td></td></tr><tr><td>Pyramid Scene Parsing Network</td><td style="text-align:center">Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia</td><td style="text-align:center">CVPR 2017</td><td style="text-align:center"><a href="https://arxiv.org/abs/1612.01105" target="_blank" rel="external">paper</a>  <a href="https://github.com/hszhao/PSPNet" target="_blank" rel="external">github</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/PSPNet.png" alt="PSPNet" title="">                </div>                <div class="image-caption">PSPNet</div>            </figure></td></tr><tr><td>ICNet for Real-Time Semantic Segmentation on High-Resolution Images</td><td style="text-align:center">Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, Jiaya Jia</td><td style="text-align:center">Tech Report</td><td style="text-align:center"><a href="https://arxiv.org/pdf/1704.08545.pdf" target="_blank" rel="external">paper</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/ICNet.png" alt="ICNet" title="">                </div>                <div class="image-caption">ICNet</div>            </figure></td></tr><tr><td>Dilated Residual Networks</td><td style="text-align:center">Fisher Yu, Vladlen Koltun, Thomas Funkhouser</td><td style="text-align:center">CVPR 2017</td><td style="text-align:center"><a href="https://arxiv.org/abs/1705.09914" target="_blank" rel="external">paper</a> <a href="https://github.com/fyu/drn" target="_blank" rel="external">github</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/DRN.png" alt="DRN" title="">                </div>                <div class="image-caption">DRN</div>            </figure></td></tr><tr><td>Fully Convolutional Instance-aware Semantic Segmentation</td><td style="text-align:center">Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, Yichen Wei</td><td style="text-align:center">CVPR 2017</td><td style="text-align:center"><a href="https://arxiv.org/abs/1611.07709" target="_blank" rel="external">paper</a> <a href="https://github.com/msracver/FCIS" target="_blank" rel="external">github</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/FCIS.png" alt="FCIS" title="">                </div>                <div class="image-caption">FCIS</div>            </figure></td></tr><tr><td>Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes</td><td style="text-align:center">Tobias Pohlen, Alexander Hermans, Markus Mathias, Bastian Leibe</td><td style="text-align:center">CVPR 2017</td><td style="text-align:center"><a href="https://arxiv.org/abs/1611.08323" target="_blank" rel="external">paper</a> <a href="https://github.com/TobyPDE/FRRN" target="_blank" rel="external">github</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/FRRN.png" alt="FRRN" title="">                </div>                <div class="image-caption">FRRN</div>            </figure></td></tr><tr><td>Object Region Mining with Adversarial Erasing: A Simple Classification toSemantic Segmentation Approach</td><td style="text-align:center">Yunchao Wei, Jiashi Feng, Xiaodan Liang, Ming-Ming Cheng, Yao Zhao, Shuicheng Yan</td><td style="text-align:center">CVPR 2017</td><td style="text-align:center"><a href="https://arxiv.org/abs/1703.08448" target="_blank" rel="external">paper</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/A-Erasing.png" alt="A-Erasing" title="">                </div>                <div class="image-caption">A-Erasing</div>            </figure></td></tr><tr><td>Not All Pixels Are Equal: Difficulty-Aware Semantic Segmentation via Deep Layer Cascade</td><td style="text-align:center">Xiaoxiao Li, Ziwei Liu, Ping Luo, Chen Change Loy, Xiaoou Tang</td><td style="text-align:center">CVPR 2017</td><td style="text-align:center"><a href="https://arxiv.org/pdf/1704.01344.pdf" target="_blank" rel="external">paper</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/Not-All-Pixels-Are-Equal.png" alt="Not-All-Pixels-Are-Equal" title="">                </div>                <div class="image-caption">Not-All-Pixels-Are-Equal</div>            </figure></td></tr><tr><td>Semantic Segmentation with Reverse Attention</td><td style="text-align:center">Qin Huang, Chunyang Xia, Wuchi Hao, Siyang Li, Ye Wang, Yuhang Song and C.-C. Jay Kuo</td><td style="text-align:center">BMVC 2017</td><td style="text-align:center"><a href="https://arxiv.org/abs/1707.06426" target="_blank" rel="external">paper</a> <a href="https://drive.google.com/drive/folders/0By2w_AaM8Rzbllnc3JCQjhHYnM?usp=sharing" target="_blank" rel="external"><code>code</code></a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/Rev-Attention.png" alt="Rev-Attention" title="">                </div>                <div class="image-caption">Rev-Attention</div>            </figure></td></tr><tr><td>Predicting Deeper into the Future of Semantic Segmentation</td><td style="text-align:center">Pauline Luc, Natalia Neverova, Camille Couprie, Jakob Verbeek and Yann LeCun</td><td style="text-align:center">ICCV 2017</td><td style="text-align:center"><a href="https://arxiv.org/abs/1703.07684" target="_blank" rel="external">paper</a> <a href="https://thoth.inrialpes.fr/people/pluc/iccv2017" target="_blank" rel="external"><code>project page</code></a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/Deeper-into-Future.png" alt="Deeper-into-Future" title="">                </div>                <div class="image-caption">Deeper-into-Future</div>            </figure></td></tr><tr><td>Learning to Segment Every Thing</td><td style="text-align:center">Ronghang Hu, Piotr Dollar, Kaiming He, Trevor Darrell, Ross Girshick</td><td style="text-align:center">Tech Report</td><td style="text-align:center"><a href="https://arxiv.org/abs/1711.10370" target="_blank" rel="external">paper</a></td><td style="text-align:center"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/data/Seg-Everything.png" alt="Seg-Everything" title="">                </div>                <div class="image-caption">Seg-Everything</div>            </figure></td></tr></tbody></table><h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><ul><li>Dropout- A Simple Way to Prevent Neural Networks from Overfitting</li><li>Batch Normalization- Accelerating Deep Network Training by Reducing Internal Covariate Shift</li></ul><h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><ul><li>Generating Sequences With Recurrent Neural Networks</li><li>Word embedding</li><li>Distributed Representations of Words and Phrases and their Compositionality</li></ul><h2 id="Image-captioning"><a href="#Image-captioning" class="headerlink" title="Image captioning"></a>Image captioning</h2><p>Show and Tell: A Neural Image Caption Generator<br>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Image-Classification&quot;&gt;&lt;a href=&quot;#Image-Classification&quot; class=&quot;headerlink&quot; title=&quot;Image Classification&quot;&gt;&lt;/a&gt;Image Classification&lt;/h2&gt;&lt;
      
    
    </summary>
    
      <category term="Deep Learning" scheme="https://world4jason.github.io/categories/Deep-Learning/"/>
    
    
      <category term="Deep Learning" scheme="https://world4jason.github.io/tags/Deep-Learning/"/>
    
      <category term="Paper List" scheme="https://world4jason.github.io/tags/Paper-List/"/>
    
      <category term="Self-Study" scheme="https://world4jason.github.io/tags/Self-Study/"/>
    
      <category term="Computer Vision" scheme="https://world4jason.github.io/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>Person re-ID Summary</title>
    <link href="https://world4jason.github.io/2018/01/02/person_re-ID_summary/"/>
    <id>https://world4jason.github.io/2018/01/02/person_re-ID_summary/</id>
    <published>2018-01-02T13:50:49.000Z</published>
    <updated>2018-01-28T09:19:05.000Z</updated>
    
    <content type="html"><![CDATA[<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15149820234945.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h2 id="目標"><a href="#目標" class="headerlink" title="目標"></a>目標</h2><h2 id="難度"><a href="#難度" class="headerlink" title="難度"></a>難度</h2><ol><li>目標遮擋（Occlusion）導致部分特徵丟失</li><li>不同的 View，Illumination 導致同一目標的特徵差異</li><li>不同目標衣服顏色近似、特徵近似導致區分度下降</li></ol><h2 id="解決方案"><a href="#解決方案" class="headerlink" title="解決方案"></a>解決方案</h2><h3 id="1-Representation-learning-ReID"><a href="#1-Representation-learning-ReID" class="headerlink" title="1. Representation learning + ReID"></a>1. Representation learning + ReID</h3><p>看做分類(Classification/Identification)問題或者驗證(Verification)問題：<br>(1) 分類問題是指利用行人的ID或者屬性等作為訓練標籤來訓練模型；<br>(2) 驗證問題是指輸入一對（兩張）行人圖片，讓網絡來學習這兩張圖片是否屬於同一個行人。</p><p>Classification/Identification loss和verification loss</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15150472221676.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>額外改進方向[2]是在加上許多行人的label，像是性別、頭髮以及服裝等等。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15150474384428.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h3 id="2-Metric-learning-ReID"><a href="#2-Metric-learning-ReID" class="headerlink" title="2. Metric learning + ReID"></a>2. Metric learning + ReID</h3><p>常用於圖像檢索的方法，通過網絡學習出兩張圖片的相似度。<br>(Contrastive loss)[5]、三元組損失(Triplet loss)、 四元組損失(Quadruplet loss)、難樣本採樣三元組損失(Triplet hard loss with batch hard mining, TriHard loss)、邊界挖掘損失(Margin sample mining loss, MSML</p><p>Contrastive loss 基本上就是Siamese CNN<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15150477657739.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>訓練時是三個正樣本一個副樣本，test時未知<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15150479915859.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15150480384222.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h3 id="3-Local-Feature-ReID"><a href="#3-Local-Feature-ReID" class="headerlink" title="3. Local Feature + ReID"></a>3. Local Feature + ReID</h3><p>論文[3]用local feature而不用global feature，切割好以後送到LSTM去學<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15150481401848.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>但論文[3]會有對齊問題，所以論文[4]用pose跟skeleton來做姿勢預測，再通過仿射變換對齊</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15150485085441.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>論文[5]直接拿關節點切出ROI，14個人體關節點，得到7個ROI區域，(頭、上身、下身和四肢)<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15150485782017.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h3 id="4-Video-Sequence-ReID"><a href="#4-Video-Sequence-ReID" class="headerlink" title="4. Video Sequence + ReID"></a>4. Video Sequence + ReID</h3><p>這方向不熟 貼兩張圖參考參考而已<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15150487793131.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15150488009610.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h3 id="5-GAN-ReID"><a href="#5-GAN-ReID" class="headerlink" title="5. GAN + ReID"></a>5. GAN + ReID</h3><p>ReID數據集目前最大的也只有幾千個ID，跟萬張圖片而已，CNN based還容易overfitting<br>GAN主要是用在遷移學習跟基於條件的生成</p><p>第一篇就是ICCV2017的論文[5]以及後來同作者改進的論文[6]，是可以避免overfitting但生成效果就很慘<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15150491105798.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>為了處理不同數據集，甚至是不同camera所造成bias的問題，論文[7]是利用cycleGAN based的設計，利用遷移學習來處理兩個數同數據集的問題，先切割分前景跟背景，在轉換過去。<br>D有兩個loss(還是有兩個D不確定，paper內沒架構圖)一個是前景的絕對誤差loss，一個是正常的判別器loss。判別器loss是用來判斷生成的圖屬於哪個domain，前景的loss是為了保證行人前景儘可能逼真不變。mask用PSPnet來找的。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15150492644360.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>Pose Normalization[8]<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15150490021821.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15150490274695.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15150495642066.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h2 id="資料種類"><a href="#資料種類" class="headerlink" title="資料種類"></a>資料種類</h2><ul><li>Video-based</li><li>Image-based</li><li>Long-term activity</li><li>Individual action </li></ul><h2 id="資料庫"><a href="#資料庫" class="headerlink" title="資料庫"></a>資料庫</h2><p><a href="http://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/projectpages/reiddataset.html" target="_blank" rel="external">Robust Systems Lab</a></p><h2 id="程式碼"><a href="#程式碼" class="headerlink" title="程式碼"></a>程式碼</h2><p><a href="https://zhuanlan.zhihu.com/p/32585203" target="_blank" rel="external">简单行人重识别代码到88%准确率</a><br><a href="https://github.com/layumi/Person_reID_baseline_pytorch" target="_blank" rel="external">https://github.com/layumi/Person_reID_baseline_pytorch</a></p><ul><li><h3 id="ICCV-2017"><a href="#ICCV-2017" class="headerlink" title="ICCV 2017"></a>ICCV 2017</h3><ul><li><a href="https://github.com/KovenYu/CAMEL" target="_blank" rel="external">Cross-view Asymmetric Metric Learning for Unsupervised Re-id </a></li><li><a href="https://github.com/zlmzju/part_reid" target="_blank" rel="external">Deeply-Learned Part-Aligned Representations for Person Re-Identification </a></li><li><a href="https://github.com/VisualComputingInstitute/triplet-reid" target="_blank" rel="external">In Defense of the Triplet Loss for Person Re-Identification </a></li><li><a href="https://github.com/shuangjiexu/Spatial-Temporal-Pooling-Networks-ReID" target="_blank" rel="external">Jointly Attentive Spatial-Temporal Pooling Networks for Video-based Person Re-Identification</a></li><li><a href="http://github.com/syfafterzy/SVDNet-for-Pedestrian-Retrieval" target="_blank" rel="external">SVDNet for Pedestrian Retrieval</a></li><li><a href="http://github.com/layumi/Person-reID_GAN" target="_blank" rel="external">Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro</a></li></ul></li></ul><ul><li><h3 id="CVPR-2017"><a href="#CVPR-2017" class="headerlink" title="CVPR 2017"></a>CVPR 2017</h3><ul><li><a href="http://github.com/yokattame/SpindleNet" target="_blank" rel="external">Spindle Net: Person Re-Identification With Human Body Region Guided Feature Decomposition and Fusion</a></li><li><a href="http://github.com/liangzheng06/PRW-baseline" target="_blank" rel="external">Person Re-Identification in the Wild</a> </li><li><a href="http://github.com/ShuangLI59/person_search" target="_blank" rel="external">Joint Detection and Identification Feature Learning for Person Search</a></li><li><a href="http//github.com/sciencefans/Quality-Aware-Network">Quality Aware Network for Set to Set Recognition</a></li></ul></li></ul><h2 id="Paper-List"><a href="#Paper-List" class="headerlink" title="Paper List"></a>Paper List</h2><pre><code>- Point to Set Similarity Based Deep Feature Learning for Person Re-Identification- Fast Person Re-Identification via Cross-Camera Semantic Binary Transformation- See the Forest for the Trees: Joint Spatial and Temporal Recurrent Neural Networks for Video-Based Person Re-Identification- Learning Deep Context-Aware Features Over Body and Latent Parts for Person Re-Identification- Consistent-Aware Deep Learning for Person Re-Identification in a Camera Network- Re-Ranking Person Re-Identification With k-Reciprocal Encoding- Multiple People Tracking by Lifted Multicut and Person Re-Identification</code></pre><p>[1] Mengyue Geng, Yaowei Wang, Tao Xiang, Yonghong Tian. Deep transfer learning for person reidentification[J]. arXiv preprint arXiv:1611.05244, 2016.</p><p>[2] Yutian Lin, Liang Zheng, Zhedong Zheng, YuWu, Yi Yang. Improving person re-identification by attribute and identity learning[J]. arXiv preprint arXiv:1703.07220, 2017.</p><p>[3] Rahul Rama Varior, Bing Shuai, Jiwen Lu, Dong Xu, Gang Wang. A siamese long short-term memory architecture for human re-identification[C]//European Conference on Computer Vision. Springer, 2016:135–153.</p><p>[4]Liang Zheng, Yujia Huang, Huchuan Lu, Yi Yang. Pose invariant embedding for deep person reidentification[J]. arXiv preprint arXiv:1701.07732, 2017.</p><p>[5]  Haiyu Zhao, Maoqing Tian, Shuyang Sun, Jing Shao, Junjie Yan, Shuai Yi, Xiaogang Wang, Xiaoou Tang. Spindle net: Person re-identification with human body region guided feature decomposition and fusion[C]. CVPR, 2017.</p><p>[6] Zhong Z, Zheng L, Zheng Z, et al. Camera Style Adaptation for Person Re-identification[J]. arXiv preprint arXiv:1711.10295, 2017.</p><p>[7] Wei L, Zhang S, Gao W, et al. Person Transfer GAN to Bridge Domain Gap for Person Re-Identification[J]. arXiv preprint arXiv:1711.08565, 2017.</p><p>[8] Qian X, Fu Y, Wang W, et al. Pose-Normalized Image Generation for Person Re-identification[J]. arXiv preprint arXiv:1712.02225, 2017.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
                    &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt;
                   
      
    
    </summary>
    
      <category term="Re-ID" scheme="https://world4jason.github.io/categories/Re-ID/"/>
    
    
      <category term="Tracking" scheme="https://world4jason.github.io/tags/Tracking/"/>
    
      <category term="image retrieval" scheme="https://world4jason.github.io/tags/image-retrieval/"/>
    
      <category term="survey" scheme="https://world4jason.github.io/tags/survey/"/>
    
  </entry>
  
  <entry>
    <title>PYTHON中如何使用*ARGS和**KWARGS</title>
    <link href="https://world4jason.github.io/2017/12/24/PYTHON%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-ARGS%E5%92%8C-KWARGS/"/>
    <id>https://world4jason.github.io/2017/12/24/PYTHON中如何使用-ARGS和-KWARGS/</id>
    <published>2017-12-24T10:48:14.000Z</published>
    <updated>2018-01-04T07:24:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>範例與翻譯理解自<a href="http://www.saltycrane.com/blog/2008/01/how-to-use-args-and-kwargs-in-python/" target="_blank" rel="external">連結</a>與<a href="http://appsgaga.com/python%E5%AD%B8%E7%BF%92%E7%AD%86%E8%A8%98-init%E6%96%B9%E6%B3%95%E4%B8%AD%EF%BC%8Cargs-kwargs%E9%80%99%E5%85%A9%E5%80%8B%E5%8F%83%E6%95%B8%E6%98%AF%E4%BB%80%E9%BA%BC%EF%BC%9F/" target="_blank" rel="external">連結</a></p><h2 id="args跟-kwargs是類似的東西，是可有可無的參數。-一顆星的-args是tuple，可以接受很多的值。兩顆星的-kwargs一樣是可以接受很多值，但是是接受dictionary。"><a href="#args跟-kwargs是類似的東西，是可有可無的參數。-一顆星的-args是tuple，可以接受很多的值。兩顆星的-kwargs一樣是可以接受很多值，但是是接受dictionary。" class="headerlink" title="*args跟 **kwargs是類似的東西，是可有可無的參數。 一顆星的*args是tuple，可以接受很多的值。兩顆星的**kwargs一樣是可以接受很多值，但是是接受dictionary。"></a>*args跟 **kwargs是類似的東西，是可有可無的參數。 一顆星的*args是tuple，可以接受很多的值。兩顆星的**kwargs一樣是可以接受很多值，但是是接受dictionary。</h2><p>###一顆星用法</p><p>範例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_var_args</span><span class="params">(farg, *args)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">"formal arg:"</span>, farg</div><div class="line">    <span class="keyword">for</span> arg <span class="keyword">in</span> args:</div><div class="line">        <span class="keyword">print</span> <span class="string">"another arg:"</span>, arg</div><div class="line">        </div><div class="line">test_var_args(<span class="number">1</span>, <span class="string">"two"</span>, <span class="number">3</span>)</div></pre></td></tr></table></figure><p>结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">formal arg: <span class="number">1</span></div><div class="line">another arg: two</div><div class="line">another arg: <span class="number">3</span></div></pre></td></tr></table></figure><p>###兩顆星用法</p><p>範例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_var_kwargs</span><span class="params">(farg, **kwargs)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">"formal arg:"</span>, farg</div><div class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> kwargs:</div><div class="line">        <span class="keyword">print</span> <span class="string">"another keyword arg: %s: %s"</span> % (key, kwargs[key])</div><div class="line"></div><div class="line">test_var_kwargs(farg=<span class="number">1</span>, myarg2=<span class="string">"two"</span>, myarg3=<span class="number">3</span>)</div></pre></td></tr></table></figure><p>结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">formal arg: <span class="number">1</span></div><div class="line">another keyword arg: myarg2: two</div><div class="line">another keyword arg: myarg3: <span class="number">3</span></div></pre></td></tr></table></figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15141999823321.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="順序問題"><a href="#順序問題" class="headerlink" title="順序問題"></a>順序問題</h3><p>如果function定義時如上圖先放了tuple才是dictionary，呼叫時參數先放dictionary再放tuple會跳error。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;範例與翻譯理解自&lt;a href=&quot;http://www.saltycrane.com/blog/2008/01/how-to-use-args-and-kwargs-in-python/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;連結&lt;/a&gt;與&lt;a h
      
    
    </summary>
    
      <category term="Code" scheme="https://world4jason.github.io/categories/Code/"/>
    
    
      <category term="code" scheme="https://world4jason.github.io/tags/code/"/>
    
      <category term="python" scheme="https://world4jason.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Python 筆記</title>
    <link href="https://world4jason.github.io/2017/12/13/python-note/"/>
    <id>https://world4jason.github.io/2017/12/13/python-note/</id>
    <published>2017-12-13T07:35:34.000Z</published>
    <updated>2017-12-25T10:49:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>Different in Py2 and Py3</p><p>Pickle module</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">if</span> sys.version_info[<span class="number">0</span>]&lt;<span class="number">3</span>:</div><div class="line">    <span class="keyword">import</span> cPickle <span class="keyword">as</span> pickle</div><div class="line"><span class="keyword">else</span></div><div class="line">    <span class="keyword">import</span> _pickle <span class="keyword">as</span> pickle</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Different in Py2 and Py3&lt;/p&gt;
&lt;p&gt;Pickle module&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1
      
    
    </summary>
    
      <category term="Python" scheme="https://world4jason.github.io/categories/Python/"/>
    
    
      <category term="code" scheme="https://world4jason.github.io/tags/code/"/>
    
  </entry>
  
  <entry>
    <title>zi2zi: Master Chinese Calligraphy with Conditional Adversarial Networks</title>
    <link href="https://world4jason.github.io/2017/12/01/zi2zi/"/>
    <id>https://world4jason.github.io/2017/12/01/zi2zi/</id>
    <published>2017-11-30T19:36:19.000Z</published>
    <updated>2017-12-09T19:46:26.000Z</updated>
    
    <content type="html"><![CDATA[<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15128482902404.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>Generated samples. Related code can be found <a href="https://github.com/kaonashi-tyc/zi2zi" target="_blank" rel="external">here</a></p><h2 id="目標"><a href="#目標" class="headerlink" title="目標"></a>目標</h2><p>字體風格轉換<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15128485151045.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h2 id="動機"><a href="#動機" class="headerlink" title="動機"></a>動機</h2><p>直接用CNN進行風格轉換會有下列問題</p><ol><li>生成常常是模糊的</li><li>多數生成結果是失敗的</li><li>只能做一對一生成</li></ol><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15128485338820.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>結論：用GAN試試看</p><h2 id="用GAN秒殺一切！"><a href="#用GAN秒殺一切！" class="headerlink" title="用GAN秒殺一切！"></a>用GAN秒殺一切！</h2><p>這篇借鑒了三篇paper內容</p><p><a href="https://arxiv.org/abs/1611.07004" target="_blank" rel="external">Image-to-Image Translation with Conditional Adversarial Networks</a><br><a href="https://arxiv.org/abs/1610.09585" target="_blank" rel="external">Conditional Image Synthesis With Auxiliary Classifier GANs</a><br><a href="https://arxiv.org/abs/1611.02200" target="_blank" rel="external">Unsupervised Cross-Domain Image Generation</a></p><p>主要是由pix2pix這篇修改而來的<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15128487302209.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>其中Encoder跟Decoder還有Discriminator是直接用pix2pix的, 尤其是裡面的Unet模型</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
                    &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt;
                   
      
    
    </summary>
    
      <category term="GAN" scheme="https://world4jason.github.io/categories/GAN/"/>
    
    
      <category term="Generation" scheme="https://world4jason.github.io/tags/Generation/"/>
    
      <category term="Generative Model" scheme="https://world4jason.github.io/tags/Generative-Model/"/>
    
      <category term="GAN" scheme="https://world4jason.github.io/tags/GAN/"/>
    
      <category term="Style Transfer" scheme="https://world4jason.github.io/tags/Style-Transfer/"/>
    
  </entry>
  
  <entry>
    <title>Mask R-CNN</title>
    <link href="https://world4jason.github.io/2017/11/10/Mask%20R-CNN/"/>
    <id>https://world4jason.github.io/2017/11/10/Mask R-CNN/</id>
    <published>2017-11-09T18:13:59.000Z</published>
    <updated>2017-12-28T06:53:26.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15144439857026.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h2 id="Concept"><a href="#Concept" class="headerlink" title="Concept"></a>Concept</h2><p>Mask R-CNN 利用了相當簡潔與彈性的方法進行實例分割, 主要跟 Faster R-CNN 不同的地方在於原架構有 2 個分支 </p><ol><li>Classification branch </li><li>Bounding box branch </li></ol><p>而 Mask R-CNN 的方法則是多加了另一個分支  — Mask branch。</p><h2 id="Mask-branch"><a href="#Mask-branch" class="headerlink" title="Mask branch"></a>Mask branch</h2><h2 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h2><p>L<sub>total</sub> = L<sub>cls</sub> + L<sub>box</sub>  + L<sub>mask</sub> </p><p>L<sub>cls</sub> 跟 L<sub>box</sub>的可以參考Fast-RCNN</p><figure class="highlight gcode"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">J<span class="comment">(θ)</span>=−<span class="number">1</span>m[∑mi=<span class="number">1</span>y<span class="comment">(i)</span>loghθ<span class="comment">(x(i)</span>)+<span class="comment">(1−y(i)</span>)log<span class="comment">(1−hθ(x(i)</span>))]</div></pre></td></tr></table></figure><h2 id="ROI-POOLING"><a href="#ROI-POOLING" class="headerlink" title="ROI POOLING"></a>ROI POOLING</h2><p>ROI POOLING 概念如下圖所示<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15137535435975.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15137536007979.gif" alt="" title="">                </div>                <div class="image-caption"></div>            </figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h2&gt;&lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;di
      
    
    </summary>
    
      <category term="Segmentation" scheme="https://world4jason.github.io/categories/Segmentation/"/>
    
      <category term="Detection" scheme="https://world4jason.github.io/categories/Segmentation/Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://world4jason.github.io/tags/Deep-Learning/"/>
    
      <category term="R-CNN" scheme="https://world4jason.github.io/tags/R-CNN/"/>
    
      <category term="Segmentation" scheme="https://world4jason.github.io/tags/Segmentation/"/>
    
      <category term="Pose Estimation" scheme="https://world4jason.github.io/tags/Pose-Estimation/"/>
    
  </entry>
  
  <entry>
    <title>Conditional GAN - 條件式生成對抗網路</title>
    <link href="https://world4jason.github.io/2017/11/08/Conditional-GAN/"/>
    <id>https://world4jason.github.io/2017/11/08/Conditional-GAN/</id>
    <published>2017-11-08T05:48:12.000Z</published>
    <updated>2017-11-08T12:37:52.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="GAN" scheme="https://world4jason.github.io/categories/GAN/"/>
    
    
      <category term="Deep Learning" scheme="https://world4jason.github.io/tags/Deep-Learning/"/>
    
      <category term="Generation" scheme="https://world4jason.github.io/tags/Generation/"/>
    
      <category term="Generative Model" scheme="https://world4jason.github.io/tags/Generative-Model/"/>
    
      <category term="GAN" scheme="https://world4jason.github.io/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>GAN - Generative Adversarial Network</title>
    <link href="https://world4jason.github.io/2017/11/07/GAN-Generative-Adversarial-Network/"/>
    <id>https://world4jason.github.io/2017/11/07/GAN-Generative-Adversarial-Network/</id>
    <published>2017-11-07T15:00:02.000Z</published>
    <updated>2017-11-08T12:37:57.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Generative-Adversarial-Networks-GANs"><a href="#Generative-Adversarial-Networks-GANs" class="headerlink" title="Generative Adversarial Networks (GANs)"></a>Generative Adversarial Networks (GANs)</h2><h3 id="Lists"><a href="#Lists" class="headerlink" title="Lists"></a>Lists</h3><table><thead><tr><th style="text-align:center"><em>Name</em></th><th style="text-align:center"><em>Paper Link</em></th><th style="text-align:left"><em>Value Function</em></th></tr></thead><tbody><tr><td style="text-align:center"><strong>GAN</strong></td><td style="text-align:center"><a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="external">Arxiv</a></td><td style="text-align:left"><img src="/media/equations/GAN.png"></td></tr><tr><td style="text-align:center"><strong>LSGAN</strong></td><td style="text-align:center"><a href="https://arxiv.org/abs/1611.04076" target="_blank" rel="external">Arxiv</a></td><td style="text-align:left"><img src="/media/equations/LSGAN.png"></td></tr><tr><td style="text-align:center"><strong>WGAN</strong></td><td style="text-align:center"><a href="https://arxiv.org/abs/1701.07875" target="_blank" rel="external">Arxiv</a></td><td style="text-align:left"><img src="/media/equations/WGAN.png"></td></tr><tr><td style="text-align:center"><strong>WGAN-GP</strong></td><td style="text-align:center"><a href="https://arxiv.org/abs/1704.00028" target="_blank" rel="external">Arxiv</a></td><td style="text-align:left"><img src="/media/equations/WGAN_GP.png"></td></tr><tr><td style="text-align:center"><strong>DRAGAN</strong></td><td style="text-align:center"><a href="https://arxiv.org/abs/1705.07215" target="_blank" rel="external">Arxiv</a></td><td style="text-align:left"><img src="/media/equations/DRAGAN.png"></td></tr><tr><td style="text-align:center"><strong>CGAN</strong></td><td style="text-align:center"><a href="https://arxiv.org/abs/1411.1784" target="_blank" rel="external">Arxiv</a></td><td style="text-align:left"><img src="/media/equations/CGAN.png"></td></tr><tr><td style="text-align:center"><strong>infoGAN</strong></td><td style="text-align:center"><a href="https://arxiv.org/abs/1606.03657" target="_blank" rel="external">Arxiv</a></td><td style="text-align:left"><img src="/media/equations/infoGAN.png"></td></tr><tr><td style="text-align:center"><strong>ACGAN</strong></td><td style="text-align:center"><a href="https://arxiv.org/abs/1610.09585" target="_blank" rel="external">Arxiv</a></td><td style="text-align:left"><img src="/media/equations/ACGAN.png"></td></tr><tr><td style="text-align:center"><strong>EBGAN</strong></td><td style="text-align:center"><a href="https://arxiv.org/abs/1609.03126" target="_blank" rel="external">Arxiv</a></td><td style="text-align:left"><img src="/media/equations/EBGAN.png"></td></tr><tr><td style="text-align:center"><strong>BEGAN</strong></td><td style="text-align:center"><a href="https://arxiv.org/abs/1702.08431" target="_blank" rel="external">Arxiv</a></td><td style="text-align:left"><img src="/media/equations/BEGAN.png">  </td></tr></tbody></table><h4 id="Variants-of-GAN-structure"><a href="#Variants-of-GAN-structure" class="headerlink" title="Variants of GAN structure"></a>Variants of GAN structure</h4><p><img src="/media/etc/GAN_structure.png"></p><h3 id="Results-for-mnist"><a href="#Results-for-mnist" class="headerlink" title="Results for mnist"></a>Results for mnist</h3><p>Network architecture of generator and discriminator is the exaclty sames as in <a href="https://arxiv.org/abs/1606.03657" target="_blank" rel="external">infoGAN paper</a>.<br>For fair comparison of core ideas in all gan variants, all implementations for network architecture are kept same except EBGAN and BEGAN. Small modification is made for EBGAN/BEGAN, since those adopt auto-encoder strucutre for discriminator. But I tried to keep the capacity of discirminator.</p><h4 id="Random-generation"><a href="#Random-generation" class="headerlink" title="Random generation"></a>Random generation</h4><p>All results are randomly sampled.</p><table><thead><tr><th style="text-align:center"><em>Name</em></th><th style="text-align:center"><em>Epoch 2</em></th><th style="text-align:center"><em>Epoch 10</em></th><th style="text-align:center"><em>Epoch 25</em></th></tr></thead><tbody><tr><td style="text-align:center">GAN</td><td style="text-align:center"><img src="/media/mnist_results/random_generation/GAN_epoch001_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/GAN_epoch009_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/GAN_epoch024_test_all_classes.png"></td></tr><tr><td style="text-align:center">LSGAN</td><td style="text-align:center"><img src="/media/mnist_results/random_generation/LSGAN_epoch001_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/LSGAN_epoch009_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/LSGAN_epoch024_test_all_classes.png"></td></tr><tr><td style="text-align:center">WGAN</td><td style="text-align:center"><img src="/media/mnist_results/random_generation/WGAN_epoch001_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/WGAN_epoch009_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/WGAN_epoch024_test_all_classes.png"></td></tr><tr><td style="text-align:center">WGAN-GP</td><td style="text-align:center"><img src="/media/mnist_results/random_generation/WGAN-GP_epoch001_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/WGAN-GP_epoch009_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/WGAN-GP_epoch024_test_all_classes.png"></td></tr><tr><td style="text-align:center">DRAGAN</td><td style="text-align:center"><img src="/media/mnist_results/random_generation/DRAGAN_epoch001_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/DRAGAN_epoch009_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/DRAGAN_epoch024_test_all_classes.png"></td></tr><tr><td style="text-align:center">EBGAN</td><td style="text-align:center"><img src="/media/mnist_results/random_generation/EBGAN_epoch001_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/EBGAN_epoch009_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/EBGAN_epoch024_test_all_classes.png"></td></tr><tr><td style="text-align:center">BEGAN</td><td style="text-align:center"><img src="/media/mnist_results/random_generation/BEGAN_epoch001_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/BEGAN_epoch009_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/BEGAN_epoch024_test_all_classes.png"></td></tr></tbody></table><h4 id="Conditional-generation"><a href="#Conditional-generation" class="headerlink" title="Conditional generation"></a>Conditional generation</h4><p>Each row has the same noise vector and each column has the same label condition.</p><table><thead><tr><th style="text-align:center"><em>Name</em></th><th style="text-align:center"><em>Epoch 1</em></th><th style="text-align:center"><em>Epoch 10</em></th><th style="text-align:center"><em>Epoch 25</em></th></tr></thead><tbody><tr><td style="text-align:center">CGAN</td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/CGAN_epoch000_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/CGAN_epoch009_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/CGAN_epoch024_test_all_classes_style_by_style.png"></td></tr><tr><td style="text-align:center">ACGAN</td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/ACGAN_epoch000_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/ACGAN_epoch009_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/ACGAN_epoch024_test_all_classes_style_by_style.png"></td></tr><tr><td style="text-align:center">infoGAN</td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/infoGAN_epoch000_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/infoGAN_epoch009_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/infoGAN_epoch024_test_all_classes_style_by_style.png"></td></tr></tbody></table><h4 id="InfoGAN-Manipulating-two-continous-codes"><a href="#InfoGAN-Manipulating-two-continous-codes" class="headerlink" title="InfoGAN : Manipulating two continous codes"></a>InfoGAN : Manipulating two continous codes</h4><table align="center"><br><td><img src="/media/mnist_results/infogan/infoGAN_epoch024_test_class_c1c2_2.png"></td><br><td><img src="/media/mnist_results/infogan/infoGAN_epoch024_test_class_c1c2_5.png"></td><br><td><img src="/media/mnist_results/infogan/infoGAN_epoch024_test_class_c1c2_7.png"></td><br><td><img src="/media/mnist_results/infogan/infoGAN_epoch024_test_class_c1c2_9.png"></td><br></table><h3 id="Results-for-fashion-mnist"><a href="#Results-for-fashion-mnist" class="headerlink" title="Results for fashion-mnist"></a>Results for fashion-mnist</h3><p>Comments on network architecture in mnist are also applied to here.<br><a href="https://github.com/zalandoresearch/fashion-mnist" target="_blank" rel="external">Fashion-mnist</a> is a recently proposed dataset consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. (T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot)</p><h4 id="Random-generation-1"><a href="#Random-generation-1" class="headerlink" title="Random generation"></a>Random generation</h4><p>All results are randomly sampled.</p><table><thead><tr><th style="text-align:center"><em>Name</em></th><th style="text-align:center"><em>Epoch 1</em></th><th style="text-align:center"><em>Epoch 20</em></th><th style="text-align:center"><em>Epoch 40</em></th></tr></thead><tbody><tr><td style="text-align:center">GAN</td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/GAN_epoch000_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/GAN_epoch019_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/GAN_epoch039_test_all_classes.png"></td></tr><tr><td style="text-align:center">LSGAN</td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/LSGAN_epoch000_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/LSGAN_epoch019_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/LSGAN_epoch039_test_all_classes.png"></td></tr><tr><td style="text-align:center">WGAN</td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/WGAN_epoch000_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/WGAN_epoch019_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/WGAN_epoch039_test_all_classes.png"></td></tr><tr><td style="text-align:center">WGAN-GP</td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/WGAN-GP_epoch000_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/WGAN-GP_epoch019_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/WGAN-GP_epoch039_test_all_classes.png"></td></tr><tr><td style="text-align:center">DRAGAN</td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/DRAGAN_epoch000_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/DRAGAN_epoch019_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/DRAGAN_epoch039_test_all_classes.png"></td></tr><tr><td style="text-align:center">EBGAN</td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/EBGAN_epoch000_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/EBGAN_epoch019_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/EBGAN_epoch039_test_all_classes.png"></td></tr><tr><td style="text-align:center">BEGAN</td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/BEGAN_epoch000_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/BEGAN_epoch019_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/BEGAN_epoch039_test_all_classes.png"></td></tr></tbody></table><h4 id="Conditional-generation-1"><a href="#Conditional-generation-1" class="headerlink" title="Conditional generation"></a>Conditional generation</h4><p>Each row has the same noise vector and each column has the same label condition.</p><table><thead><tr><th style="text-align:center"><em>Name</em></th><th style="text-align:center"><em>Epoch 1</em></th><th style="text-align:center"><em>Epoch 20</em></th><th style="text-align:center"><em>Epoch 40</em></th></tr></thead><tbody><tr><td style="text-align:center">CGAN</td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/CGAN_epoch000_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/CGAN_epoch019_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/CGAN_epoch039_test_all_classes_style_by_style.png"></td></tr><tr><td style="text-align:center">ACGAN</td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/ACGAN_epoch000_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/ACGAN_epoch019_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/ACGAN_epoch039_test_all_classes_style_by_style.png"></td></tr><tr><td style="text-align:center">infoGAN</td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/infoGAN_epoch000_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/infoGAN_epoch019_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/infoGAN_epoch039_test_all_classes_style_by_style.png"></td></tr></tbody></table><p>Without hyper-parameter tuning from mnist-version, ACGAN/infoGAN does not work well as compared with CGAN.<br>ACGAN tends to fall into mode-collapse.<br>infoGAN tends to ignore noise-vector. It results in that various style within the same class can not be represented.</p><h4 id="InfoGAN-Manipulating-two-continous-codes-1"><a href="#InfoGAN-Manipulating-two-continous-codes-1" class="headerlink" title="InfoGAN : Manipulating two continous codes"></a>InfoGAN : Manipulating two continous codes</h4><table align="center"><br><td><img src="/media/fashion_mnist_results/infogan/infoGAN_epoch039_test_class_c1c2_1.png"></td><br><td><img src="/media/fashion_mnist_results/infogan/infoGAN_epoch039_test_class_c1c2_4.png"></td><br><td><img src="/media/fashion_mnist_results/infogan/infoGAN_epoch039_test_class_c1c2_5.png"></td><br><td><img src="/media/fashion_mnist_results/infogan/infoGAN_epoch039_test_class_c1c2_8.png"></td><br></table><h3 id="Some-results-for-celebA"><a href="#Some-results-for-celebA" class="headerlink" title="Some results for celebA"></a>Some results for celebA</h3><p>(to be added)</p><h2 id="Variational-Auto-Encoders-VAEs"><a href="#Variational-Auto-Encoders-VAEs" class="headerlink" title="Variational Auto-Encoders (VAEs)"></a>Variational Auto-Encoders (VAEs)</h2><h3 id="Lists-1"><a href="#Lists-1" class="headerlink" title="Lists"></a>Lists</h3><table><thead><tr><th style="text-align:center"><em>Name</em></th><th style="text-align:center"><em>Paper Link</em></th><th style="text-align:left"><em>Loss Function</em></th></tr></thead><tbody><tr><td style="text-align:center"><strong>VAE</strong></td><td style="text-align:center"><a href="https://arxiv.org/abs/1312.6114" target="_blank" rel="external">Arxiv</a></td><td style="text-align:left"><img src="/media/equations/CVAE.png"> </td></tr><tr><td style="text-align:center"><strong>CVAE</strong></td><td style="text-align:center"><a href="https://arxiv.org/abs/1406.5298" target="_blank" rel="external">Arxiv</a></td><td style="text-align:left"><img src="/media/equations/CVAE.png"></td></tr><tr><td style="text-align:center"><strong>DVAE</strong></td><td style="text-align:center"><a href="https://arxiv.org/abs/1511.06406" target="_blank" rel="external">Arxiv</a></td><td style="text-align:left">(to be added)</td></tr><tr><td style="text-align:center"><strong>AAE</strong></td><td style="text-align:center"><a href="https://arxiv.org/abs/1511.05644" target="_blank" rel="external">Arxiv</a></td><td style="text-align:left">(to be added) </td></tr></tbody></table><h4 id="Variants-of-VAE-structure"><a href="#Variants-of-VAE-structure" class="headerlink" title="Variants of VAE structure"></a>Variants of VAE structure</h4><p><img src="/media/etc/VAE_structure.png"></p><h3 id="Results-for-mnist-1"><a href="#Results-for-mnist-1" class="headerlink" title="Results for mnist"></a>Results for mnist</h3><p>Network architecture of decoder(generator) and encoder(discriminator) is the exaclty sames as in <a href="https://arxiv.org/abs/1606.0365" target="_blank" rel="external">infoGAN paper</a>. The number of output nodes in encoder is different. (2x z_dim for VAE, 1 for GAN)</p><h4 id="Random-generation-2"><a href="#Random-generation-2" class="headerlink" title="Random generation"></a>Random generation</h4><p>All results are randomly sampled.</p><table><thead><tr><th style="text-align:center"><em>Name</em></th><th style="text-align:center"><em>Epoch 1</em></th><th style="text-align:center"><em>Epoch 10</em></th><th style="text-align:center"><em>Epoch 25</em></th></tr></thead><tbody><tr><td style="text-align:center">VAE</td><td style="text-align:center"><img src="/media/mnist_results/random_generation/VAE_epoch000_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/VAE_epoch009_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/VAE_epoch024_test_all_classes.png"></td></tr><tr><td style="text-align:center">GAN</td><td style="text-align:center"><img src="/media/mnist_results/random_generation/GAN_epoch000_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/GAN_epoch009_test_all_classes.png"></td><td style="text-align:center"><img src="/media/mnist_results/random_generation/GAN_epoch024_test_all_classes.png"></td></tr></tbody></table><p>Results of GAN is also given to compare images generated from VAE and GAN.<br>The main difference (VAE generates smooth and blurry images, otherwise GAN generates sharp and artifact images) is cleary observed from the results.</p><h4 id="Conditional-generation-2"><a href="#Conditional-generation-2" class="headerlink" title="Conditional generation"></a>Conditional generation</h4><p>Each row has the same noise vector and each column has the same label condition.</p><table><thead><tr><th style="text-align:center"><em>Name</em></th><th style="text-align:center"><em>Epoch 1</em></th><th style="text-align:center"><em>Epoch 10</em></th><th style="text-align:center"><em>Epoch 25</em></th></tr></thead><tbody><tr><td style="text-align:center">CVAE</td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/CVAE_epoch000_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/CVAE_epoch009_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/CVAE_epoch024_test_all_classes_style_by_style.png"></td></tr><tr><td style="text-align:center">CGAN</td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/CGAN_epoch000_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/CGAN_epoch009_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/mnist_results/conditional_generation/CGAN_epoch024_test_all_classes_style_by_style.png"></td></tr></tbody></table><p>Results of CGAN is also given to compare images generated from CVAE and CGAN.</p><h4 id="Learned-manifold"><a href="#Learned-manifold" class="headerlink" title="Learned manifold"></a>Learned manifold</h4><p>The following results can be reproduced with command:<br><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="comment">python</span> <span class="comment">main</span><span class="string">.</span><span class="comment">py</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">dataset</span> <span class="comment">mnist</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">gan_type</span> <span class="comment">VAE</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">epoch</span> <span class="comment">25</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">batch_size</span> <span class="comment">64</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">dim_z</span> <span class="comment">2</span></div></pre></td></tr></table></figure></p><p>Please notice that dimension of noise-vector z is 2.</p><table><thead><tr><th style="text-align:center"><em>Name</em></th><th style="text-align:center"><em>Epoch 1</em></th><th style="text-align:center"><em>Epoch 10</em></th><th style="text-align:center"><em>Epoch 25</em></th></tr></thead><tbody><tr><td style="text-align:center">VAE</td><td style="text-align:center"><img src="/media/mnist_results/learned_manifold/VAE_epoch000_learned_manifold.png"></td><td style="text-align:center"><img src="/media/mnist_results/learned_manifold/VAE_epoch009_learned_manifold.png"></td><td style="text-align:center"><img src="/media/mnist_results/learned_manifold/VAE_epoch024_learned_manifold.png"></td></tr></tbody></table><h3 id="Results-for-fashion-mnist-1"><a href="#Results-for-fashion-mnist-1" class="headerlink" title="Results for fashion-mnist"></a>Results for fashion-mnist</h3><p>Comments on network architecture in mnist are also applied to here. </p><p>The following results can be reproduced with command:<br><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="comment">python</span> <span class="comment">main</span><span class="string">.</span><span class="comment">py</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">dataset</span> <span class="comment">fashion</span><span class="literal">-</span><span class="comment">mnist</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">gan_type</span> &lt;<span class="comment">TYPE</span>&gt; <span class="literal">-</span><span class="literal">-</span><span class="comment">epoch</span> <span class="comment">40</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">batch_size</span> <span class="comment">64</span></div></pre></td></tr></table></figure></p><h4 id="Random-generation-3"><a href="#Random-generation-3" class="headerlink" title="Random generation"></a>Random generation</h4><p>All results are randomly sampled.</p><table><thead><tr><th style="text-align:center"><em>Name</em></th><th style="text-align:center"><em>Epoch 1</em></th><th style="text-align:center"><em>Epoch 20</em></th><th style="text-align:center"><em>Epoch 40</em></th></tr></thead><tbody><tr><td style="text-align:center">VAE</td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/VAE_epoch000_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/VAE_epoch019_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/VAE_epoch039_test_all_classes.png"></td></tr><tr><td style="text-align:center">GAN</td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/GAN_epoch000_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/GAN_epoch019_test_all_classes.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/random_generation/GAN_epoch039_test_all_classes.png"></td></tr></tbody></table><p>Results of GAN is also given to compare images generated from VAE and GAN.</p><h4 id="Conditional-generation-3"><a href="#Conditional-generation-3" class="headerlink" title="Conditional generation"></a>Conditional generation</h4><p>Each row has the same noise vector and each column has the same label condition.</p><table><thead><tr><th style="text-align:center"><em>Name</em></th><th style="text-align:center"><em>Epoch 1</em></th><th style="text-align:center"><em>Epoch 20</em></th><th style="text-align:center"><em>Epoch 40</em></th></tr></thead><tbody><tr><td style="text-align:center">CVAE</td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/CVAE_epoch000_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/CVAE_epoch019_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/CVAE_epoch039_test_all_classes_style_by_style.png"></td></tr><tr><td style="text-align:center">CGAN</td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/CGAN_epoch000_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/CGAN_epoch019_test_all_classes_style_by_style.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/conditional_generation/CGAN_epoch039_test_all_classes_style_by_style.png"></td></tr></tbody></table><p>Results of CGAN is also given to compare images generated from CVAE and CGAN.</p><h4 id="Learned-manifold-1"><a href="#Learned-manifold-1" class="headerlink" title="Learned manifold"></a>Learned manifold</h4><p>The following results can be reproduced with command:<br><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="comment">python</span> <span class="comment">main</span><span class="string">.</span><span class="comment">py</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">dataset</span> <span class="comment">fashion</span><span class="literal">-</span><span class="comment">mnist</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">gan_type</span> <span class="comment">VAE</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">epoch</span> <span class="comment">25</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">batch_size</span> <span class="comment">64</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">dim_z</span> <span class="comment">2</span></div></pre></td></tr></table></figure></p><p>Please notice that dimension of noise-vector z is 2.</p><table><thead><tr><th style="text-align:center"><em>Name</em></th><th style="text-align:center"><em>Epoch 1</em></th><th style="text-align:center"><em>Epoch 10</em></th><th style="text-align:center"><em>Epoch 25</em></th></tr></thead><tbody><tr><td style="text-align:center">VAE</td><td style="text-align:center"><img src="/media/fashion_mnist_results/learned_manifold/VAE_epoch000_learned_manifold.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/learned_manifold/VAE_epoch009_learned_manifold.png"></td><td style="text-align:center"><img src="/media/fashion_mnist_results/learned_manifold/VAE_epoch024_learned_manifold.png"></td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Generative-Adversarial-Networks-GANs&quot;&gt;&lt;a href=&quot;#Generative-Adversarial-Networks-GANs&quot; class=&quot;headerlink&quot; title=&quot;Generative Adversari
      
    
    </summary>
    
      <category term="GAN" scheme="https://world4jason.github.io/categories/GAN/"/>
    
    
      <category term="Deep Learning" scheme="https://world4jason.github.io/tags/Deep-Learning/"/>
    
      <category term="Generation" scheme="https://world4jason.github.io/tags/Generation/"/>
    
      <category term="Generative Model" scheme="https://world4jason.github.io/tags/Generative-Model/"/>
    
      <category term="GAN" scheme="https://world4jason.github.io/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>Self-Study Courses List</title>
    <link href="https://world4jason.github.io/2017/11/07/Self-Study-Courses-List/"/>
    <id>https://world4jason.github.io/2017/11/07/Self-Study-Courses-List/</id>
    <published>2017-11-07T03:26:18.000Z</published>
    <updated>2018-01-31T04:03:31.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="學習路程"><a href="#學習路程" class="headerlink" title="學習路程"></a>學習路程</h2><ul><li>Python<br>  政大 or MIT python都好 後者比較舊但作業比較多</li><li>ML<br>  先看莫凡 看完再看NTU基礎的 剩下就看要看進階的還是standford的</li></ul><h2 id="Basic-Concept-of-Machine-Learning"><a href="#Basic-Concept-of-Machine-Learning" class="headerlink" title="Basic Concept of Machine Learning"></a>Basic Concept of Machine Learning</h2><ul><li>Morvan莫凡<ul><li><a href="https://www.youtube.com/playlist?list=PLXO45tsB95cIFm8Y8vMkNNPPXAtYXwKin" target="_blank" rel="external">有趣的機器學習</a></li></ul></li></ul><h2 id="Machine-Learning"><a href="#Machine-Learning" class="headerlink" title="Machine Learning"></a>Machine Learning</h2><p>-<a href="https://cn.udacity.com/course/intro-to-machine-learning--ud120" target="_blank" rel="external">Udacity ud120</a></p><h2 id="Machine-Learning-Deep-Learning-Series"><a href="#Machine-Learning-Deep-Learning-Series" class="headerlink" title="Machine Learning + Deep Learning Series"></a>Machine Learning + Deep Learning Series</h2><ul><li><h3 id="National-Taiwan-University-李宏毅"><a href="#National-Taiwan-University-李宏毅" class="headerlink" title="National Taiwan University - 李宏毅"></a>National Taiwan University - 李宏毅</h3><ul><li><p>BASIC</p><ul><li><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html" target="_blank" rel="external">Machine Learning (2016,Fall)</a></li><li><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML17.html" target="_blank" rel="external">Machine Learning (2017,Spring)</a></li><li><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML17_2.html" target="_blank" rel="external">Machine Learning (2017,Spring)</a></li></ul></li><li><p>ADV</p><ul><li><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS17.html" target="_blank" rel="external">Machine Learning and having it deep and structured (2017,Spring</a></li><li><a href="speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS17.html">Machine Learning and having it deep and structured (2017,Fall)</a><br>-</li><li><h3 id="National-Taiwan-University-林軒田"><a href="#National-Taiwan-University-林軒田" class="headerlink" title="National Taiwan University - 林軒田"></a>National Taiwan University - 林軒田</h3></li></ul></li><li><a href="https://www.youtube.com/watch?v=nQvpFSMPhr0&amp;list=PLXVfgk9fNX2I7tB6oIINGBmW50rrmFTqf" target="_blank" rel="external">機器學習基石</a></li><li><a href="https://www.youtube.com/watch?v=A-GxGCCAIrg&amp;list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2" target="_blank" rel="external">機器學習技法</a></li></ul><ul><li><h3 id="Coursera"><a href="#Coursera" class="headerlink" title="Coursera"></a>Coursera</h3><ul><li><a href="https://www.coursera.org/specializations/deep-learning" target="_blank" rel="external">Deep Learning Specialization</a></li></ul></li></ul></li><li><h3 id="blog"><a href="#blog" class="headerlink" title="blog"></a>blog</h3><ul><li><a href="http://www.cosmosshadow.com/ml" target="_blank" rel="external">Blog</a></li><li><a href="https://brohrer.mcknote.com/zh-Hant/" target="_blank" rel="external">資料科學・機器・人</a></li></ul></li><li><h3 id="GitHub-sjchoi86"><a href="#GitHub-sjchoi86" class="headerlink" title="GitHub-sjchoi86"></a>GitHub-sjchoi86</h3><p>  <a href="https://github.com/sjchoi86/dl-workshop" target="_blank" rel="external">https://github.com/sjchoi86/dl-workshop</a><br>  <a href="https://github.com/sjchoi86/dl_tutorials" target="_blank" rel="external">https://github.com/sjchoi86/dl_tutorials</a><br>  <a href="https://github.com/sjchoi86/Deep-Learning-101" target="_blank" rel="external">https://github.com/sjchoi86/Deep-Learning-101</a><br>  <a href="https://github.com/sjchoi86/dl_tutorials_10weeks" target="_blank" rel="external">https://github.com/sjchoi86/dl_tutorials_10weeks</a><br>  <a href="https://github.com/sjchoi86/dl_tutorials_3rd" target="_blank" rel="external">https://github.com/sjchoi86/dl_tutorials_3rd</a></p></li><li><h3 id="Deep-Learning-Specialize"><a href="#Deep-Learning-Specialize" class="headerlink" title="Deep Learning Specialize"></a>Deep Learning Specialize</h3><ul><li><a href="https://mooc.study.163.com/smartSpec/detail/1001319001.htm" target="_blank" rel="external">網易雲中文版</a>  </li></ul></li><li><h3 id="Colorado"><a href="#Colorado" class="headerlink" title="Colorado"></a>Colorado</h3><ul><li><a href="https://www.cs.colorado.edu/~mozer/Teaching/syllabi/DeepLearningFall2017/" target="_blank" rel="external">Neural Networks and Deep Learning 2017 Fall</a> </li></ul></li><li><h3 id="Book"><a href="#Book" class="headerlink" title="Book"></a>Book</h3><ul><li><a href="https://github.com/zsdonghao/deep-learning-book" target="_blank" rel="external">花書</a> </li></ul></li><li><h3 id="Google-Brain-Hugo-Larochelle"><a href="#Google-Brain-Hugo-Larochelle" class="headerlink" title="Google Brain Hugo Larochelle"></a>Google Brain Hugo Larochelle</h3><ul><li><a href="http://www.mooc.ai/course/300#modal" target="_blank" rel="external">Mooc.ai</a><h2 id="Visual"><a href="#Visual" class="headerlink" title="Visual"></a>Visual</h2></li></ul></li><li><h3 id="Stanford-CS213"><a href="#Stanford-CS213" class="headerlink" title="Stanford CS213"></a>Stanford CS213</h3><ul><li><a href="http://cs231n.stanford.edu/2016/syllabus" target="_blank" rel="external">2016 Syllabus</a> </li><li><a href="https://www.youtube.com/playlist?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC" target="_blank" rel="external">2016 Winter</a></li><li><p><a href="http://study.163.com/course/introduction/1003223001.htm" target="_blank" rel="external">2016 winter 中文版</a></p></li><li><p><a href="http://cs231n.stanford.edu/2017/syllabus" target="_blank" rel="external">2017 Syllabus</a></p></li><li><a href="https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv" target="_blank" rel="external">2017 Spring</a></li><li><a href="http://www.mooc.ai/course/268" target="_blank" rel="external">2017 winter 中文版</a></li></ul></li></ul><h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><ul><li><h3 id="NCCU"><a href="#NCCU" class="headerlink" title="NCCU"></a>NCCU</h3><ul><li><a href="http://moocs.nccu.edu.tw/course/121/intro" target="_blank" rel="external">Data science with Python</a></li></ul></li><li><h3 id="MIT"><a href="#MIT" class="headerlink" title="MIT"></a>MIT</h3><ul><li><a href="http://learn.edx.org/mit-python/" target="_blank" rel="external">MIT Python</a> </li></ul></li><li><h3 id="Blog"><a href="#Blog" class="headerlink" title="Blog"></a>Blog</h3><ul><li><a href="https://www.saltycrane.com/blog/tag/python/" target="_blank" rel="external">saltycrane</a>   </li><li><a href="https://ithelp.ithome.com.tw/users/20107274/ironman/1578" target="_blank" rel="external">30天python雜談</a></li></ul></li><li><h3 id="Effective-Python"><a href="#Effective-Python" class="headerlink" title="Effective Python"></a>Effective Python</h3><ul><li><a href="https://guoruibiao.gitbooks.io/effective-python/content/" target="_blank" rel="external">59 Specific Ways to Write Better Python簡中翻譯</a><br>-<a href="http://seanlin.logdown.com/archives" target="_blank" rel="external">Python慣用語&amp;Effective Python</a></li></ul></li></ul><h2 id="Deep-Learning-Framework-Tutorial"><a href="#Deep-Learning-Framework-Tutorial" class="headerlink" title="Deep Learning Framework Tutorial"></a>Deep Learning Framework Tutorial</h2><ul><li><h3 id="Tensorflow"><a href="#Tensorflow" class="headerlink" title="Tensorflow"></a>Tensorflow</h3><ul><li><a href="tensorflow.org">Tensorflow 官方</a></li><li><a href="http://usyiyi.cn/documents/tensorflow_13/tutorials/index.html" target="_blank" rel="external">Tensorflow 中文</a> </li><li><a href="wiki.jikexueyuan.com/project/tensorflow-zh">Tensorflow 中文2</a></li><li><a href="https://www.youtube.com/playlist?list=PLQ0sVbIj3URf94DQtGPJV629ctn2c1zN-" target="_blank" rel="external">Stanford CS20SI - TensorFlow</a></li><li><a href="https://www.udacity.com/course/deep-learning--ud730" target="_blank" rel="external">Udacity ud730</a></li><li><a href="https://www.youtube.com/playlist?list=PL-XeOa5hMEYxNzHM7YLRjIwE1k3VQpqEh" target="_blank" rel="external">cognitiveclass.ai</a></li><li><a href="http://learningtensorflow.com/" target="_blank" rel="external">learning Tensorflow Web</a></li><li><a href="https://web.stanford.edu/class/cs20si/index.html" target="_blank" rel="external">Stanford CS 20</a></li><li></li></ul></li><li><h3 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h3><ul><li><a href=""></a></li></ul></li><li><h3 id="Keras"><a href="#Keras" class="headerlink" title="Keras"></a>Keras</h3></li></ul><h2 id="Tech-Paper"><a href="#Tech-Paper" class="headerlink" title="Tech Paper"></a>Tech Paper</h2><ul><li><a href="https://github.com/sjchoi86/dl_tutorials" target="_blank" rel="external">dl_tutorials</a>   </li><li><a href="https://github.com/sjchoi86/dl_tutorials_10weeks" target="_blank" rel="external">dl_tutorials_10weeks</a></li></ul><h2 id="Deep-Learning-in-practice"><a href="#Deep-Learning-in-practice" class="headerlink" title="Deep Learning in practice"></a>Deep Learning in practice</h2><ul><li><p><a href="http://www.fast.ai/" target="_blank" rel="external">Fast.ai</a> </p></li><li><h3 id="Tensorflow-1"><a href="#Tensorflow-1" class="headerlink" title="Tensorflow"></a>Tensorflow</h3><ul><li><a href="https://github.com/sjchoi86/Tensorflow-101" target="_blank" rel="external">Tensorflow 101</a></li></ul></li><li><h3 id="Keras-1"><a href="#Keras-1" class="headerlink" title="Keras"></a>Keras</h3><ul><li><a href="https://github.com/erhwenkuo/deep-learning-with-keras-notebooks" target="_blank" rel="external">緯創</a></li></ul></li><li><h3 id="MXNET"><a href="#MXNET" class="headerlink" title="MXNET"></a>MXNET</h3><p>  -<a href="https://zh.gluon.ai/" target="_blank" rel="external">Gluon</a><br>  -<a href="https://github.com/mli/gluon-tutorials-zh/" target="_blank" rel="external">Gluon-Github</a></p></li><li><h3 id="Udacity-Self-drive"><a href="#Udacity-Self-drive" class="headerlink" title="Udacity Self-drive"></a>Udacity Self-drive</h3><ul><li><a href="https://github.com/ndrplz/self-driving-car" target="_blank" rel="external">self-driving-car</a> <h2 id="Concept"><a href="#Concept" class="headerlink" title="Concept"></a>Concept</h2>-CapsuleNet</li></ul></li><li><a href="https://www.youtube.com/watch?v=UhGWH3hb3Hk" target="_blank" rel="external">李宏毅</a></li></ul><h2 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h2><ul><li><a href="https://try.github.io/" target="_blank" rel="external">Git</a></li><li><h2 id="Online-IDE"><a href="#Online-IDE" class="headerlink" title="Online IDE"></a>Online IDE</h2></li><li><a href="https://repl.it/" target="_blank" rel="external">repl.it</a></li></ul><h2 id="Online-practice"><a href="#Online-practice" class="headerlink" title="Online practice"></a>Online practice</h2><ul><li><a href="http://www.codewars.com/" target="_blank" rel="external">codewars</a></li></ul><h2 id="Two-Minutes-Paper"><a href="#Two-Minutes-Paper" class="headerlink" title="Two Minutes Paper"></a>Two Minutes Paper</h2><ul><li><a href="">Two Minute Papers</a><br>雷鋒網也有兩分鐘系列</li></ul><h2 id="Leiphone"><a href="#Leiphone" class="headerlink" title="Leiphone"></a>Leiphone</h2><ul><li><a href="https://www.leiphone.com/news/201710/ipWGpR7iTGkvNhHL.html" target="_blank" rel="external">Video Collection</a></li></ul><h2 id="尚未整理"><a href="#尚未整理" class="headerlink" title="尚未整理"></a>尚未整理</h2><p><a href="http://open.163.com/special/opencourse/daishu.html" target="_blank" rel="external">http://open.163.com/special/opencourse/daishu.html</a></p><p><a href="http://www.kaierlong.me/blog/post/kaierlong/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%80%E8%AF%BE%E6%B1%87%E6%80%BB" target="_blank" rel="external">http://www.kaierlong.me/blog/post/kaierlong/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%80%E8%AF%BE%E6%B1%87%E6%80%BB</a></p><p>Cognitive Class<br><a href="https://cognitiveclass.ai/" target="_blank" rel="external">https://cognitiveclass.ai/</a><br>Cognitive Class - IBM Big Data Learn<br><a href="https://cognitiveclass.ai/learn/big-data/" target="_blank" rel="external">https://cognitiveclass.ai/learn/big-data/</a><br>Cognitive Class - IBM Hadoop Foundations Learn<br><a href="https://cognitiveclass.ai/learn/hadoop/" target="_blank" rel="external">https://cognitiveclass.ai/learn/hadoop/</a><br>Cognitive Class - IBM Data Science Foundations Learn<br><a href="https://cognitiveclass.ai/learn/data-science/" target="_blank" rel="external">https://cognitiveclass.ai/learn/data-science/</a><br>Cognitive Class - IBM Data Science for Business Learn<br><a href="https://cognitiveclass.ai/learn/data-science-business/" target="_blank" rel="external">https://cognitiveclass.ai/learn/data-science-business/</a><br>Cognitive Class - IBM Deep Learning Learn<br><a href="https://cognitiveclass.ai/learn/deep-learning/" target="_blank" rel="external">https://cognitiveclass.ai/learn/deep-learning/</a></p><p><a href="http://dlib.net/ml_guide.svg" target="_blank" rel="external">http://dlib.net/ml_guide.svg</a></p><p><a href="http://www.deepideas.net/deep-learning-from-scratch-theory-and-implementation/" target="_blank" rel="external">http://www.deepideas.net/deep-learning-from-scratch-theory-and-implementation/</a></p><p><a href="https://www.ctolib.com/" target="_blank" rel="external">https://www.ctolib.com/</a></p><p><a href="http://blog.csdn.net/u011974639/article/details/73196349" target="_blank" rel="external">http://blog.csdn.net/u011974639/article/details/73196349</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;學習路程&quot;&gt;&lt;a href=&quot;#學習路程&quot; class=&quot;headerlink&quot; title=&quot;學習路程&quot;&gt;&lt;/a&gt;學習路程&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Python&lt;br&gt;  政大 or MIT python都好 後者比較舊但作業比較多&lt;/li&gt;
&lt;li&gt;ML&lt;b
      
    
    </summary>
    
    
      <category term="Deep Learning" scheme="https://world4jason.github.io/tags/Deep-Learning/"/>
    
      <category term="Self-Study" scheme="https://world4jason.github.io/tags/Self-Study/"/>
    
      <category term="Courses" scheme="https://world4jason.github.io/tags/Courses/"/>
    
  </entry>
  
  <entry>
    <title>GAN - Github List</title>
    <link href="https://world4jason.github.io/2017/11/07/GAN-Github-List/"/>
    <id>https://world4jason.github.io/2017/11/07/GAN-Github-List/</id>
    <published>2017-11-07T02:34:33.000Z</published>
    <updated>2017-12-13T14:47:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>paperList<br><a href="https://github.com/zhangqianhui/AdversarialNetsPapers" target="_blank" rel="external">https://github.com/zhangqianhui/AdversarialNetsPapers</a><br><a href="https://github.com/hindupuravinash/the-gan-zoo" target="_blank" rel="external">https://github.com/hindupuravinash/the-gan-zoo</a><br><a href="https://github.com/nightrome/really-awesome-gan" target="_blank" rel="external">https://github.com/nightrome/really-awesome-gan</a><br><a href="https://github.com/nashory/gans-awesome-applications" target="_blank" rel="external">https://github.com/nashory/gans-awesome-applications</a><br><a href="https://github.com/GKalliatakis/Delving-deep-into-GANs" target="_blank" rel="external">https://github.com/GKalliatakis/Delving-deep-into-GANs</a></p><p>====IMPLEMENTATION====<br><a href="https://github.com/YadiraF/GAN" target="_blank" rel="external">https://github.com/YadiraF/GAN</a><br><a href="https://github.com/jonbruner/generative-adversarial-networks" target="_blank" rel="external">https://github.com/jonbruner/generative-adversarial-networks</a><br><a href="https://github.com/tjwei/GANotebooks" target="_blank" rel="external">https://github.com/tjwei/GANotebooks</a><br><a href="https://github.com/AaronYALai/Generative_Adversarial_Networks_PyTorch" target="_blank" rel="external">https://github.com/AaronYALai/Generative_Adversarial_Networks_PyTorch</a></p><p>最完整的GAN實作<br><a href="https://github.com/znxlwm/pytorch-generative-model-collections" target="_blank" rel="external">https://github.com/znxlwm/pytorch-generative-model-collections</a><br><a href="https://github.com/hwalsuklee/tensorflow-generative-model-collections" target="_blank" rel="external">https://github.com/hwalsuklee/tensorflow-generative-model-collections</a><br><a href="https://github.com/eriklindernoren/Keras-GAN" target="_blank" rel="external">https://github.com/eriklindernoren/Keras-GAN</a></p><p>用貓玩GAN<br><a href="https://github.com/AlexiaJM/Deep-learning-with-cats" target="_blank" rel="external">https://github.com/AlexiaJM/Deep-learning-with-cats</a></p><p>tensorflow跟GAN都有<br><a href="https://github.com/wiseodd/generative-models" target="_blank" rel="external">https://github.com/wiseodd/generative-models</a></p><p><a href="https://github.com/jtoy/awesome-tensorflow" target="_blank" rel="external">https://github.com/jtoy/awesome-tensorflow</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;paperList&lt;br&gt;&lt;a href=&quot;https://github.com/zhangqianhui/AdversarialNetsPapers&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/zhangqianh
      
    
    </summary>
    
      <category term="Github" scheme="https://world4jason.github.io/categories/Github/"/>
    
    
      <category term="Generation" scheme="https://world4jason.github.io/tags/Generation/"/>
    
      <category term="Generative Model" scheme="https://world4jason.github.io/tags/Generative-Model/"/>
    
      <category term="GAN" scheme="https://world4jason.github.io/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>R-CNN:Region proposals+CNN</title>
    <link href="https://world4jason.github.io/2017/11/06/R-CNN/"/>
    <id>https://world4jason.github.io/2017/11/06/R-CNN/</id>
    <published>2017-11-06T07:42:19.000Z</published>
    <updated>2017-11-07T15:30:03.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="RCNN-將CNN引入目標檢測的開山之作"><a href="#RCNN-將CNN引入目標檢測的開山之作" class="headerlink" title="RCNN- 將CNN引入目標檢測的開山之作"></a>RCNN- 將CNN引入目標檢測的開山之作</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15099664021450.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p><a href="http://cs231n.stanford.edu/slides/2016/winter1516_lecture8.pdf" target="_blank" rel="external">CS231n lecture8</a></p><p>RCNN (論文：Rich feature hierarchies for accurate object detection and semantic segmentation) 是將CNN方法引入目標檢測領域， 大大提高了目標檢測效果，可以說改變了目標檢測領域的主要研究思路， 緊隨其後的系列文章： （ RCNN ）, Fast RCNN , Faster RCNN 代表該領域當前最高水準。</p><p>【論文主要特點】（相對傳統方法的改進）</p><p>速度： 經典的目標檢測算法使用滑動窗法依次判斷所有可能的區域。 本文則(採用Selective Search方法)預先提取一系列較可能是物體的候選區域，之後僅在這些候選區域上(採用CNN)提取特徵，進行判斷。<br>訓練集： 經典的目標檢測算法在區域中提取人工設定的特徵。 本文則採用深度網絡進行特徵提取。 使用兩個數據庫： 一個較大的識​​別庫（ImageNet ILSVC 2012）：標定每張圖片中物體的類別。 一千萬圖像，1000類。 一個較小的檢測庫（PASCAL VOC 2007）：標定每張圖片中，物體的類別和位置，一萬圖像，20類。 本文使用識別庫進行預訓練得到CNN（有監督預訓練），而後用檢測庫調優參數，最後在檢測庫上評測。<br>看到這裡也許你已經對很多名詞很困惑，下面會解釋。 先來看看它的基本流程：</p><h1 id="基本流程"><a href="#基本流程" class="headerlink" title="基本流程"></a>基本流程</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15099664799300.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15099707654238.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>RCNN算法分為4個步驟 </p><p>候選區域生成： 一張圖像生成1K~2K個候選區域（採用Selective Search 方法）<br>特徵提取： 對每個候選區域，使用深度卷積網絡提取特徵（CNN）<br>類別判斷： 特徵送入每一類的SVM 分類器，判別是否屬於該類<br>位置精修： 使用回歸器精細修正候選框位置 </p><p>【基礎知識===================================】</p><h3 id="Selective-Search"><a href="#Selective-Search" class="headerlink" title="Selective Search"></a>Selective Search</h3><p>主要思想:<br>使用一種過分割手段，將圖像分割成小區域(1k~2k 個)<br>查看現有小區域，按照合併規則合併可能性最高的相鄰兩個區域。 重複直到整張圖像合併成一個區域位置<br>輸出所有曾經存在過的區域，所謂候選區域<br>其中合併規則如下： 優先合併以下四種區域：</p><p>顏色（顏色直方圖）相近的<br>紋理（梯度直方圖）相近的<br>合併後總面積小的： 保證合併操作的尺度較為均勻，避免一個大區域陸續“吃掉”其他小區域（例：設有區域abcdefgh。較好的合併方式是：ab-cd-ef-gh - &gt; abcd-efgh -&gt; abcdefgh。 不好的合併方法是：ab-cdefgh -&gt;abcd-efgh -&gt;abcdef-gh -&gt; abcdefgh）<br>合併後，總面積在其BBOX中所佔比例大的： 保證合併後形狀規則。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15099665196976.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>上述四條規則只涉及區域的顏色直方圖、梯度直方圖、面積和位置。 合併後的區域特徵可以直接由子區域特徵計算而來，速度較快。</p><p>有監督預訓練與無監督預訓練: </p><p>(1)無監督預訓練(Unsupervised pre-training)</p><p>預訓練階段的樣本不需要人工標註數據，所以就叫做無監督預訓練。</p><p>(2)有監督預訓練(Supervised pre-training)</p><p>所謂的有監督預訓練也可以把它稱之為遷移學習。 比如你已經有一大堆標註好的人臉年齡分類的圖片數據，訓練了一個CNN，用於人臉的年齡識別。 然後當你遇到新的項目任務時：人臉性別識別，那麼這個時候你可以利用已經訓練好的年齡識別CNN模型，去掉最後一層，然後其它的網絡層參數就直接複製過來，繼續進行訓練，讓它輸出性別。 這就是所謂的遷移學習，說的簡單一點就是把一個任務訓練好的參數，拿到另外一個任務，作為神經網絡的初始參數值,這樣相比於你直接採用隨機初始化的方法，精度可以有很大的提高。 </p><p>對於目標檢測問題： 圖片分類標註好的訓練數據非常多，但是物體檢測的標註數據卻很少，如何用少量的標註數據，訓練高質量的模型，這就是文獻最大的特點，這篇論文采用了遷移學習的思想：先用了ILSVRC2012這個訓練數據庫（這是一個圖片分類訓練數據庫），先進行網絡圖片分類訓練。 這個數據庫有大量的標註數據，共包含了1000種類別物體，因此預訓練階段CNN模型的輸出是1000個神經元（當然也直接可以採用Alexnet訓練好的模型參數）。</p><h3 id="重疊度（IOU）"><a href="#重疊度（IOU）" class="headerlink" title="重疊度（IOU）:"></a>重疊度（IOU）:</h3><p>物體檢測需要定位出物體的bounding box，就像下面的圖片一樣，我們不僅要定位出車輛的bounding box 我們還要識別出bounding box 裡面的物體就是車輛。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15099665384173.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>對於bounding box的定位精度，有一個很重要的概念： 因為我們算法不可能百分百跟人工標註的數據完全匹配，因此就存在一個定位精度評價公式：IOU。 它定義了兩個bounding box的重疊度，如下圖所示 </p><p><img src="/media/15099665471055.jpg" alt=""></p><p>就是矩形框A、B的重疊面積佔A、B並集的面積比例。</p><h3 id="非極大值抑制（-NMS-）："><a href="#非極大值抑制（-NMS-）：" class="headerlink" title="非極大值抑制（ NMS ）："></a>非極大值抑制（ NMS ）：</h3><p>RCNN會從一張圖片中找出n個可能是物體的矩形框，然後為每個矩形框為做類別分類概率：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15099665602965.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>就像上面的圖片一樣，定位一個車輛，最後算法就找出了一堆的方框，我們需要判別哪些矩形框是沒用的。 非極大值抑制的方法是：先假設有6個矩形框，根據分類器的類別分類概率做排序，假設從小到大屬於車輛的概率分別為A、B、C、D、E、F。</p><p>(1)從最大概率矩形框F開始，分別判斷A~E與F的重疊度IOU是否大於某個設定的閾值;</p><p>(2)假設B、D與F的重疊度超過閾值，那麼就扔掉B、D；並標記第一個矩形框F，是我們保留下來的。</p><p>(3)從剩下的矩形框A、C、E中，選擇概率最大的E，然後判斷E與A、C的重疊度，重疊度大於一定的閾值，那麼就扔掉；並標記E是我們保留下來的第二個矩形框。</p><p>就這樣一直重複，找到所有被保留下來的矩形框。</p><p>非極大值抑制（NMS）顧名思義就是抑制不是極大值的元素，搜索局部的極大值。 這個局部代表的是一個鄰域，鄰域有兩個參數可變，一是鄰域的維數，二是鄰域的大小。 這裡不討論通用的NMS算法，而是用於在目標檢測中用於提取分數最高的窗口的。 例如在行人檢測中，滑動窗口經提取特徵，經分類器分類識別後，每個窗口都會得到一個分數。 但是滑動窗口會導致很多窗口與其他窗口存在包含或者大部分交叉的情況。 這時就需要用到NMS來選取那些鄰域里分數最高（是行人的概率最大），並且抑制那些分數低的窗口。</p><p>###VOC物體檢測任務:</p><p>相當於一個競賽，裡麵包含了20個物體類別： PASCAL VOC2011 Example Images 還有一個背景，總共就相當於21個類別，因此一會設計fine-tuning CNN的時候，我們softmax分類輸出層為21個神經元。</p><p>【各個階段詳解===================================】 </p><p>總體思路再回顧：</p><p>首先對每一個輸入的圖片產生近2000個不分種類的候選區域（region proposals），然後使用CNNs從每個候選框中提取一個固定長度的特徵向量（4096維度），接著對每個取出的特徵向量使用特定種類的線性SVM進行分類。 也就是總個過程分為三個程序：a、找出候選框；b、利用CNN提取特徵向量；c、利用SVM進行特徵向量分類。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15099666382907.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h4 id="候選框搜索階段："><a href="#候選框搜索階段：" class="headerlink" title="候選框搜索階段："></a>候選框搜索階段：</h4><p>當我們輸入一張圖片時，我們要搜索出所有可能是物體的區域，這裡採用的就是前面提到的Selective Search方法，通過這個算法我們搜索出2000個候選框。 然後從上面的總流程圖中可以看到，搜出的候選框是矩形的，而且是大小各不相同。 然而CNN對輸入圖片的大小是有固定的，如果把搜索到的矩形選框不做處理，就扔進CNN中，肯定不行。 因此對於每個輸入的候選框都需要縮放到固定的大小。 下面我們講解要怎麼進行縮放處理，為了簡單起見我們假設下一階段CNN所需要的輸入圖片大小是個正方形圖片227*227。 因為我們經過selective search 得到的是矩形框，paper試驗了兩種不同的處理方法：</p><p>(1)各向異性縮放</p><p>這種方法很簡單，就是不管圖片的長寬比例，管它是否扭曲，進行縮放就是了，全部縮放到CNN輸入的大小227*227，如下圖(D)所示；<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15099666636066.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>(2)各向同性縮放</p><p>因為圖片扭曲後，估計會對後續CNN的訓練精度有影響，於是作者也測試了“各向同性縮放”方案。 有兩種辦法</p><p>A、先擴充後裁剪： 直接在原始圖片中，把bounding box的邊界進行擴展延伸成正方形，然後再進行裁剪；如果已經延伸到了原始圖片的外邊界，那麼就用bounding box中的顏色均值填充；如上圖(B)所示;</p><p>B、先裁剪後擴充：先把bounding box圖片裁剪出來，然後用固定的背景顏色填充成正方形圖片(背景顏色也是採用bounding box的像素顏色均值),如上圖(C)所示;</p><p>對於上面的異性、同性縮放，文獻還有個padding處理，上面的示意圖中第1、3行就是結合了padding=0,第2、4行結果圖採用padding=16的結果。 經過最後的試驗，作者發現採用各向異性縮放、padding=16的精度最高。</p><p>（備註：候選框的搜索策略作者也考慮過使用一個滑動窗口的方法，然而由於更深的網絡，更大的輸入圖片和滑動步長，使得使用滑動窗口來定位的方法充滿了挑戰。） </p><p>CNN特徵提取階段： </p><p>1、算法實現</p><p>a、網絡結構設計階段<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15099666918238.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15099666873410.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>網絡架構兩個可選方案：第一選擇經典的Alexnet；第二選擇VGG16。 經過測試Alexnet精度為58.5%，VGG16精度為66%。 VGG這個模型的特點是選擇比較小的捲積核、選擇較小的跨步，這個網絡的精度高，不過計算量是Alexnet的7倍。 後面為了簡單起見，我們就直接選用Alexnet，並進行講解；Alexnet特徵提取部分包含了5個卷積層、2個全連接層，在Alexnet中p5層神經元個數為9216、 f6、f7的神經元個數都是4096，通過這個網絡訓練完畢後，最後提取特徵每個輸入候選框圖片都能得到一個4096維的特徵向量。</p><p>b、網絡有監督預訓練階段（圖片數據庫：ImageNet ILSVC ）<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15099667042004.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>參數初始化部分：物體檢測的一個難點在於，物體標籤訓練數據少，如果要直接採用隨機初始化CNN參數的方法，那麼目前的訓練數據量是遠遠不夠的。 這種情況下，最好的是採用某些方法，把參數初始化了，然後在進行有監督的參數微調，這里文獻採用的是有監督的預訓練。 所以paper在設計網絡結構的時候，是直接用Alexnet的網絡，然後連參數也是直接採用它的參數，作為初始的參數值，然後再fine-tuning訓練。 網絡優化求解時採用隨機梯度下降法，學習率大小為0.001；</p><p>c、fine-tuning階段（圖片數據庫： PASCAL VOC）</p><p>我們接著採用selective search 搜索出來的候選框（PASCAL VOC 數據庫中的圖片） 繼續對上面預訓練的CNN模型進行fine-tuning訓練。 假設要檢測的物體類別有N類，那麼我們就需要把上面預訓練階段的CNN模型的最後一層給替換掉，替換成N+1個輸出的神經元(加1，表示還有一個背景) (20 + 1bg = 21)，然後這一層直接採用參數隨機初始化的方法，其它網絡層的參數不變；接著就可以開始繼續SGD訓練了。 開始的時候，SGD學習率選擇0.001，在每次訓練的時候，我們batch size大小選擇128，其中32個事正樣本、96個事負樣本。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15099667194897.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><p>關於正負樣本問題：</p><p>一張照片我們得到了2000個候選框。 然而人工標註的數據一張圖片中就只標註了正確的bounding box，我們搜索出來的2000個矩形框也不可能會出現一個與人工標註完全匹配的候選框。 因此在CNN階段我們需要用IOU為2000個bounding box打標籤。 如果用selective search挑選出來的候選框與物體的人工標註矩形框（PASCAL VOC的圖片都有人工標註）的重疊區域IoU大於0.5，那麼我們就把這個候選框標註成物體類別（正樣本），否則我們就把它當做背景類別（負樣本）。</p><p>（備註： 如果不針對特定任務進行fine-tuning，而是把CNN當做特徵提取器，卷積層所學到的特徵其實就是基礎的共享特徵提取層，就類似於SIFT算法一樣，可以用於提取各種圖片的特徵，而f6、f7所學習到的特徵是用於針對特定任務的特徵。打個比方：對於人臉性別識別來說，一個CNN模型前面的捲積層所學習到的特徵就類似於學習人臉共性特徵，然後全連接層所學習的特徵就是針對性別分類的特徵了）</p><p>2.疑惑點 ： CNN訓練的時候，本來就是對bounding box的物體進行識別分類訓練，在訓練的時候最後一層softmax就是分類層。 那麼為什麼作者閒著沒事幹要先用CNN做特徵提取（提取fc7層數據），然後再把提取的特徵用於訓練svm分類器？ </p><p>這個是因為svm訓練和cnn訓練過程的正負樣本定義方式各有不同，導致最後採用CNN softmax輸出比採用svm精度還低。 事情是這樣的，cnn在訓練的時候，對訓練數據做了比較寬鬆的標註，比如一個bounding box可能只包含物體的一部分，那麼我也把它標註為正樣本，用於訓練cnn；採用這個方法的主要原因在於因為CNN容易過擬合，所以需要大量的訓練數據，所以在CNN訓練階段我們是對Bounding box的位置限制條件限制的比較鬆(IOU只要大於0.5都被標註為正樣本了)；然而svm訓練的時候，因為svm適用於少樣本訓練，所以對於訓練樣本數據的IOU要求比較嚴格，我們只有當bounding box把整個物體都包含進去了，我們才把它標註為物體類別，然後訓練svm ，具體請看下文。</p><p>SVM訓練、測試階段</p><p>訓練階段 ：</p><p>這是一個二分類問題，我麼假設我們要檢測車輛。 我們知道只有當bounding box把整量車都包含在內，那才叫正樣本；如果bounding box 沒有包含到車輛，那麼我們就可以把它當做負樣本。 但問題是當我們的檢測窗口只有部分包含物體，那該怎麼定義正負樣本呢？ 作者測試了IOU閾值各種方案數值0,0.1,0.2,0.3,0.4,0.5。 最後通過訓練發現，如果選擇IOU閾值為0.3 效果最好 （選擇為0精度下降了4個百分點，選擇0.5精度下降了5個百分點）,即當重疊度小於0.3的時候，我們就把它標註為負樣本。 一旦CNN f7層特徵被提取出來，那麼我們將為每個物體類訓練一個svm分類器。 當我們用CNN提取2000個候選框，可以得到2000x4096這樣的特徵向量矩陣，然後我們只需要把這樣的一個矩陣與svm權值矩陣4096xN點乘(N為分類類別數目，因為我們訓練的N個svm，每個svm包含了4096個權值w)，就可以得到結果了。 </p><p><img src="/media/15099667603349.jpg" alt=""></p><p>得到的特徵輸入到SVM進行分類看看這個feature vector所對應的region proposal是需要的物體還是無關的實物(background) 。 排序，canny邊界檢測之後就得到了我們需要的bounding-box。 </p><p>再回顧總結一下：整個系統分為三個部分：1.產生不依賴與特定類別的region proposals，這些region proposals定義了一個整個檢測器可以獲得的候選目標2.一個大的捲積神經網絡，對每個region產生一個固定長度的特徵向量3.一系列特定類別的線性SVM分類器。</p><p>位置精修： 目標檢測問題的衡量標準是重疊面積：許多看似準確的檢測結果，往往因為候選框不夠準確，重疊面積很小。 故需要一個位置精修步驟。 回歸器：對每一類目標，使用一個線性脊回歸器進行精修。 正則項λ=10000。 輸入為深度網絡pool5層的4096維特徵，輸出為xy方向的縮放和平移。 訓練樣本：判定為本類的候選框中和真值重疊面積大於0.6的候選框。 </p><p>測試階段 ：</p><p>使用selective search的方法在測試圖片上提取2000個region propasals ，將每個region proposals歸一化到227x227，然後再CNN中正向傳播，將最後一層得到的特徵提取出來。 然後對於每一個類別，使用為這一類訓練的SVM分類器對提取的特徵向量進行打分，得到測試圖片中對於所有region proposals的對於這一類的分數，再使用貪心的非極大值抑制（ NMS）去除相交的多餘的框。 再對這些框進行canny邊緣檢測，就可以得到bounding-box(then B-BoxRegression)。</p><p>（非極大值抑制（NMS）先計算出每一個bounding box的面積，然後根據score進行排序，把score最大的bounding box作為選定的框，計算其餘bounding box與當前最大score與box的IoU，去除IoU大於設定的閾值的bounding box。然後重複上面的過程，直至候選bounding box為空，然後再將score小於一定閾值的選定框刪除得到這一類的結果（然後繼續進行下一個分類） 。作者提到花費在region propasals和提取特徵的時間是13s/張-GPU和53s/張-CPU，可以看出時間還是很長的，不能夠達到及時性。 </p><p>完。</p><p>本文主要整理自以下文章：</p><p>RCNN學習筆記(0):rcnn簡介<br>RCNN學習筆記(1):Rich feature hierarchies for accurate object detection and semantic segmentation<br>RCNN學習筆記(2):Rich feature hierarchies for accurate object detection and semantic segmentation<br>《Rich feature hierarchies for Accurate Object Detection and Segmentation》<br>《Spatial 《Pyramid Pooling in Deep Convolutional Networks for Visual Recognition》</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;RCNN-將CNN引入目標檢測的開山之作&quot;&gt;&lt;a href=&quot;#RCNN-將CNN引入目標檢測的開山之作&quot; class=&quot;headerlink&quot; title=&quot;RCNN- 將CNN引入目標檢測的開山之作&quot;&gt;&lt;/a&gt;RCNN- 將CNN引入目標檢測的開山之作&lt;/h1
      
    
    </summary>
    
      <category term="Detection" scheme="https://world4jason.github.io/categories/Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://world4jason.github.io/tags/Deep-Learning/"/>
    
      <category term="R-CNN" scheme="https://world4jason.github.io/tags/R-CNN/"/>
    
      <category term="CNN" scheme="https://world4jason.github.io/tags/CNN/"/>
    
      <category term="Object Detection" scheme="https://world4jason.github.io/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>Object Detection with Convolution Neural Network Series</title>
    <link href="https://world4jason.github.io/2017/11/06/Object-Detection-with-Convolution-Neural-Network/"/>
    <id>https://world4jason.github.io/2017/11/06/Object-Detection-with-Convolution-Neural-Network/</id>
    <published>2017-11-05T18:51:33.000Z</published>
    <updated>2017-12-28T06:55:34.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Object-Detection-with-Convolution-Neural-Network-Series"><a href="#Object-Detection-with-Convolution-Neural-Network-Series" class="headerlink" title="Object Detection with Convolution Neural Network Series"></a>Object Detection with Convolution Neural Network Series</h1><p>Outline</p><ul><li>檢測任務?</li><li>R-CNN</li><li>Fast R-CNN</li><li>Faster R-CNN</li><li>SSD</li><li>YOLO</li></ul><h2 id="檢測任務"><a href="#檢測任務" class="headerlink" title="檢測任務"></a>檢測任務</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15100388874028.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h2 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h2><center><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15099707654238.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></center><p>步驟 </p><ol><li>候選區域生成： 一張圖像生成1K~2K個候選區域（採用Selective Search 方法） </li><li>特徵提取： 對每個候選區域，使用深度卷積網絡提取特徵（CNN） </li><li>類別判斷： 特徵送入每一類的SVM 分類器，判別是否屬於該類 </li><li>位置精修： 使用回歸器精細修正候選框位置 </li><li>用Non-Maximum Selection 合併後選框</li></ol><h2 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h2><center><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15100371122830.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></center><br>步驟<br>1. 候選區域生成： 一張圖像生成1K~2K個候選區域（採用Selective Search 方法）<br>2. 特徵提取： 該張圖片，使用深度卷積網絡提取特徵（CNN）<br>3. ROI Pooling：<br>4. 類別判斷： 特徵送入每一類的SVM 分類器，判別是否屬於該類<br>5. 位置精修： 使用回歸器精細修正候選框位置<br>6. 用Non-Maximum Selection 合併後選框<br><br>## Faster R-CNN<br><center><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15100371249624.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></center><h2 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h2><center><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15100374296902.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br></center><h2 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/media/15144441218022.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>Reference:</p><ol><li><a href="http://zh.gluon.ai/" target="_blank" rel="external">http://zh.gluon.ai/</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Object-Detection-with-Convolution-Neural-Network-Series&quot;&gt;&lt;a href=&quot;#Object-Detection-with-Convolution-Neural-Network-Series&quot; class=&quot;h
      
    
    </summary>
    
      <category term="Detection" scheme="https://world4jason.github.io/categories/Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://world4jason.github.io/tags/Deep-Learning/"/>
    
      <category term="R-CNN" scheme="https://world4jason.github.io/tags/R-CNN/"/>
    
      <category term="CNN" scheme="https://world4jason.github.io/tags/CNN/"/>
    
      <category term="Object Detection" scheme="https://world4jason.github.io/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>CPP 筆記</title>
    <link href="https://world4jason.github.io/2017/10/19/CPP-note/"/>
    <id>https://world4jason.github.io/2017/10/19/CPP-note/</id>
    <published>2017-10-19T04:07:48.000Z</published>
    <updated>2018-01-04T07:23:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>##C++中cin、cin.get()、cin.getline()、getline()、gets()等函数的用法<br>学C++的时候，这几个输入函数弄的有点迷糊；这里做个小结，为了自己复习，也希望对后来者能有所帮助，如果有差错的地方还请各位多多指教</p><p>1、cin<br>2、cin.get()<br>3、cin.getline()<br>4、getline()<br>5、gets()<br>6、getchar()</p><p>附:cin.ignore();cin.get()//跳过一个字符,例如不想要的回车,空格等字符</p><p>1、cin&gt;&gt;         </p><p>用法1：最基本，也是最常用的用法，输入一个数字：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt; </span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>; </div><div class="line">main () </div><div class="line">&#123; </div><div class="line"><span class="keyword">int</span> a,b; </div><div class="line"><span class="built_in">cin</span>&gt;&gt;a&gt;&gt;b; </div><div class="line"><span class="built_in">cout</span>&lt;&lt;a+b&lt;&lt;<span class="built_in">endl</span>; </div><div class="line">&#125;</div></pre></td></tr></table></figure><p>输入：2[回车]3[回车]<br>输出：5</p><p>注意:&gt;&gt; 是会过滤掉不可见字符（如 空格 回车，TAB 等）<br>cin&gt;&gt;noskipws&gt;&gt;input[j];//不想略过空白字符，那就使用 noskipws 流控制</p><p>用法2：接受一个字符串，遇“空格”、“TAB”、“回车”都结束</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt; </span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>; </div><div class="line">main () </div><div class="line">&#123; </div><div class="line"><span class="keyword">char</span> a[<span class="number">20</span>]; </div><div class="line"><span class="built_in">cin</span>&gt;&gt;a; </div><div class="line"><span class="built_in">cout</span>&lt;&lt;a&lt;&lt;<span class="built_in">endl</span>; </div><div class="line">&#125;</div></pre></td></tr></table></figure><p>输入：jkljkljkl<br>输出：jkljkljkl</p><p>输入：jkljkl jkljkl       //遇空格结束<br>输出：jkljkl</p><p>2、cin.get()</p><p>用法1： cin.get(字符变量名)可以用来接收字符</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt; </span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>; </div><div class="line">main () </div><div class="line">&#123; </div><div class="line"><span class="keyword">char</span> ch; </div><div class="line">ch=<span class="built_in">cin</span>.get();               <span class="comment">//或者cin.get(ch); </span></div><div class="line"><span class="built_in">cout</span>&lt;&lt;ch&lt;&lt;<span class="built_in">endl</span>; </div><div class="line">&#125;</div></pre></td></tr></table></figure><p>输入：jljkljkl<br>输出：j</p><p>用法2：cin.get(字符数组名,接收字符数目)用来接收一行字符串,可以接收空格</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt; </span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>; </div><div class="line">main () </div><div class="line">&#123; </div><div class="line"><span class="keyword">char</span> a[<span class="number">20</span>]; </div><div class="line"><span class="built_in">cin</span>.get(a,<span class="number">20</span>); </div><div class="line"><span class="built_in">cout</span>&lt;&lt;a&lt;&lt;<span class="built_in">endl</span>; </div><div class="line">&#125;</div></pre></td></tr></table></figure><p>输入：jkl jkl jkl<br>输出：jkl jkl jkl</p><p>输入：abcdeabcdeabcdeabcdeabcde （输入25个字符）<br>输出：abcdeabcdeabcdeabcd              （接收19个字符+1个’\0’）</p><p>用法3：cin.get(无参数)没有参数主要是用于舍弃输入流中的不需要的字符,或者舍弃回车,弥补cin.get(字符数组名,接收字符数目)的不足.</p><p>这个我还不知道怎么用，知道的前辈请赐教；</p><p>3、cin.getline()   // 接受一个字符串，可以接收空格并输出</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt; </span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>; </div><div class="line">main () </div><div class="line">&#123; </div><div class="line"><span class="keyword">char</span> m[<span class="number">20</span>]; </div><div class="line"><span class="built_in">cin</span>.getline(m,<span class="number">5</span>); </div><div class="line"><span class="built_in">cout</span>&lt;&lt;m&lt;&lt;<span class="built_in">endl</span>; </div><div class="line">&#125;</div></pre></td></tr></table></figure><p>输入：jkljkljkl<br>输出：jklj</p><p>接受5个字符到m中，其中最后一个为’\0’，所以只看到4个字符输出；</p><p>如果把5改成20：<br>输入：jkljkljkl<br>输出：jkljkljkl</p><p>输入：jklf fjlsjf fjsdklf<br>输出：jklf fjlsjf fjsdklf</p><p>//延伸：<br>//cin.getline()实际上有三个参数，cin.getline(接受字符串的看哦那间m,接受个数5,结束字符)<br>//当第三个参数省略时，系统默认为’\0’<br>//如果将例子中cin.getline()改为cin.getline(m,5,’a’);当输入jlkjkljkl时输出jklj，输入jkaljkljkl时，输出jk</p><p>当用在多维数组中的时候，也可以用cin.getline(m[i],20)之类的用法：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt; </span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt; </span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"></div><div class="line">main () </div><div class="line">&#123; </div><div class="line"><span class="keyword">char</span> m[<span class="number">3</span>][<span class="number">20</span>]; </div><div class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="number">3</span>;i++) </div><div class="line">&#123; </div><div class="line"><span class="built_in">cout</span>&lt;&lt;<span class="string">"\n请输入第"</span>&lt;&lt;i+<span class="number">1</span>&lt;&lt;<span class="string">"个字符串："</span>&lt;&lt;<span class="built_in">endl</span>; </div><div class="line"><span class="built_in">cin</span>.getline(m[i],<span class="number">20</span>); </div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="built_in">cout</span>&lt;&lt;<span class="built_in">endl</span>; </div><div class="line"><span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;<span class="number">3</span>;j++) </div><div class="line"><span class="built_in">cout</span>&lt;&lt;<span class="string">"输出m["</span>&lt;&lt;j&lt;&lt;<span class="string">"]的值:"</span>&lt;&lt;m[j]&lt;&lt;<span class="built_in">endl</span>;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure><p>请输入第1个字符串：<br>kskr1</p><p>请输入第2个字符串：<br>kskr2</p><p>请输入第3个字符串：<br>kskr3</p><p>输出m[0]的值:kskr1<br>输出m[1]的值:kskr2<br>输出m[2]的值:kskr3</p><p>4、getline()     // 接受一个字符串，可以接收空格并输出，需包含“#include<string>”</string></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt; </span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt; </span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>; </div><div class="line">main () </div><div class="line">&#123; </div><div class="line"><span class="built_in">string</span> str; </div><div class="line">getline(<span class="built_in">cin</span>,str); </div><div class="line"><span class="built_in">cout</span>&lt;&lt;str&lt;&lt;<span class="built_in">endl</span>; </div><div class="line">&#125;</div></pre></td></tr></table></figure><p>输入：jkljkljkl<br>输出：jkljkljkl</p><p>输入：jkl jfksldfj jklsjfl<br>输出：jkl jfksldfj jklsjfl</p><p>和cin.getline()类似，但是cin.getline()属于istream流，而getline()属于string流，是不一样的两个函数</p><p>在寫程式的時候遇到這個問題<br>因為atoi函式只吃char*<br>上網GOOGLE一下找到了解決之法</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">string</span> x;</div><div class="line"><span class="keyword">int</span> temp=atoi(x.c_str());</div><div class="line">c_str()可以轉換<span class="built_in">string</span>成為<span class="keyword">char</span>*</div></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span> ;</div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line">&#123;</div><div class="line">    <span class="built_in">string</span> s ;</div><div class="line">    <span class="keyword">int</span> a;</div><div class="line">    <span class="keyword">while</span> (getline(<span class="built_in">cin</span>,s))</div><div class="line">    &#123;</div><div class="line">        <span class="keyword">int</span> n=s.length();</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++) </div><div class="line">        &#123;</div><div class="line">            a=s.at(i);</div><div class="line">            <span class="built_in">cout</span> &lt;&lt; a &lt;&lt; <span class="string">" "</span>;</div><div class="line">         &#125;</div><div class="line">         <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span> ;</div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;##C++中cin、cin.get()、cin.getline()、getline()、gets()等函数的用法&lt;br&gt;学C++的时候，这几个输入函数弄的有点迷糊；这里做个小结，为了自己复习，也希望对后来者能有所帮助，如果有差错的地方还请各位多多指教&lt;/p&gt;
&lt;p&gt;1、ci
      
    
    </summary>
    
      <category term="Code" scheme="https://world4jason.github.io/categories/Code/"/>
    
    
      <category term="code" scheme="https://world4jason.github.io/tags/code/"/>
    
      <category term="CPP" scheme="https://world4jason.github.io/tags/CPP/"/>
    
      <category term="C++" scheme="https://world4jason.github.io/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://world4jason.github.io/2017/03/28/hello-world/"/>
    <id>https://world4jason.github.io/2017/03/28/hello-world/</id>
    <published>2017-03-28T06:57:15.000Z</published>
    <updated>2017-03-28T06:57:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
